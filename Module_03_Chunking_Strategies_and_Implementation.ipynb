{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Chunking Strategies & Implementation\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand why chunking is essential for RAG systems\n",
    "- Implement different chunking strategies using LangChain\n",
    "- Compare fixed-size, semantic, and recursive chunking approaches\n",
    "- Optimize chunk sizes for different use cases\n",
    "- Preserve context and metadata across chunks\n",
    "- Apply 2025's latest semantic chunking techniques\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### Why Chunking is Critical üî™\n",
    "\n",
    "**Think of chunking like creating an index for a book:**\n",
    "- You don't read the entire book to find one fact\n",
    "- You look up the relevant chapter/section\n",
    "- RAG does the same with your documents\n",
    "\n",
    "### The Chunking Challenge\n",
    "| Too Small üìè | Just Right ‚úÖ | Too Large üìê |\n",
    "|--------------|---------------|---------------|\n",
    "| Loses context | Preserves meaning | Hard to match |\n",
    "| Poor retrieval | Good relevance | Noise |\n",
    "| Fast processing | Balanced | Slow processing |\n",
    "\n",
    "### 2025 Chunking Evolution üöÄ\n",
    "- **Semantic Chunking**: AI determines natural boundaries\n",
    "- **Adaptive Chunking**: Dynamic sizing based on content type\n",
    "- **Contextual Preservation**: Better metadata and overlap strategies\n",
    "- **Embedding-Based Splitting**: Use embeddings to find semantic breaks\n",
    "\n",
    "### Common Chunking Strategies\n",
    "1. **Fixed-Size**: Split by character/token count\n",
    "2. **Semantic**: Split by meaning and context\n",
    "3. **Recursive**: Try multiple separators hierarchically\n",
    "4. **Adaptive**: Adjust parameters by content type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "Let's install and import the required packages for chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community tiktoken\n",
    "!pip install -q sentence-transformers numpy matplotlib\n",
    "!pip install -q scikit-learn  # For clustering in semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# For token counting\n",
    "import tiktoken\n",
    "\n",
    "# For semantic chunking (2025 approach)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üî™ Ready to chunk documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÑ Exercise 1: Creating Sample Documents for Chunking\n",
    "\n",
    "Let's create different types of documents to see how chunking strategies work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents directory\n",
    "docs_dir = Path(\"chunking_samples\")\n",
    "docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Sample 1: Technical documentation\n",
    "technical_doc = \"\"\"\n",
    "# API Documentation\n",
    "\n",
    "## Authentication\n",
    "\n",
    "Our API uses Bearer token authentication. Include your API key in the Authorization header:\n",
    "\n",
    "```\n",
    "Authorization: Bearer YOUR_API_KEY\n",
    "```\n",
    "\n",
    "All API requests must be made over HTTPS. Requests made over plain HTTP will fail.\n",
    "\n",
    "## Rate Limiting\n",
    "\n",
    "API calls are rate limited to prevent abuse. The current limits are:\n",
    "\n",
    "- Free tier: 100 requests per hour\n",
    "- Pro tier: 1,000 requests per hour  \n",
    "- Enterprise: 10,000 requests per hour\n",
    "\n",
    "When you exceed your rate limit, you'll receive a 429 status code with details about when you can make your next request.\n",
    "\n",
    "## User Management\n",
    "\n",
    "### Creating Users\n",
    "\n",
    "To create a new user, send a POST request to `/api/v1/users` with the following payload:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"username\": \"john_doe\",\n",
    "  \"email\": \"john@example.com\",\n",
    "  \"password\": \"secure_password\",\n",
    "  \"role\": \"user\"\n",
    "}\n",
    "```\n",
    "\n",
    "### Updating Users\n",
    "\n",
    "User information can be updated by sending a PUT request to `/api/v1/users/{user_id}`. Only the user themselves or an admin can update user information.\n",
    "\n",
    "### Deleting Users\n",
    "\n",
    "Users can be deleted by sending a DELETE request to `/api/v1/users/{user_id}`. This action is irreversible and will permanently remove all user data.\n",
    "\n",
    "## Data Analytics\n",
    "\n",
    "The analytics endpoint provides insights into your application usage. Available metrics include:\n",
    "\n",
    "- User engagement statistics\n",
    "- API usage patterns\n",
    "- Performance metrics\n",
    "- Error rates and debugging information\n",
    "\n",
    "Analytics data is updated every 15 minutes and retained for 90 days.\n",
    "\n",
    "## Error Handling\n",
    "\n",
    "Our API uses conventional HTTP response codes to indicate success or failure:\n",
    "\n",
    "- 200: Success\n",
    "- 400: Bad Request - often missing required parameters\n",
    "- 401: Unauthorized - invalid or missing API key\n",
    "- 403: Forbidden - valid API key but insufficient permissions\n",
    "- 404: Not Found - resource doesn't exist\n",
    "- 429: Too Many Requests - rate limit exceeded\n",
    "- 500: Internal Server Error - something went wrong on our end\n",
    "\n",
    "Error responses include a detailed message explaining what went wrong and how to fix it.\n",
    "\"\"\"\n",
    "\n",
    "with open(docs_dir / \"api_documentation.txt\", \"w\") as f:\n",
    "    f.write(technical_doc.strip())\n",
    "\n",
    "print(\"‚úÖ Created: api_documentation.txt\")\n",
    "print(f\"üìè Length: {len(technical_doc)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 2: Narrative content (different structure)\n",
    "narrative_doc = \"\"\"\n",
    "The Evolution of Artificial Intelligence in Healthcare\n",
    "\n",
    "Healthcare has always been at the forefront of technological innovation, but the integration of artificial intelligence represents perhaps the most significant transformation in modern medical practice. From early expert systems in the 1970s to today's sophisticated machine learning algorithms, AI has steadily evolved to address complex medical challenges.\n",
    "\n",
    "The journey began with simple rule-based systems that could assist doctors in diagnosis. These early systems, while limited, laid the groundwork for more sophisticated approaches. They demonstrated that machines could process medical knowledge in structured ways, even if they couldn't match human intuition and experience.\n",
    "\n",
    "In the 1990s, the advent of neural networks brought new possibilities. Researchers began experimenting with pattern recognition in medical imaging, leading to breakthrough applications in radiology. The ability to detect subtle patterns in X-rays, CT scans, and MRIs that might escape the human eye became a game-changer for early disease detection.\n",
    "\n",
    "The real revolution came with deep learning in the 2010s. Convolutional neural networks achieved superhuman performance in specific imaging tasks. Google's DeepMind developed AI systems that could diagnose diabetic retinopathy from retinal photographs with greater accuracy than specialist doctors. Similarly, IBM's Watson for Oncology promised to revolutionize cancer treatment by analyzing vast amounts of medical literature and patient data.\n",
    "\n",
    "However, the path hasn't been without challenges. Early enthusiasm sometimes outpaced practical implementation. Watson for Oncology, despite initial promise, faced criticism for providing treatment recommendations that didn't always align with standard care practices. This highlighted the importance of rigorous testing and validation in medical AI systems.\n",
    "\n",
    "Today, AI in healthcare has matured significantly. Electronic health records powered by natural language processing can extract insights from unstructured clinical notes. Predictive analytics help identify patients at risk of sepsis or readmission. Drug discovery processes that once took decades are being accelerated through AI-driven molecular modeling.\n",
    "\n",
    "The COVID-19 pandemic accelerated AI adoption in unprecedented ways. Contact tracing apps, vaccine distribution optimization, and real-time monitoring of virus variants all relied on artificial intelligence. Telemedicine platforms integrated AI-powered triage systems to manage the surge in virtual consultations.\n",
    "\n",
    "Looking forward, the future of AI in healthcare appears incredibly promising. Personalized medicine, where treatments are tailored to individual genetic profiles, is becoming reality through AI analysis of genomic data. Robotic surgery assisted by AI provides unprecedented precision. Mental health applications use natural language processing to provide support and early intervention.\n",
    "\n",
    "Yet challenges remain. Data privacy and security are paramount concerns when dealing with sensitive medical information. Algorithmic bias can perpetuate healthcare disparities if not carefully addressed. The need for transparency and explainability in AI medical decisions is crucial for maintaining physician and patient trust.\n",
    "\n",
    "Regulatory frameworks are evolving to keep pace with technological advancement. The FDA has approved numerous AI-based medical devices, establishing precedents for safety and efficacy standards. International collaboration on AI healthcare standards ensures global compatibility and safety.\n",
    "\n",
    "As we move into the next decade, the integration of AI in healthcare will likely become even more seamless and pervasive. The key lies in maintaining the balance between technological innovation and human-centered care, ensuring that artificial intelligence enhances rather than replaces the fundamental human elements of medical practice.\n",
    "\"\"\"\n",
    "\n",
    "with open(docs_dir / \"ai_healthcare.txt\", \"w\") as f:\n",
    "    f.write(narrative_doc.strip())\n",
    "\n",
    "print(\"‚úÖ Created: ai_healthcare.txt\")\n",
    "print(f\"üìè Length: {len(narrative_doc)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our documents\n",
    "docs = []\n",
    "\n",
    "# Load technical documentation\n",
    "tech_loader = TextLoader(str(docs_dir / \"api_documentation.txt\"))\n",
    "tech_docs = tech_loader.load()\n",
    "docs.extend(tech_docs)\n",
    "\n",
    "# Load narrative document\n",
    "narrative_loader = TextLoader(str(docs_dir / \"ai_healthcare.txt\"))\n",
    "narrative_docs = narrative_loader.load()\n",
    "docs.extend(narrative_docs)\n",
    "\n",
    "print(f\"üìö Loaded {len(docs)} documents for chunking experiments\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"   Doc {i+1}: {len(doc.page_content):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî™ Exercise 2: Fixed-Size Chunking\n",
    "\n",
    "Let's start with the simplest approach: splitting documents into fixed-size chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_chunks(chunks, method_name):\n",
    "    \"\"\"\n",
    "    Analyze and visualize chunk characteristics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä {method_name} Analysis:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        # Calculate statistics\n",
    "        chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "        \n",
    "        print(f\"   Avg chunk size: {np.mean(chunk_sizes):.0f} chars\")\n",
    "        print(f\"   Min chunk size: {min(chunk_sizes)} chars\")\n",
    "        print(f\"   Max chunk size: {max(chunk_sizes)} chars\")\n",
    "        print(f\"   Std deviation: {np.std(chunk_sizes):.0f} chars\")\n",
    "        \n",
    "        # Show first chunk preview\n",
    "        print(f\"\\nüìù First chunk preview:\")\n",
    "        print(f\"   {chunks[0].page_content[:150]}...\")\n",
    "        \n",
    "        return chunk_sizes\n",
    "    return []\n",
    "\n",
    "# Test different fixed-size approaches\n",
    "print(\"üî™ FIXED-SIZE CHUNKING EXPERIMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the technical document for comparison\n",
    "test_doc = docs[0]  # API documentation\n",
    "\n",
    "# 1. Character-based chunking\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separator=\"\\n\\n\"  # Split on paragraphs when possible\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_documents([test_doc])\n",
    "char_sizes = analyze_chunks(char_chunks, \"Character-based (500 chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Token-based chunking (more precise for LLMs)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,  # 100 tokens\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_documents([test_doc])\n",
    "token_sizes = analyze_chunks(token_chunks, \"Token-based (100 tokens)\")\n",
    "\n",
    "# Let's also show token vs character relationship\n",
    "if token_chunks:\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 encoding\n",
    "    first_chunk_text = token_chunks[0].page_content\n",
    "    token_count = len(encoding.encode(first_chunk_text))\n",
    "    char_count = len(first_chunk_text)\n",
    "    \n",
    "    print(f\"\\nüîç Token vs Character relationship:\")\n",
    "    print(f\"   First chunk: {char_count} chars = {token_count} tokens\")\n",
    "    print(f\"   Ratio: {char_count/token_count:.1f} chars per token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Different chunk sizes comparison\n",
    "chunk_size_experiments = [200, 500, 1000, 2000]\n",
    "size_results = {}\n",
    "\n",
    "print(f\"\\nüî¨ Chunk Size Comparison:\")\n",
    "for size in chunk_size_experiments:\n",
    "    splitter = CharacterTextSplitter(\n",
    "        chunk_size=size,\n",
    "        chunk_overlap=size // 10,  # 10% overlap\n",
    "        separator=\"\\n\\n\"\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents([test_doc])\n",
    "    chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    \n",
    "    size_results[size] = {\n",
    "        'count': len(chunks),\n",
    "        'avg_size': np.mean(chunk_sizes) if chunk_sizes else 0,\n",
    "        'std_dev': np.std(chunk_sizes) if chunk_sizes else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"   Size {size}: {len(chunks)} chunks, avg {np.mean(chunk_sizes):.0f} chars\")\n",
    "\n",
    "# Visualize the results\n",
    "sizes = list(size_results.keys())\n",
    "counts = [size_results[s]['count'] for s in sizes]\n",
    "avg_sizes = [size_results[s]['avg_size'] for s in sizes]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot chunk count vs target size\n",
    "ax1.bar(sizes, counts, color='skyblue', alpha=0.7)\n",
    "ax1.set_xlabel('Target Chunk Size (chars)')\n",
    "ax1.set_ylabel('Number of Chunks')\n",
    "ax1.set_title('Chunk Count vs Target Size')\n",
    "\n",
    "# Plot actual vs target size\n",
    "ax2.plot(sizes, sizes, 'r--', label='Target Size', alpha=0.7)\n",
    "ax2.plot(sizes, avg_sizes, 'bo-', label='Actual Avg Size')\n",
    "ax2.set_xlabel('Target Chunk Size (chars)')\n",
    "ax2.set_ylabel('Actual Chunk Size (chars)')\n",
    "ax2.set_title('Target vs Actual Chunk Sizes')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Chart shows: Larger target sizes = fewer chunks, but actual sizes may vary due to natural boundaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Exercise 3: Recursive Chunking (LangChain's Smart Approach)\n",
    "\n",
    "Recursive chunking tries multiple separators to find the best split points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursive Character Text Splitter - LangChain's recommended approach\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\n",
    "        \"\\n\\n\",  # Paragraphs (best)\n",
    "        \"\\n\",    # Lines  \n",
    "        \". \",    # Sentences\n",
    "        \" \",     # Words\n",
    "        \"\"       # Characters (last resort)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üîÑ RECURSIVE CHUNKING EXPERIMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test on both documents\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_name = [\"Technical (API Docs)\", \"Narrative (AI Healthcare)\"][i]\n",
    "    \n",
    "    recursive_chunks = recursive_splitter.split_documents([doc])\n",
    "    sizes = analyze_chunks(recursive_chunks, f\"Recursive - {doc_name}\")\n",
    "    \n",
    "    # Let's see where it chose to split\n",
    "    if len(recursive_chunks) >= 2:\n",
    "        print(f\"\\nüîç Split analysis:\")\n",
    "        chunk1_end = recursive_chunks[0].page_content[-50:]\n",
    "        chunk2_start = recursive_chunks[1].page_content[:50]\n",
    "        \n",
    "        print(f\"   Chunk 1 ends: ...{chunk1_end}\")\n",
    "        print(f\"   Chunk 2 starts: {chunk2_start}...\")\n",
    "        \n",
    "        # Check if it found a good boundary\n",
    "        if chunk1_end.endswith('\\n') or chunk2_start.startswith('\\n'):\n",
    "            print(f\"   ‚úÖ Found natural paragraph boundary\")\n",
    "        elif '. ' in chunk1_end[-10:]:\n",
    "            print(f\"   ‚úÖ Found sentence boundary\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  Had to split mid-sentence\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 4: Semantic Chunking (2025 Advanced Technique)\n",
    "\n",
    "Let's implement semantic chunking using embeddings to find natural topic boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a lightweight embedding model for semantic analysis\n",
    "print(\"ü§ñ Loading embedding model for semantic chunking...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')  # Fast, lightweight model\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "def semantic_chunking(text, max_chunk_size=800, similarity_threshold=0.7):\n",
    "    \"\"\"\n",
    "    Advanced semantic chunking using embeddings\n",
    "    \"\"\"\n",
    "    # First, split into sentences\n",
    "    sentences = [s.strip() for s in text.split('.') if s.strip()]\n",
    "    \n",
    "    if len(sentences) < 2:\n",
    "        return [text]  # Too short to chunk meaningfully\n",
    "    \n",
    "    # Get embeddings for each sentence\n",
    "    print(f\"   üßÆ Computing embeddings for {len(sentences)} sentences...\")\n",
    "    embeddings = embedding_model.encode(sentences)\n",
    "    \n",
    "    # Find semantic boundaries\n",
    "    chunks = []\n",
    "    current_chunk = [sentences[0]]\n",
    "    current_length = len(sentences[0])\n",
    "    \n",
    "    for i in range(1, len(sentences)):\n",
    "        # Calculate similarity with previous sentence\n",
    "        similarity = cosine_similarity(\n",
    "            embeddings[i-1].reshape(1, -1),\n",
    "            embeddings[i].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        sentence_length = len(sentences[i])\n",
    "        \n",
    "        # Decide whether to continue current chunk or start new one\n",
    "        should_split = (\n",
    "            current_length + sentence_length > max_chunk_size or  # Size limit\n",
    "            similarity < similarity_threshold  # Semantic boundary\n",
    "        )\n",
    "        \n",
    "        if should_split and current_chunk:\n",
    "            # Finish current chunk\n",
    "            chunks.append('. '.join(current_chunk) + '.')\n",
    "            current_chunk = [sentences[i]]\n",
    "            current_length = sentence_length\n",
    "        else:\n",
    "            # Continue current chunk\n",
    "            current_chunk.append(sentences[i])\n",
    "            current_length += sentence_length\n",
    "    \n",
    "    # Add final chunk\n",
    "    if current_chunk:\n",
    "        chunks.append('. '.join(current_chunk) + '.')\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "print(\"\\nüß† SEMANTIC CHUNKING EXPERIMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test semantic chunking on narrative document (better for this approach)\n",
    "narrative_text = docs[1].page_content\n",
    "\n",
    "semantic_chunks_text = semantic_chunking(\n",
    "    narrative_text, \n",
    "    max_chunk_size=800, \n",
    "    similarity_threshold=0.75\n",
    ")\n",
    "\n",
    "# Convert to LangChain documents\n",
    "semantic_chunks = [\n",
    "    Document(\n",
    "        page_content=chunk,\n",
    "        metadata={\"chunk_type\": \"semantic\", \"chunk_id\": i}\n",
    "    ) \n",
    "    for i, chunk in enumerate(semantic_chunks_text)\n",
    "]\n",
    "\n",
    "semantic_sizes = analyze_chunks(semantic_chunks, \"Semantic Chunking\")\n",
    "\n",
    "# Show topic analysis\n",
    "print(f\"\\nüéØ Semantic Chunk Topics:\")\n",
    "for i, chunk in enumerate(semantic_chunks[:5]):  # Show first 5\n",
    "    first_sentence = chunk.page_content.split('.')[0][:80]\n",
    "    print(f\"   Chunk {i+1}: {first_sentence}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Exercise 5: Comparing Chunking Strategies\n",
    "\n",
    "Let's compare all our chunking approaches side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all methods on the same document\n",
    "test_document = docs[1]  # Use narrative document\n",
    "\n",
    "# 1. Fixed-size chunking\n",
    "fixed_splitter = CharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    "    separator=\"\\n\\n\"\n",
    ")\n",
    "fixed_chunks = fixed_splitter.split_documents([test_document])\n",
    "\n",
    "# 2. Recursive chunking\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "recursive_chunks = recursive_splitter.split_documents([test_document])\n",
    "\n",
    "# 3. Semantic chunking (already done above)\n",
    "# semantic_chunks is already available\n",
    "\n",
    "# Compare results\n",
    "methods = {\n",
    "    \"Fixed-Size\": fixed_chunks,\n",
    "    \"Recursive\": recursive_chunks,\n",
    "    \"Semantic\": semantic_chunks\n",
    "}\n",
    "\n",
    "print(\"üìä CHUNKING STRATEGY COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_data = {}\n",
    "\n",
    "for method, chunks in methods.items():\n",
    "    if chunks:\n",
    "        sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "        \n",
    "        comparison_data[method] = {\n",
    "            'count': len(chunks),\n",
    "            'avg_size': np.mean(sizes),\n",
    "            'std_dev': np.std(sizes),\n",
    "            'min_size': min(sizes),\n",
    "            'max_size': max(sizes)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nüî™ {method}:\")\n",
    "        print(f\"   Chunks: {len(chunks)}\")\n",
    "        print(f\"   Avg size: {np.mean(sizes):.0f} ¬± {np.std(sizes):.0f} chars\")\n",
    "        print(f\"   Range: {min(sizes)} - {max(sizes)} chars\")\n",
    "        \n",
    "        # Calculate consistency score (lower std dev = more consistent)\n",
    "        consistency = 1 - (np.std(sizes) / np.mean(sizes))\n",
    "        print(f\"   Consistency: {consistency:.2f} (higher = more uniform)\")\n",
    "\n",
    "# Visualize comparison\n",
    "methods_list = list(comparison_data.keys())\n",
    "counts = [comparison_data[m]['count'] for m in methods_list]\n",
    "avg_sizes = [comparison_data[m]['avg_size'] for m in methods_list]\n",
    "std_devs = [comparison_data[m]['std_dev'] for m in methods_list]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Chunk count comparison\n",
    "bars1 = ax1.bar(methods_list, counts, color=['lightcoral', 'lightblue', 'lightgreen'])\n",
    "ax1.set_ylabel('Number of Chunks')\n",
    "ax1.set_title('Chunk Count by Method')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Average size with error bars\n",
    "bars2 = ax2.bar(methods_list, avg_sizes, yerr=std_devs, \n",
    "                capsize=10, color=['lightcoral', 'lightblue', 'lightgreen'], alpha=0.7)\n",
    "ax2.set_ylabel('Average Chunk Size (chars)')\n",
    "ax2.set_title('Average Chunk Size ¬± Std Dev')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Key Insights:\")\n",
    "print(\"   ‚Ä¢ Fixed-size: Most predictable, may break context\")\n",
    "print(\"   ‚Ä¢ Recursive: Good balance of size and natural boundaries\")\n",
    "print(\"   ‚Ä¢ Semantic: Most context-aware, variable sizes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Exercise 6: Advanced Chunking with Metadata Preservation\n",
    "\n",
    "Let's see how to preserve important metadata and context across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_chunking_with_metadata(document, chunk_size=600):\n",
    "    \"\"\"\n",
    "    Advanced chunking that preserves metadata and adds context\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents([document])\n",
    "    \n",
    "    # Enhance each chunk with additional metadata\n",
    "    enhanced_chunks = []\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        # Calculate position information\n",
    "        start_char = document.page_content.find(chunk.page_content[:50])\n",
    "        position_ratio = start_char / len(document.page_content) if document.page_content else 0\n",
    "        \n",
    "        # Extract key terms (simple approach)\n",
    "        words = chunk.page_content.lower().split()\n",
    "        word_freq = Counter(words)\n",
    "        key_terms = [word for word, freq in word_freq.most_common(5) \n",
    "                    if len(word) > 4 and word.isalpha()]\n",
    "        \n",
    "        # Create enhanced metadata\n",
    "        enhanced_metadata = chunk.metadata.copy()\n",
    "        enhanced_metadata.update({\n",
    "            'chunk_id': i,\n",
    "            'total_chunks': len(chunks),\n",
    "            'chunk_size': len(chunk.page_content),\n",
    "            'position_in_doc': position_ratio,\n",
    "            'position_label': 'beginning' if position_ratio < 0.33 else \n",
    "                            'middle' if position_ratio < 0.67 else 'end',\n",
    "            'key_terms': key_terms,\n",
    "            'word_count': len(words),\n",
    "            'has_code': '```' in chunk.page_content or 'def ' in chunk.page_content,\n",
    "            'has_headers': any(line.startswith('#') for line in chunk.page_content.split('\\n')),\n",
    "            'next_chunk_preview': chunks[i+1].page_content[:50] + '...' if i < len(chunks)-1 else None,\n",
    "            'prev_chunk_preview': chunks[i-1].page_content[-50:] + '...' if i > 0 else None\n",
    "        })\n",
    "        \n",
    "        enhanced_chunks.append(Document(\n",
    "            page_content=chunk.page_content,\n",
    "            metadata=enhanced_metadata\n",
    "        ))\n",
    "    \n",
    "    return enhanced_chunks\n",
    "\n",
    "# Test advanced chunking\n",
    "print(\"üîß ADVANCED CHUNKING WITH METADATA\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "enhanced_chunks = advanced_chunking_with_metadata(docs[0])  # Technical doc\n",
    "\n",
    "print(f\"üìö Created {len(enhanced_chunks)} enhanced chunks\")\n",
    "\n",
    "# Show detailed metadata for first few chunks\n",
    "for i, chunk in enumerate(enhanced_chunks[:3]):\n",
    "    meta = chunk.metadata\n",
    "    print(f\"\\nüìÑ Chunk {i+1}:\")\n",
    "    print(f\"   Position: {meta['position_label']} ({meta['position_in_doc']:.2f})\")\n",
    "    print(f\"   Size: {meta['chunk_size']} chars, {meta['word_count']} words\")\n",
    "    print(f\"   Key terms: {', '.join(meta['key_terms'][:3])}\")\n",
    "    print(f\"   Has code: {'Yes' if meta['has_code'] else 'No'}\")\n",
    "    print(f\"   Has headers: {'Yes' if meta['has_headers'] else 'No'}\")\n",
    "    \n",
    "    if meta['prev_chunk_preview']:\n",
    "        print(f\"   Previous: ...{meta['prev_chunk_preview']}\")\n",
    "    if meta['next_chunk_preview']:\n",
    "        print(f\"   Next: {meta['next_chunk_preview']}\")\n",
    "    \n",
    "    print(f\"   Content preview: {chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 7: Adaptive Chunking (2025 Technique)\n",
    "\n",
    "Let's implement adaptive chunking that adjusts parameters based on content type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_chunking_strategy(document):\n",
    "    \"\"\"\n",
    "    Adaptive chunking that adjusts parameters based on content analysis\n",
    "    \"\"\"\n",
    "    content = document.page_content\n",
    "    \n",
    "    # Analyze content characteristics\n",
    "    lines = content.split('\\n')\n",
    "    words = content.split()\n",
    "    \n",
    "    # Content type detection\n",
    "    code_indicators = content.count('```') + content.count('def ') + content.count('class ')\n",
    "    header_count = sum(1 for line in lines if line.strip().startswith('#'))\n",
    "    list_items = sum(1 for line in lines if line.strip().startswith(('-', '*', '‚Ä¢')))\n",
    "    avg_line_length = np.mean([len(line) for line in lines if line.strip()])\n",
    "    \n",
    "    # Determine content type and optimal parameters\n",
    "    if code_indicators > 5:\n",
    "        content_type = \"code_heavy\"\n",
    "        chunk_size = 800  # Larger chunks to preserve code context\n",
    "        overlap = 50      # Minimal overlap\n",
    "        separators = [\"\\n\\n\", \"\\ndef \", \"\\nclass \", \"\\n\", \" \"]\n",
    "    elif header_count > 3:\n",
    "        content_type = \"structured_docs\"\n",
    "        chunk_size = 600  # Medium chunks\n",
    "        overlap = 80      # Good overlap\n",
    "        separators = [\"\\n# \", \"\\n## \", \"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    elif list_items > 5:\n",
    "        content_type = \"list_heavy\"\n",
    "        chunk_size = 400  # Smaller chunks\n",
    "        overlap = 60      # Medium overlap\n",
    "        separators = [\"\\n\\n\", \"\\n- \", \"\\n* \", \"\\n\", \". \", \" \"]\n",
    "    elif avg_line_length > 100:\n",
    "        content_type = \"narrative\"\n",
    "        chunk_size = 1000  # Larger chunks for narrative flow\n",
    "        overlap = 150      # High overlap for context\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    else:\n",
    "        content_type = \"general\"\n",
    "        chunk_size = 600   # Default\n",
    "        overlap = 100      # Default\n",
    "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    \n",
    "    # Create splitter with adaptive parameters\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        separators=separators\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents([document])\n",
    "    \n",
    "    # Add adaptive metadata\n",
    "    for chunk in chunks:\n",
    "        chunk.metadata.update({\n",
    "            'adaptive_strategy': content_type,\n",
    "            'chunk_size_used': chunk_size,\n",
    "            'overlap_used': overlap,\n",
    "            'content_analysis': {\n",
    "                'code_indicators': code_indicators,\n",
    "                'header_count': header_count,\n",
    "                'list_items': list_items,\n",
    "                'avg_line_length': round(avg_line_length, 1)\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return chunks, content_type\n",
    "\n",
    "print(\"üéØ ADAPTIVE CHUNKING EXPERIMENT\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Test adaptive chunking on both documents\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_name = [\"Technical (API Docs)\", \"Narrative (AI Healthcare)\"][i]\n",
    "    \n",
    "    adaptive_chunks, detected_type = adaptive_chunking_strategy(doc)\n",
    "    \n",
    "    print(f\"\\nüìÑ {doc_name}:\")\n",
    "    print(f\"   Detected type: {detected_type}\")\n",
    "    print(f\"   Chunks created: {len(adaptive_chunks)}\")\n",
    "    \n",
    "    if adaptive_chunks:\n",
    "        first_chunk_meta = adaptive_chunks[0].metadata\n",
    "        print(f\"   Strategy used:\")\n",
    "        print(f\"     Chunk size: {first_chunk_meta['chunk_size_used']}\")\n",
    "        print(f\"     Overlap: {first_chunk_meta['overlap_used']}\")\n",
    "        \n",
    "        analysis = first_chunk_meta['content_analysis']\n",
    "        print(f\"   Content analysis:\")\n",
    "        print(f\"     Code indicators: {analysis['code_indicators']}\")\n",
    "        print(f\"     Headers: {analysis['header_count']}\")\n",
    "        print(f\"     List items: {analysis['list_items']}\")\n",
    "        print(f\"     Avg line length: {analysis['avg_line_length']}\")\n",
    "        \n",
    "        # Show size distribution\n",
    "        sizes = [len(chunk.page_content) for chunk in adaptive_chunks]\n",
    "        print(f\"   Size distribution: {min(sizes)} - {max(sizes)} chars (avg: {np.mean(sizes):.0f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exercise 8: Chunking Quality Assessment\n",
    "\n",
    "Let's create metrics to evaluate chunking quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chunking_quality(chunks, method_name):\n",
    "    \"\"\"\n",
    "    Comprehensive quality assessment for chunking\n",
    "    \"\"\"\n",
    "    if not chunks:\n",
    "        return {\"error\": \"No chunks to evaluate\"}\n",
    "    \n",
    "    sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    \n",
    "    # 1. Size consistency (lower coefficient of variation = better)\n",
    "    size_consistency = 1 - (np.std(sizes) / np.mean(sizes)) if np.mean(sizes) > 0 else 0\n",
    "    \n",
    "    # 2. Boundary quality (check for broken sentences/words)\n",
    "    broken_boundaries = 0\n",
    "    for chunk in chunks:\n",
    "        content = chunk.page_content.strip()\n",
    "        if content and not content[0].isupper():  # Doesn't start with capital\n",
    "            broken_boundaries += 1\n",
    "        if content and not content.endswith(('.', '!', '?', '\\n')):  # Doesn't end properly\n",
    "            broken_boundaries += 1\n",
    "    \n",
    "    boundary_quality = 1 - (broken_boundaries / (len(chunks) * 2))  # Max 2 issues per chunk\n",
    "    \n",
    "    # 3. Content completeness (no very short chunks)\n",
    "    very_short_chunks = sum(1 for size in sizes if size < 50)\n",
    "    completeness = 1 - (very_short_chunks / len(chunks))\n",
    "    \n",
    "    # 4. Context preservation (simple heuristic)\n",
    "    context_breaks = 0\n",
    "    for i in range(len(chunks) - 1):\n",
    "        current_end = chunks[i].page_content.strip()[-20:]\n",
    "        next_start = chunks[i+1].page_content.strip()[:20:]\n",
    "        \n",
    "        # Check if there's a semantic connection\n",
    "        if not any(word in next_start.lower() for word in current_end.lower().split() if len(word) > 3):\n",
    "            context_breaks += 1\n",
    "    \n",
    "    context_preservation = 1 - (context_breaks / max(1, len(chunks) - 1))\n",
    "    \n",
    "    # 5. Overall quality score\n",
    "    overall_quality = (size_consistency + boundary_quality + completeness + context_preservation) / 4\n",
    "    \n",
    "    return {\n",
    "        'method': method_name,\n",
    "        'chunk_count': len(chunks),\n",
    "        'avg_size': np.mean(sizes),\n",
    "        'size_consistency': size_consistency,\n",
    "        'boundary_quality': boundary_quality,\n",
    "        'completeness': completeness,\n",
    "        'context_preservation': context_preservation,\n",
    "        'overall_quality': overall_quality\n",
    "    }\n",
    "\n",
    "# Evaluate all our chunking methods\n",
    "print(\"üìä CHUNKING QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Prepare test chunks (using narrative document)\n",
    "test_doc = docs[1]\n",
    "\n",
    "# Get chunks from different methods\n",
    "fixed_chunks = CharacterTextSplitter(chunk_size=600, chunk_overlap=100).split_documents([test_doc])\n",
    "recursive_chunks = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100).split_documents([test_doc])\n",
    "adaptive_chunks, _ = adaptive_chunking_strategy(test_doc)\n",
    "\n",
    "methods_to_evaluate = [\n",
    "    (fixed_chunks, \"Fixed-Size\"),\n",
    "    (recursive_chunks, \"Recursive\"),\n",
    "    (semantic_chunks, \"Semantic\"),\n",
    "    (adaptive_chunks, \"Adaptive\")\n",
    "]\n",
    "\n",
    "quality_results = []\n",
    "\n",
    "for chunks, method_name in methods_to_evaluate:\n",
    "    quality = evaluate_chunking_quality(chunks, method_name)\n",
    "    quality_results.append(quality)\n",
    "    \n",
    "    print(f\"\\nüî™ {method_name}:\")\n",
    "    print(f\"   Overall Quality: {quality['overall_quality']:.3f}\")\n",
    "    print(f\"   Size Consistency: {quality['size_consistency']:.3f}\")\n",
    "    print(f\"   Boundary Quality: {quality['boundary_quality']:.3f}\")\n",
    "    print(f\"   Completeness: {quality['completeness']:.3f}\")\n",
    "    print(f\"   Context Preservation: {quality['context_preservation']:.3f}\")\n",
    "\n",
    "# Visualize quality comparison\n",
    "methods = [r['method'] for r in quality_results]\n",
    "overall_scores = [r['overall_quality'] for r in quality_results]\n",
    "metrics = ['size_consistency', 'boundary_quality', 'completeness', 'context_preservation']\n",
    "\n",
    "# Create radar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Overall quality bar chart\n",
    "bars = ax1.bar(methods, overall_scores, color=['lightcoral', 'lightblue', 'lightgreen', 'gold'])\n",
    "ax1.set_ylabel('Overall Quality Score')\n",
    "ax1.set_title('Chunking Methods - Overall Quality')\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, overall_scores):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Detailed metrics comparison\n",
    "x = np.arange(len(methods))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [r[metric] for r in quality_results]\n",
    "    ax2.bar(x + i*width, values, width, label=metric.replace('_', ' ').title())\n",
    "\n",
    "ax2.set_xlabel('Chunking Methods')\n",
    "ax2.set_ylabel('Quality Score')\n",
    "ax2.set_title('Detailed Quality Metrics')\n",
    "ax2.set_xticks(x + width * 1.5)\n",
    "ax2.set_xticklabels(methods, rotation=45)\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax2.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine best method\n",
    "best_method = max(quality_results, key=lambda x: x['overall_quality'])\n",
    "print(f\"\\nüèÜ Best performing method: {best_method['method']} (Quality: {best_method['overall_quality']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### üî™ Chunking Fundamentals:\n",
    "1. **Why chunking matters**: Enables precise retrieval and fits LLM context windows\n",
    "2. **Size trade-offs**: Small chunks = precise but lack context; Large chunks = context but noise\n",
    "3. **Boundary importance**: Natural breaks preserve meaning and readability\n",
    "4. **Overlap strategy**: Helps maintain context across chunk boundaries\n",
    "\n",
    "### üìä Strategy Comparison:\n",
    "- **Fixed-Size**: Simple, predictable, but may break context\n",
    "- **Recursive**: Good balance, respects natural boundaries\n",
    "- **Semantic**: Context-aware, but variable sizes and more complex\n",
    "- **Adaptive**: Optimizes for content type, best overall approach\n",
    "\n",
    "### üöÄ 2025 Best Practices:\n",
    "1. **Semantic chunking** with embeddings for topic-aware splitting\n",
    "2. **Adaptive parameters** based on content type analysis\n",
    "3. **Rich metadata** preservation for better retrieval\n",
    "4. **Quality assessment** metrics to optimize chunking strategies\n",
    "\n",
    "### üéØ Practical Guidelines:\n",
    "- **Technical docs**: Use recursive chunking with code-aware separators\n",
    "- **Narrative text**: Use semantic or adaptive chunking for flow preservation\n",
    "- **Mixed content**: Use adaptive chunking for automatic optimization\n",
    "- **Production systems**: Always include quality assessment and monitoring\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In **Module 4**, we'll explore embedding models that convert our chunks into vectors:\n",
    "- Understanding different embedding models and their characteristics\n",
    "- Model selection criteria for different use cases\n",
    "- Cost vs performance trade-offs\n",
    "- Latest 2025 embedding models and benchmarks\n",
    "\n",
    "Good chunking is the foundation of effective RAG - it directly impacts retrieval quality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. For your use case, which chunking strategy would work best and why?\n",
    "2. How would you handle documents with mixed content types (code + text + tables)?\n",
    "3. What additional quality metrics would be useful for your domain?\n",
    "4. How would you handle multilingual documents or documents with special formatting?\n",
    "\n",
    "## üìù Optional Exercise\n",
    "\n",
    "**Advanced Challenge**: Implement a custom chunking strategy that:\n",
    "1. Detects document sections automatically\n",
    "2. Preserves hierarchical structure (headers, subheaders)\n",
    "3. Handles code blocks specially\n",
    "4. Includes cross-references between related chunks\n",
    "\n",
    "Test it on documents from your domain and compare with the methods in this module!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}