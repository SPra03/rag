{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Document Types & Data Sources\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Identify different document types and their unique challenges\n",
    "- Use LangChain document loaders for various file formats\n",
    "- Compare extraction quality across different methods\n",
    "- Understand when to use traditional vs AI-powered parsing\n",
    "- Handle real-world document processing challenges\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### Why Document Processing Matters\n",
    "In RAG systems, the quality of your document processing directly impacts your final results. **Garbage In = Garbage Out!**\n",
    "\n",
    "### Document Types & Challenges\n",
    "\n",
    "| Document Type | Common Issues | Best Approach |\n",
    "|---------------|---------------|---------------|\n",
    "| **Plain Text** | Encoding, structure | Simple loaders |\n",
    "| **PDF** | Tables, images, layout | AI-powered parsing |\n",
    "| **HTML** | Noise, dynamic content | Smart cleaning |\n",
    "| **CSV/JSON** | Structure preservation | Specialized loaders |\n",
    "| **Images** | OCR accuracy | Multimodal models |\n",
    "| **Code** | Syntax preservation | Language-aware parsing |\n",
    "\n",
    "### 2025 Breakthroughs üöÄ\n",
    "- **LlamaParse**: AI-powered PDF parsing with vision-language models\n",
    "- **Hybrid Multimodal**: Combining traditional + AI approaches\n",
    "- **Markdown Intermediate**: Better structure preservation\n",
    "- **Unstructured Library**: Production-ready document processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "Let's install the required packages for document processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community python-dotenv\n",
    "!pip install -q pypdf beautifulsoup4 pandas requests\n",
    "!pip install -q python-docx openpyxl\n",
    "# For advanced parsing (optional)\n",
    "# !pip install -q unstructured[local-inference] llamaparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.document_loaders import (\n",
    "    TextLoader,\n",
    "    PyPDFLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    CSVLoader,\n",
    "    JSONLoader,\n",
    "    UnstructuredWordDocumentLoader\n",
    ")\n",
    "from langchain.schema import Document\n",
    "\n",
    "# For web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üìÅ Ready to process various document types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÅ Exercise 1: Creating Sample Documents\n",
    "\n",
    "Let's create different types of documents to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample documents directory\n",
    "docs_dir = Path(\"sample_documents\")\n",
    "docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Created directory: {docs_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a plain text document\n",
    "text_content = \"\"\"\n",
    "COMPANY POLICY DOCUMENT\n",
    "=======================\n",
    "\n",
    "Remote Work Policy\n",
    "------------------\n",
    "\n",
    "Effective Date: January 1, 2024\n",
    "\n",
    "Overview:\n",
    "TechCorp Solutions supports flexible work arrangements to promote work-life balance \n",
    "and employee satisfaction while maintaining productivity and collaboration.\n",
    "\n",
    "Eligibility:\n",
    "- All full-time employees after 90-day probationary period\n",
    "- Must have reliable internet connection (minimum 25 Mbps)\n",
    "- Dedicated workspace meeting security requirements\n",
    "\n",
    "Guidelines:\n",
    "1. Maximum 3 remote days per week for hybrid workers\n",
    "2. Core collaboration hours: 10 AM - 3 PM EST\n",
    "3. Weekly team meeting attendance mandatory\n",
    "4. Monthly in-person team building events\n",
    "\n",
    "Equipment:\n",
    "- Company provides laptop, monitor, and basic office supplies\n",
    "- $500 annual stipend for home office improvements\n",
    "- IT support available 9 AM - 6 PM EST\n",
    "\n",
    "Performance Metrics:\n",
    "- Same performance standards apply regardless of location\n",
    "- Weekly check-ins with direct supervisor\n",
    "- Quarterly 360-degree feedback reviews\n",
    "\n",
    "Contact: hr@techcorp.com for questions\n",
    "\"\"\"\n",
    "\n",
    "with open(docs_dir / \"remote_work_policy.txt\", \"w\") as f:\n",
    "    f.write(text_content)\n",
    "\n",
    "print(\"‚úÖ Created: remote_work_policy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create an HTML document\n",
    "html_content = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Product Specifications</title>\n",
    "    <style>\n",
    "        .spec-table { border-collapse: collapse; width: 100%; }\n",
    "        .spec-table th, .spec-table td { border: 1px solid #ddd; padding: 8px; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <header>\n",
    "        <h1>TechCorp Model-X Specifications</h1>\n",
    "        <nav>\n",
    "            <a href=\"#overview\">Overview</a> | \n",
    "            <a href=\"#specs\">Technical Specs</a> | \n",
    "            <a href=\"#pricing\">Pricing</a>\n",
    "        </nav>\n",
    "    </header>\n",
    "    \n",
    "    <main>\n",
    "        <section id=\"overview\">\n",
    "            <h2>Product Overview</h2>\n",
    "            <p>Model-X is our flagship AI-powered analytics platform designed for enterprise customers. \n",
    "            It processes large datasets in real-time and provides actionable insights through \n",
    "            machine learning algorithms.</p>\n",
    "            \n",
    "            <h3>Key Features</h3>\n",
    "            <ul>\n",
    "                <li>Real-time data processing</li>\n",
    "                <li>Advanced machine learning models</li>\n",
    "                <li>Customizable dashboards</li>\n",
    "                <li>API integrations</li>\n",
    "                <li>Enterprise-grade security</li>\n",
    "            </ul>\n",
    "        </section>\n",
    "        \n",
    "        <section id=\"specs\">\n",
    "            <h2>Technical Specifications</h2>\n",
    "            <table class=\"spec-table\">\n",
    "                <tr><th>Component</th><th>Specification</th></tr>\n",
    "                <tr><td>Processing Speed</td><td>1M+ queries per second</td></tr>\n",
    "                <tr><td>Uptime Guarantee</td><td>99.9% SLA</td></tr>\n",
    "                <tr><td>Data Storage</td><td>Unlimited cloud storage</td></tr>\n",
    "                <tr><td>API Rate Limit</td><td>10,000 requests/minute</td></tr>\n",
    "                <tr><td>Supported Formats</td><td>JSON, CSV, XML, Parquet</td></tr>\n",
    "            </table>\n",
    "        </section>\n",
    "        \n",
    "        <section id=\"pricing\">\n",
    "            <h2>Pricing Tiers</h2>\n",
    "            <div class=\"pricing-card\">\n",
    "                <h3>Starter: $99/month</h3>\n",
    "                <p>Up to 100GB data processing, basic analytics</p>\n",
    "            </div>\n",
    "            <div class=\"pricing-card\">\n",
    "                <h3>Professional: $499/month</h3>\n",
    "                <p>Up to 1TB data processing, advanced ML models</p>\n",
    "            </div>\n",
    "            <div class=\"pricing-card\">\n",
    "                <h3>Enterprise: Custom pricing</h3>\n",
    "                <p>Unlimited processing, dedicated support</p>\n",
    "            </div>\n",
    "        </section>\n",
    "    </main>\n",
    "    \n",
    "    <footer>\n",
    "        <p>¬© 2024 TechCorp Solutions. All rights reserved.</p>\n",
    "        <script>\n",
    "            // Some JavaScript that should be ignored\n",
    "            console.log(\"Page loaded\");\n",
    "        </script>\n",
    "    </footer>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open(docs_dir / \"product_specs.html\", \"w\") as f:\n",
    "    f.write(html_content)\n",
    "\n",
    "print(\"‚úÖ Created: product_specs.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a CSV document\n",
    "employee_data = {\n",
    "    'Employee_ID': ['EMP001', 'EMP002', 'EMP003', 'EMP004', 'EMP005'],\n",
    "    'Name': ['Alice Johnson', 'Bob Smith', 'Carol Davis', 'David Wilson', 'Eva Brown'],\n",
    "    'Department': ['Engineering', 'Marketing', 'Sales', 'HR', 'Engineering'],\n",
    "    'Position': ['Senior Developer', 'Marketing Manager', 'Sales Rep', 'HR Specialist', 'DevOps Engineer'],\n",
    "    'Salary': [120000, 85000, 65000, 70000, 110000],\n",
    "    'Hire_Date': ['2020-03-15', '2021-07-20', '2019-11-30', '2022-01-10', '2023-05-08'],\n",
    "    'Remote_Days': [3, 2, 1, 2, 4],\n",
    "    'Performance_Rating': [4.8, 4.2, 3.9, 4.5, 4.7],\n",
    "    'Skills': [\n",
    "        'Python|JavaScript|Docker|AWS',\n",
    "        'SEO|Content Marketing|Analytics|Adobe Creative',\n",
    "        'CRM|Salesforce|Communication|Negotiation',\n",
    "        'Recruitment|Training|Employee Relations|HRIS',\n",
    "        'Kubernetes|CI/CD|Monitoring|Cloud Infrastructure'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(employee_data)\n",
    "df.to_csv(docs_dir / \"employee_data.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Created: employee_data.csv\")\n",
    "print(f\"üìä Contains {len(df)} employee records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a JSON document\n",
    "api_config = {\n",
    "    \"api_version\": \"v2.1\",\n",
    "    \"base_url\": \"https://api.techcorp.com\",\n",
    "    \"authentication\": {\n",
    "        \"type\": \"Bearer Token\",\n",
    "        \"header_name\": \"Authorization\",\n",
    "        \"token_prefix\": \"Bearer \"\n",
    "    },\n",
    "    \"endpoints\": {\n",
    "        \"users\": {\n",
    "            \"path\": \"/users\",\n",
    "            \"methods\": [\"GET\", \"POST\", \"PUT\", \"DELETE\"],\n",
    "            \"rate_limit\": \"1000/hour\",\n",
    "            \"description\": \"Manage user accounts and profiles\"\n",
    "        },\n",
    "        \"analytics\": {\n",
    "            \"path\": \"/analytics\",\n",
    "            \"methods\": [\"GET\", \"POST\"],\n",
    "            \"rate_limit\": \"500/hour\",\n",
    "            \"description\": \"Access analytics data and reports\"\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"path\": \"/data\",\n",
    "            \"methods\": [\"GET\", \"POST\"],\n",
    "            \"rate_limit\": \"100/hour\",\n",
    "            \"description\": \"Upload and retrieve datasets\"\n",
    "        }\n",
    "    },\n",
    "    \"error_codes\": {\n",
    "        \"400\": \"Bad Request - Invalid parameters\",\n",
    "        \"401\": \"Unauthorized - Invalid or missing token\",\n",
    "        \"403\": \"Forbidden - Insufficient permissions\",\n",
    "        \"404\": \"Not Found - Resource does not exist\",\n",
    "        \"429\": \"Too Many Requests - Rate limit exceeded\",\n",
    "        \"500\": \"Internal Server Error - Contact support\"\n",
    "    },\n",
    "    \"supported_formats\": [\"JSON\", \"XML\", \"CSV\"],\n",
    "    \"max_payload_size\": \"10MB\",\n",
    "    \"timeout\": \"30 seconds\"\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(docs_dir / \"api_configuration.json\", \"w\") as f:\n",
    "    json.dump(api_config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Created: api_configuration.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Exercise 2: Loading Documents with LangChain\n",
    "\n",
    "Now let's use LangChain's document loaders to process each document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(documents, doc_type):\n",
    "    \"\"\"Helper function to analyze loaded documents\"\"\"\n",
    "    print(f\"\\nüìÑ {doc_type} Analysis:\")\n",
    "    print(f\"   Number of documents: {len(documents)}\")\n",
    "    \n",
    "    if documents:\n",
    "        total_chars = sum(len(doc.page_content) for doc in documents)\n",
    "        print(f\"   Total characters: {total_chars:,}\")\n",
    "        print(f\"   Average chars per doc: {total_chars // len(documents):,}\")\n",
    "        \n",
    "        # Show first document preview\n",
    "        first_doc = documents[0]\n",
    "        print(f\"   First 200 chars: {first_doc.page_content[:200]}...\")\n",
    "        print(f\"   Metadata: {first_doc.metadata}\")\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Plain Text Document\n",
    "print(\"üîÑ Loading Plain Text Document...\")\n",
    "try:\n",
    "    text_loader = TextLoader(str(docs_dir / \"remote_work_policy.txt\"))\n",
    "    text_docs = text_loader.load()\n",
    "    analyze_document(text_docs, \"Plain Text\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading text: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load HTML Document\n",
    "print(\"\\nüîÑ Loading HTML Document...\")\n",
    "try:\n",
    "    html_loader = UnstructuredHTMLLoader(str(docs_dir / \"product_specs.html\"))\n",
    "    html_docs = html_loader.load()\n",
    "    analyze_document(html_docs, \"HTML\")\n",
    "    \n",
    "    # Let's also see how much content was extracted vs original\n",
    "    with open(docs_dir / \"product_specs.html\", \"r\") as f:\n",
    "        original_html = f.read()\n",
    "    \n",
    "    print(f\"   üéØ Content extraction efficiency:\")\n",
    "    print(f\"      Original HTML: {len(original_html):,} chars\")\n",
    "    print(f\"      Extracted text: {len(html_docs[0].page_content):,} chars\")\n",
    "    print(f\"      Efficiency: {len(html_docs[0].page_content)/len(original_html)*100:.1f}% text extracted\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading HTML: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load CSV Document\n",
    "print(\"\\nüîÑ Loading CSV Document...\")\n",
    "try:\n",
    "    csv_loader = CSVLoader(str(docs_dir / \"employee_data.csv\"))\n",
    "    csv_docs = csv_loader.load()\n",
    "    analyze_document(csv_docs, \"CSV\")\n",
    "    \n",
    "    # CSV creates one document per row by default\n",
    "    print(f\"   üìä CSV Processing Details:\")\n",
    "    print(f\"      Each row becomes a separate document\")\n",
    "    print(f\"      Sample document content:\")\n",
    "    print(f\"      {csv_docs[0].page_content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading CSV: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load JSON Document\n",
    "print(\"\\nüîÑ Loading JSON Document...\")\n",
    "try:\n",
    "    # JSONLoader requires a jq_schema to specify what to extract\n",
    "    json_loader = JSONLoader(\n",
    "        file_path=str(docs_dir / \"api_configuration.json\"),\n",
    "        jq_schema=\".\",  # Extract everything\n",
    "        text_content=False\n",
    "    )\n",
    "    json_docs = json_loader.load()\n",
    "    analyze_document(json_docs, \"JSON\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading JSON: {e}\")\n",
    "    \n",
    "    # Alternative approach for JSON\n",
    "    print(\"\\nüîÑ Trying alternative JSON loading...\")\n",
    "    with open(docs_dir / \"api_configuration.json\", \"r\") as f:\n",
    "        json_content = json.load(f)\n",
    "    \n",
    "    # Convert JSON to readable text format\n",
    "    json_text = json.dumps(json_content, indent=2)\n",
    "    json_doc = Document(\n",
    "        page_content=json_text,\n",
    "        metadata={\"source\": \"api_configuration.json\", \"type\": \"json\"}\n",
    "    )\n",
    "    \n",
    "    analyze_document([json_doc], \"JSON (Manual)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üåê Exercise 3: Web Content Loading\n",
    "\n",
    "Let's try loading content from web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple web scraping function\n",
    "def scrape_web_content(url, max_chars=2000):\n",
    "    \"\"\"\n",
    "    Simple web scraping with error handling\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"üåê Scraping: {url}\")\n",
    "        \n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Remove script and style elements\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.decompose()\n",
    "        \n",
    "        # Get text content\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        # Limit text length for demo\n",
    "        if len(text) > max_chars:\n",
    "            text = text[:max_chars] + \"...\"\n",
    "        \n",
    "        # Create LangChain document\n",
    "        doc = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": url,\n",
    "                \"title\": soup.title.string if soup.title else \"No title\",\n",
    "                \"type\": \"web_page\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return [doc]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error scraping {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test web scraping with a simple page\n",
    "demo_urls = [\n",
    "    \"https://httpbin.org/html\",  # Simple test page\n",
    "    \"https://example.com\",      # Basic example page\n",
    "]\n",
    "\n",
    "for url in demo_urls:\n",
    "    web_docs = scrape_web_content(url)\n",
    "    if web_docs:\n",
    "        analyze_document(web_docs, f\"Web Content ({url})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exercise 4: Comparing Document Processing Quality\n",
    "\n",
    "Let's compare how well different approaches extract meaningful content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extraction_quality(documents, doc_type):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of document extraction\n",
    "    \"\"\"\n",
    "    if not documents:\n",
    "        return {\"quality_score\": 0, \"issues\": [\"No documents loaded\"]}\n",
    "    \n",
    "    doc = documents[0]  # Analyze first document\n",
    "    content = doc.page_content\n",
    "    issues = []\n",
    "    \n",
    "    # Check for common issues\n",
    "    if len(content.strip()) < 50:\n",
    "        issues.append(\"Very short content\")\n",
    "    \n",
    "    if content.count('\\n\\n\\n') > 5:\n",
    "        issues.append(\"Too many empty lines\")\n",
    "    \n",
    "    if len(content.split()) < 10:\n",
    "        issues.append(\"Very few words extracted\")\n",
    "    \n",
    "    # Check for HTML artifacts\n",
    "    html_artifacts = ['<', '>', '&nbsp;', '&amp;', '\\xa0']\n",
    "    if any(artifact in content for artifact in html_artifacts):\n",
    "        issues.append(\"Contains HTML artifacts\")\n",
    "    \n",
    "    # Calculate quality score (simple heuristic)\n",
    "    word_count = len(content.split())\n",
    "    char_count = len(content)\n",
    "    \n",
    "    quality_score = min(100, (\n",
    "        (word_count / 10) +  # More words = better\n",
    "        (char_count / 100) + # More chars = better\n",
    "        (100 - len(issues) * 20)  # Fewer issues = better\n",
    "    ) / 3)\n",
    "    \n",
    "    return {\n",
    "        \"quality_score\": max(0, quality_score),\n",
    "        \"word_count\": word_count,\n",
    "        \"char_count\": char_count,\n",
    "        \"issues\": issues\n",
    "    }\n",
    "\n",
    "# Evaluate all our loaded documents\n",
    "document_sets = [\n",
    "    (text_docs, \"Plain Text\"),\n",
    "    (html_docs, \"HTML\"),\n",
    "    (csv_docs, \"CSV\"),\n",
    "    ([json_doc] if 'json_doc' in locals() else [], \"JSON\")\n",
    "]\n",
    "\n",
    "print(\"üìä DOCUMENT PROCESSING QUALITY REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for docs, doc_type in document_sets:\n",
    "    if docs:\n",
    "        quality = evaluate_extraction_quality(docs, doc_type)\n",
    "        \n",
    "        print(f\"\\nüìÑ {doc_type}:\")\n",
    "        print(f\"   Quality Score: {quality['quality_score']:.1f}/100\")\n",
    "        print(f\"   Words: {quality['word_count']:,}\")\n",
    "        print(f\"   Characters: {quality['char_count']:,}\")\n",
    "        \n",
    "        if quality['issues']:\n",
    "            print(f\"   Issues: {', '.join(quality['issues'])}\")\n",
    "        else:\n",
    "            print(f\"   Issues: None ‚úÖ\")\n",
    "    else:\n",
    "        print(f\"\\nüìÑ {doc_type}: No documents to evaluate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Exercise 5: Advanced Document Processing (2025 Techniques)\n",
    "\n",
    "Let's explore some advanced document processing techniques that are popular in 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smart text cleaning and preprocessing\n",
    "import re\n",
    "\n",
    "def smart_text_cleaner(text):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning using 2025 best practices\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace but preserve paragraph breaks\n",
    "    text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)  # Normalize paragraph breaks\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)      # Normalize spaces\n",
    "    \n",
    "    # Remove common artifacts\n",
    "    text = re.sub(r'\\xa0', ' ', text)        # Non-breaking spaces\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\xff]', '', text)  # Control chars\n",
    "    \n",
    "    # Smart handling of bullet points and lists\n",
    "    text = re.sub(r'^\\s*[‚Ä¢\\-\\*]\\s+', '- ', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Clean up multiple consecutive punctuation\n",
    "    text = re.sub(r'[.]{3,}', '...', text)\n",
    "    text = re.sub(r'[!]{2,}', '!', text)\n",
    "    text = re.sub(r'[?]{2,}', '?', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def create_enhanced_document(original_doc, cleaning_type=\"standard\"):\n",
    "    \"\"\"\n",
    "    Create an enhanced version of a document with better processing\n",
    "    \"\"\"\n",
    "    content = original_doc.page_content\n",
    "    \n",
    "    if cleaning_type == \"aggressive\":\n",
    "        content = smart_text_cleaner(content)\n",
    "    \n",
    "    # Add processing metadata\n",
    "    enhanced_metadata = original_doc.metadata.copy()\n",
    "    enhanced_metadata.update({\n",
    "        \"processed\": True,\n",
    "        \"cleaning_type\": cleaning_type,\n",
    "        \"original_length\": len(original_doc.page_content),\n",
    "        \"processed_length\": len(content),\n",
    "        \"compression_ratio\": len(content) / len(original_doc.page_content) if original_doc.page_content else 0\n",
    "    })\n",
    "    \n",
    "    return Document(\n",
    "        page_content=content,\n",
    "        metadata=enhanced_metadata\n",
    "    )\n",
    "\n",
    "# Test enhanced processing\n",
    "print(\"üîß ENHANCED DOCUMENT PROCESSING\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "if html_docs:\n",
    "    original_html = html_docs[0]\n",
    "    enhanced_html = create_enhanced_document(original_html, \"aggressive\")\n",
    "    \n",
    "    print(f\"\\nüìÑ HTML Document Enhancement:\")\n",
    "    print(f\"   Original length: {enhanced_html.metadata['original_length']:,} chars\")\n",
    "    print(f\"   Processed length: {enhanced_html.metadata['processed_length']:,} chars\")\n",
    "    print(f\"   Compression ratio: {enhanced_html.metadata['compression_ratio']:.2f}\")\n",
    "    \n",
    "    print(f\"\\nüìù Content comparison:\")\n",
    "    print(f\"   Original preview: {original_html.page_content[:150]}...\")\n",
    "    print(f\"   Enhanced preview: {enhanced_html.page_content[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Exercise 6: Document Metadata Enrichment\n",
    "\n",
    "Let's add useful metadata to our documents for better RAG performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import hashlib\n",
    "\n",
    "def enrich_document_metadata(doc, additional_info=None):\n",
    "    \"\"\"\n",
    "    Add comprehensive metadata to documents\n",
    "    \"\"\"\n",
    "    content = doc.page_content\n",
    "    \n",
    "    # Calculate content statistics\n",
    "    words = content.split()\n",
    "    sentences = content.split('.')\n",
    "    \n",
    "    # Create content hash for deduplication\n",
    "    content_hash = hashlib.md5(content.encode()).hexdigest()[:16]\n",
    "    \n",
    "    # Analyze content type\n",
    "    content_type = \"general\"\n",
    "    if any(keyword in content.lower() for keyword in ['policy', 'procedure', 'guideline']):\n",
    "        content_type = \"policy\"\n",
    "    elif any(keyword in content.lower() for keyword in ['spec', 'technical', 'api', 'configuration']):\n",
    "        content_type = \"technical\"\n",
    "    elif any(keyword in content.lower() for keyword in ['employee', 'salary', 'department']):\n",
    "        content_type = \"hr_data\"\n",
    "    \n",
    "    # Enhance metadata\n",
    "    enriched_metadata = doc.metadata.copy()\n",
    "    enriched_metadata.update({\n",
    "        # Content statistics\n",
    "        \"word_count\": len(words),\n",
    "        \"sentence_count\": len([s for s in sentences if s.strip()]),\n",
    "        \"character_count\": len(content),\n",
    "        \"avg_word_length\": sum(len(word) for word in words) / len(words) if words else 0,\n",
    "        \n",
    "        # Content identification\n",
    "        \"content_hash\": content_hash,\n",
    "        \"content_type\": content_type,\n",
    "        \n",
    "        # Processing metadata\n",
    "        \"processed_at\": datetime.now().isoformat(),\n",
    "        \"extraction_method\": \"langchain\",\n",
    "        \n",
    "        # Quality indicators\n",
    "        \"has_structure\": \"\\n\\n\" in content or \"\\n-\" in content,\n",
    "        \"readability_score\": min(10, len(words) / max(1, len(sentences)) * 2),  # Simple readability\n",
    "    })\n",
    "    \n",
    "    # Add any additional info\n",
    "    if additional_info:\n",
    "        enriched_metadata.update(additional_info)\n",
    "    \n",
    "    return Document(\n",
    "        page_content=content,\n",
    "        metadata=enriched_metadata\n",
    "    )\n",
    "\n",
    "# Enrich all our documents\n",
    "enriched_docs = []\n",
    "\n",
    "# Process each document type\n",
    "if text_docs:\n",
    "    enriched_text = enrich_document_metadata(\n",
    "        text_docs[0], \n",
    "        {\"department\": \"HR\", \"confidentiality\": \"internal\"}\n",
    "    )\n",
    "    enriched_docs.append((enriched_text, \"Policy Document\"))\n",
    "\n",
    "if html_docs:\n",
    "    enriched_html = enrich_document_metadata(\n",
    "        html_docs[0], \n",
    "        {\"department\": \"Product\", \"public\": True}\n",
    "    )\n",
    "    enriched_docs.append((enriched_html, \"Product Specs\"))\n",
    "\n",
    "print(\"üîç ENRICHED DOCUMENT METADATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for doc, doc_name in enriched_docs:\n",
    "    print(f\"\\nüìÑ {doc_name}:\")\n",
    "    \n",
    "    # Display key metadata\n",
    "    meta = doc.metadata\n",
    "    print(f\"   Content Type: {meta.get('content_type', 'unknown')}\")\n",
    "    print(f\"   Word Count: {meta.get('word_count', 0):,}\")\n",
    "    print(f\"   Readability Score: {meta.get('readability_score', 0):.1f}/10\")\n",
    "    print(f\"   Has Structure: {'Yes' if meta.get('has_structure') else 'No'}\")\n",
    "    print(f\"   Content Hash: {meta.get('content_hash', 'unknown')}\")\n",
    "    \n",
    "    if 'department' in meta:\n",
    "        print(f\"   Department: {meta['department']}\")\n",
    "    if 'confidentiality' in meta:\n",
    "        print(f\"   Confidentiality: {meta['confidentiality']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### üìÑ Document Processing Insights:\n",
    "1. **Different formats require different approaches** - CSV creates multiple docs, HTML needs cleaning, JSON needs structure preservation\n",
    "2. **Metadata is crucial** - Rich metadata enables better filtering and relevance in RAG systems\n",
    "3. **Quality varies significantly** - Some formats extract cleaner text than others\n",
    "4. **Processing choices matter** - Aggressive cleaning vs. structure preservation trade-offs\n",
    "\n",
    "### üõ†Ô∏è LangChain Document Loaders:\n",
    "- **TextLoader**: Simple and reliable for plain text\n",
    "- **PyPDFLoader**: Good for basic PDFs (we'll cover advanced PDF parsing later)\n",
    "- **UnstructuredHTMLLoader**: Effectively removes HTML markup\n",
    "- **CSVLoader**: Creates one document per row by default\n",
    "- **JSONLoader**: Requires jq schema for complex JSON structures\n",
    "\n",
    "### üöÄ 2025 Best Practices:\n",
    "1. **Smart text cleaning** - Remove artifacts while preserving structure\n",
    "2. **Comprehensive metadata** - Add processing info, content type, quality metrics\n",
    "3. **Content hashing** - Enable deduplication and change detection\n",
    "4. **Quality assessment** - Automated evaluation of extraction quality\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In **Module 3**, we'll learn how to break these documents into optimal chunks for RAG systems:\n",
    "- Why chunking is necessary\n",
    "- Different chunking strategies (fixed-size, semantic, adaptive)\n",
    "- How to preserve context while splitting documents\n",
    "- Handling overlap and metadata propagation\n",
    "\n",
    "The quality of your document processing directly impacts your RAG system's performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. Which document type presented the biggest processing challenges and why?\n",
    "2. How would you handle documents with mixed content (text + tables + images)?\n",
    "3. What additional metadata would be useful for your specific use case?\n",
    "4. How would you handle document updates and versioning in a RAG system?\n",
    "\n",
    "## üìù Optional Exercise\n",
    "\n",
    "Try processing documents from your own domain:\n",
    "1. Find 2-3 documents in different formats from your work or projects\n",
    "2. Process them using the techniques from this module\n",
    "3. Evaluate the extraction quality and identify any domain-specific challenges\n",
    "4. Create appropriate metadata schemas for your document types"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}