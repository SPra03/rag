{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 11: LLM Integration and Model Selection for RAG\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand different LLM categories and their RAG suitability\n",
    "- Learn practical integration techniques with LangChain\n",
    "- Implement cost-performance optimization strategies\n",
    "- Build multi-model RAG systems\n",
    "- Handle model-specific requirements and limitations\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. LLM Categories for RAG Systems\n",
    "\n",
    "#### API-Based Models (Recommended for Production)\n",
    "- **GPT-4/GPT-4 Turbo**: Excellent reasoning, complex queries\n",
    "- **GPT-3.5-Turbo**: Fast, cost-effective for simple RAG tasks\n",
    "- **Claude-3 (Opus/Sonnet/Haiku)**: Strong analytical capabilities\n",
    "- **Gemini Pro/Ultra**: Google's competitive offering\n",
    "\n",
    "#### Open Source Models (Self-Hosted)\n",
    "- **Llama 2/3**: Meta's open models, various sizes\n",
    "- **Mistral 7B/8x7B**: Efficient European alternative\n",
    "- **Code Llama**: Specialized for code-related RAG\n",
    "- **Falcon**: UAE's competitive open model\n",
    "\n",
    "### 2. RAG-Specific Model Considerations\n",
    "\n",
    "#### Context Window Size\n",
    "- **GPT-4 Turbo**: 128K tokens\n",
    "- **Claude-3**: 200K tokens\n",
    "- **Gemini Pro**: 1M tokens (limited preview)\n",
    "- **Open Source**: Usually 2K-32K tokens\n",
    "\n",
    "#### Instruction Following\n",
    "- Critical for RAG prompt adherence\n",
    "- Citation generation accuracy\n",
    "- Context utilization efficiency\n",
    "\n",
    "#### Cost Structure (2025 Pricing)\n",
    "- **Input tokens**: Usually cheaper (context)\n",
    "- **Output tokens**: More expensive (generation)\n",
    "- **RAG impact**: High input/output ratio\n",
    "\n",
    "### 3. Multi-Model Strategies\n",
    "\n",
    "#### Router-Based Approach\n",
    "- Simple queries â†’ Cheaper model\n",
    "- Complex queries â†’ Premium model\n",
    "- Code queries â†’ Specialized model\n",
    "\n",
    "#### Fallback Systems\n",
    "- Primary model fails â†’ Secondary model\n",
    "- Rate limits â†’ Alternative provider\n",
    "- Cost optimization â†’ Model switching\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-openai langchain-anthropic langchain-google-genai\n",
    "!pip install langchain-community tiktoken chromadb\n",
    "!pip install openai anthropic google-generativeai\n",
    "!pip install python-dotenv pandas matplotlib seaborn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Model Performance Comparison\n",
    "\n",
    "Let's create a comprehensive system to compare different LLMs for RAG tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelMetrics:\n",
    "    \"\"\"Store performance metrics for each model\"\"\"\n",
    "    model_name: str\n",
    "    response_time: float\n",
    "    tokens_used: int\n",
    "    cost: float\n",
    "    quality_score: float\n",
    "    context_utilization: float\n",
    "    citation_accuracy: float\n",
    "\n",
    "class LLMComparator:\n",
    "    \"\"\"Compare different LLMs for RAG tasks\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.test_results = []\n",
    "        self.setup_models()\n",
    "    \n",
    "    def setup_models(self):\n",
    "        \"\"\"Initialize different LLM models\"\"\"\n",
    "        try:\n",
    "            # OpenAI Models\n",
    "            if os.getenv(\"OPENAI_API_KEY\"):\n",
    "                self.models['gpt-4-turbo'] = ChatOpenAI(\n",
    "                    model=\"gpt-4-turbo-preview\",\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                self.models['gpt-3.5-turbo'] = ChatOpenAI(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    temperature=0.1\n",
    "                )\n",
    "            \n",
    "            # Anthropic Models\n",
    "            if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "                self.models['claude-3-opus'] = ChatAnthropic(\n",
    "                    model=\"claude-3-opus-20240229\",\n",
    "                    temperature=0.1\n",
    "                )\n",
    "                self.models['claude-3-sonnet'] = ChatAnthropic(\n",
    "                    model=\"claude-3-sonnet-20240229\",\n",
    "                    temperature=0.1\n",
    "                )\n",
    "            \n",
    "            # Google Models\n",
    "            if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "                self.models['gemini-pro'] = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-pro\",\n",
    "                    temperature=0.1\n",
    "                )\n",
    "            \n",
    "            print(f\"âœ… Initialized {len(self.models)} models\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Some models failed to initialize: {e}\")\n",
    "            print(\"ðŸ’¡ Make sure to set your API keys in .env file\")\n",
    "    \n",
    "    def create_rag_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"Create standardized RAG prompt\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are a helpful assistant that answers questions based on the provided context.\n",
    "Use ONLY the information from the context to answer the question.\n",
    "If the context doesn't contain enough information, say \"I don't have enough information to answer this question.\"\n",
    "Always cite specific parts of the context in your answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def evaluate_response_quality(self, response: str, expected_keywords: List[str]) -> float:\n",
    "        \"\"\"Simple quality evaluation based on keyword presence\"\"\"\n",
    "        score = 0.0\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        for keyword in expected_keywords:\n",
    "            if keyword.lower() in response_lower:\n",
    "                score += 1.0\n",
    "        \n",
    "        return min(score / len(expected_keywords), 1.0) if expected_keywords else 0.0\n",
    "    \n",
    "    def check_citation_accuracy(self, response: str, context: str) -> float:\n",
    "        \"\"\"Check if citations are accurate\"\"\"\n",
    "        # Simple heuristic: check if quoted text appears in context\n",
    "        import re\n",
    "        quotes = re.findall(r'\"([^\"]+)\"', response)\n",
    "        \n",
    "        if not quotes:\n",
    "            return 0.5  # No citations, neutral score\n",
    "        \n",
    "        accurate_citations = 0\n",
    "        for quote in quotes:\n",
    "            if quote.lower() in context.lower():\n",
    "                accurate_citations += 1\n",
    "        \n",
    "        return accurate_citations / len(quotes)\n",
    "    \n",
    "    def test_model(self, model_name: str, query: str, context: str, \n",
    "                  expected_keywords: List[str]) -> ModelMetrics:\n",
    "        \"\"\"Test a single model and return metrics\"\"\"\n",
    "        \n",
    "        if model_name not in self.models:\n",
    "            print(f\"âš ï¸ Model {model_name} not available\")\n",
    "            return None\n",
    "        \n",
    "        model = self.models[model_name]\n",
    "        prompt = self.create_rag_prompt(query, context)\n",
    "        \n",
    "        # Measure response time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if 'gpt' in model_name:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = model.invoke([HumanMessage(content=prompt)])\n",
    "                    response_text = response.content\n",
    "                    tokens_used = cb.total_tokens\n",
    "                    cost = cb.total_cost\n",
    "            else:\n",
    "                response = model.invoke([HumanMessage(content=prompt)])\n",
    "                response_text = response.content\n",
    "                tokens_used = len(prompt.split()) + len(response_text.split())  # Rough estimate\n",
    "                cost = self.estimate_cost(model_name, tokens_used)\n",
    "            \n",
    "            response_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluate quality metrics\n",
    "            quality_score = self.evaluate_response_quality(response_text, expected_keywords)\n",
    "            citation_accuracy = self.check_citation_accuracy(response_text, context)\n",
    "            context_utilization = min(len(response_text) / len(context), 1.0)\n",
    "            \n",
    "            metrics = ModelMetrics(\n",
    "                model_name=model_name,\n",
    "                response_time=response_time,\n",
    "                tokens_used=tokens_used,\n",
    "                cost=cost,\n",
    "                quality_score=quality_score,\n",
    "                context_utilization=context_utilization,\n",
    "                citation_accuracy=citation_accuracy\n",
    "            )\n",
    "            \n",
    "            print(f\"âœ… {model_name}: {response_time:.2f}s, Quality: {quality_score:.2f}\")\n",
    "            return metrics\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error testing {model_name}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def estimate_cost(self, model_name: str, tokens: int) -> float:\n",
    "        \"\"\"Estimate cost based on 2025 pricing (approximate)\"\"\"\n",
    "        pricing = {\n",
    "            'claude-3-opus': 0.015,      # per 1K tokens\n",
    "            'claude-3-sonnet': 0.003,\n",
    "            'gemini-pro': 0.0005,\n",
    "            'gpt-4-turbo': 0.01,\n",
    "            'gpt-3.5-turbo': 0.0015\n",
    "        }\n",
    "        \n",
    "        rate = pricing.get(model_name, 0.001)\n",
    "        return (tokens / 1000) * rate\n",
    "    \n",
    "    def run_comparison(self, test_cases: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Run comparison across all models and test cases\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases):\n",
    "            print(f\"\\nðŸ§ª Test Case {i+1}: {test_case['query'][:50]}...\")\n",
    "            \n",
    "            for model_name in self.models.keys():\n",
    "                metrics = self.test_model(\n",
    "                    model_name,\n",
    "                    test_case['query'],\n",
    "                    test_case['context'],\n",
    "                    test_case['expected_keywords']\n",
    "                )\n",
    "                \n",
    "                if metrics:\n",
    "                    result_dict = {\n",
    "                        'test_case': f\"Test {i+1}\",\n",
    "                        'model': metrics.model_name,\n",
    "                        'response_time': metrics.response_time,\n",
    "                        'tokens_used': metrics.tokens_used,\n",
    "                        'cost': metrics.cost,\n",
    "                        'quality_score': metrics.quality_score,\n",
    "                        'citation_accuracy': metrics.citation_accuracy,\n",
    "                        'context_utilization': metrics.context_utilization\n",
    "                    }\n",
    "                    results.append(result_dict)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize comparator\n",
    "comparator = LLMComparator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test cases for model comparison\n",
    "test_cases = [\n",
    "    {\n",
    "        'query': 'What are the main benefits of renewable energy?',\n",
    "        'context': '''\n",
    "        Renewable energy sources offer numerous advantages over fossil fuels. \n",
    "        First, they significantly reduce greenhouse gas emissions, helping combat climate change. \n",
    "        Solar and wind power produce no direct carbon emissions during operation. \n",
    "        Second, renewable energy sources are inexhaustible - the sun and wind will continue \n",
    "        to provide energy for billions of years. Third, they reduce dependence on imported \n",
    "        fossil fuels, enhancing energy security. Fourth, renewable energy creates jobs in \n",
    "        manufacturing, installation, and maintenance sectors. Finally, operating costs are \n",
    "        typically lower than fossil fuels once infrastructure is in place.\n",
    "        ''',\n",
    "        'expected_keywords': ['greenhouse gas', 'climate change', 'inexhaustible', 'energy security', 'jobs']\n",
    "    },\n",
    "    {\n",
    "        'query': 'How does machine learning work in recommendation systems?',\n",
    "        'context': '''\n",
    "        Machine learning powers modern recommendation systems through several approaches. \n",
    "        Collaborative filtering analyzes user behavior patterns to find similar users or items. \n",
    "        Content-based filtering recommends items similar to those a user previously liked. \n",
    "        Matrix factorization techniques decompose user-item interaction matrices to discover \n",
    "        latent features. Deep learning models can capture complex non-linear patterns in \n",
    "        user preferences. Hybrid systems combine multiple approaches for better accuracy. \n",
    "        Real-time learning allows systems to adapt quickly to changing user preferences.\n",
    "        ''',\n",
    "        'expected_keywords': ['collaborative filtering', 'content-based', 'matrix factorization', 'deep learning', 'hybrid']\n",
    "    },\n",
    "    {\n",
    "        'query': 'What are the key principles of sustainable agriculture?',\n",
    "        'context': '''\n",
    "        Sustainable agriculture focuses on meeting current food needs while preserving \n",
    "        resources for future generations. Key principles include soil health management \n",
    "        through crop rotation, cover cropping, and minimal tillage. Water conservation \n",
    "        involves efficient irrigation systems and drought-resistant crops. Biodiversity \n",
    "        preservation includes maintaining diverse crop varieties and supporting beneficial \n",
    "        insects. Integrated pest management reduces chemical pesticide use through \n",
    "        biological controls. Economic viability ensures farmers can maintain profitable \n",
    "        operations while following sustainable practices.\n",
    "        ''',\n",
    "        'expected_keywords': ['soil health', 'water conservation', 'biodiversity', 'pest management', 'economic viability']\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‹ Created {len(test_cases)} test cases for model comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparison (this may take a few minutes)\n",
    "print(\"ðŸš€ Starting model comparison...\")\n",
    "results_df = comparator.run_comparison(test_cases)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"\\nðŸ“Š Results Summary:\")\n",
    "    print(results_df.groupby('model').agg({\n",
    "        'response_time': 'mean',\n",
    "        'cost': 'mean',\n",
    "        'quality_score': 'mean',\n",
    "        'citation_accuracy': 'mean'\n",
    "    }).round(4))\n",
    "else:\n",
    "    print(\"âŒ No results generated. Please check your API keys.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Cost-Performance Optimization\n",
    "\n",
    "Let's build a system that automatically selects the best model based on cost and performance requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGOptimizer:\n",
    "    \"\"\"Optimize model selection for RAG tasks based on requirements\"\"\"\n",
    "    \n",
    "    def __init__(self, comparator: LLMComparator):\n",
    "        self.comparator = comparator\n",
    "        self.model_profiles = self.create_model_profiles()\n",
    "    \n",
    "    def create_model_profiles(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Create performance profiles for each model\"\"\"\n",
    "        profiles = {\n",
    "            'gpt-4-turbo': {\n",
    "                'speed': 0.6,        # Relative speed (0-1)\n",
    "                'quality': 0.95,     # Quality score (0-1)\n",
    "                'cost': 0.8,         # Cost factor (0-1, higher = more expensive)\n",
    "                'context_window': 128000,\n",
    "                'specialties': ['complex reasoning', 'analysis', 'code'],\n",
    "                'best_for': 'high-quality responses, complex queries'\n",
    "            },\n",
    "            'gpt-3.5-turbo': {\n",
    "                'speed': 0.9,\n",
    "                'quality': 0.8,\n",
    "                'cost': 0.3,\n",
    "                'context_window': 16000,\n",
    "                'specialties': ['general queries', 'speed'],\n",
    "                'best_for': 'fast responses, simple to medium complexity'\n",
    "            },\n",
    "            'claude-3-opus': {\n",
    "                'speed': 0.5,\n",
    "                'quality': 0.98,\n",
    "                'cost': 0.9,\n",
    "                'context_window': 200000,\n",
    "                'specialties': ['analysis', 'reasoning', 'long context'],\n",
    "                'best_for': 'highest quality, complex analysis'\n",
    "            },\n",
    "            'claude-3-sonnet': {\n",
    "                'speed': 0.7,\n",
    "                'quality': 0.85,\n",
    "                'cost': 0.4,\n",
    "                'context_window': 200000,\n",
    "                'specialties': ['balanced performance', 'long context'],\n",
    "                'best_for': 'balanced cost-quality, long documents'\n",
    "            },\n",
    "            'gemini-pro': {\n",
    "                'speed': 0.8,\n",
    "                'quality': 0.82,\n",
    "                'cost': 0.2,\n",
    "                'context_window': 30000,\n",
    "                'specialties': ['multimodal', 'cost-effective'],\n",
    "                'best_for': 'cost-effective, multimodal tasks'\n",
    "            }\n",
    "        }\n",
    "        return profiles\n",
    "    \n",
    "    def calculate_suitability_score(self, model_name: str, requirements: Dict) -> float:\n",
    "        \"\"\"Calculate how suitable a model is for given requirements\"\"\"\n",
    "        if model_name not in self.model_profiles:\n",
    "            return 0.0\n",
    "        \n",
    "        profile = self.model_profiles[model_name]\n",
    "        \n",
    "        # Weight factors based on requirements\n",
    "        speed_weight = requirements.get('speed_priority', 0.3)\n",
    "        quality_weight = requirements.get('quality_priority', 0.4)\n",
    "        cost_weight = requirements.get('cost_priority', 0.3)\n",
    "        \n",
    "        # Calculate weighted score\n",
    "        speed_score = profile['speed'] * speed_weight\n",
    "        quality_score = profile['quality'] * quality_weight\n",
    "        cost_score = (1 - profile['cost']) * cost_weight  # Invert cost (lower cost = higher score)\n",
    "        \n",
    "        total_score = speed_score + quality_score + cost_score\n",
    "        \n",
    "        # Apply context window requirement\n",
    "        required_context = requirements.get('context_length', 0)\n",
    "        if required_context > profile['context_window']:\n",
    "            total_score *= 0.1  # Heavy penalty for insufficient context window\n",
    "        \n",
    "        return total_score\n",
    "    \n",
    "    def recommend_model(self, requirements: Dict) -> Tuple[str, Dict]:\n",
    "        \"\"\"Recommend the best model based on requirements\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for model_name in self.model_profiles.keys():\n",
    "            if model_name in self.comparator.models:\n",
    "                score = self.calculate_suitability_score(model_name, requirements)\n",
    "                scores[model_name] = score\n",
    "        \n",
    "        if not scores:\n",
    "            return None, {}\n",
    "        \n",
    "        best_model = max(scores.keys(), key=lambda k: scores[k])\n",
    "        recommendation = {\n",
    "            'model': best_model,\n",
    "            'score': scores[best_model],\n",
    "            'profile': self.model_profiles[best_model],\n",
    "            'all_scores': scores\n",
    "        }\n",
    "        \n",
    "        return best_model, recommendation\n",
    "    \n",
    "    def create_optimization_report(self, scenarios: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Create optimization recommendations for different scenarios\"\"\"\n",
    "        reports = []\n",
    "        \n",
    "        for scenario in scenarios:\n",
    "            best_model, recommendation = self.recommend_model(scenario['requirements'])\n",
    "            \n",
    "            if best_model:\n",
    "                reports.append({\n",
    "                    'scenario': scenario['name'],\n",
    "                    'recommended_model': best_model,\n",
    "                    'score': recommendation['score'],\n",
    "                    'reason': recommendation['profile']['best_for'],\n",
    "                    'estimated_cost': self.estimate_scenario_cost(best_model, scenario),\n",
    "                    'speed': recommendation['profile']['speed'],\n",
    "                    'quality': recommendation['profile']['quality']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(reports)\n",
    "    \n",
    "    def estimate_scenario_cost(self, model_name: str, scenario: Dict) -> float:\n",
    "        \"\"\"Estimate cost for a scenario\"\"\"\n",
    "        monthly_queries = scenario.get('monthly_queries', 1000)\n",
    "        avg_tokens = scenario.get('avg_tokens', 2000)\n",
    "        \n",
    "        cost_per_query = self.comparator.estimate_cost(model_name, avg_tokens)\n",
    "        return monthly_queries * cost_per_query\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = RAGOptimizer(comparator)\n",
    "print(\"âœ… RAG Optimizer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Optimization Scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different usage scenarios\n",
    "optimization_scenarios = [\n",
    "    {\n",
    "        'name': 'High-Volume Customer Support',\n",
    "        'requirements': {\n",
    "            'speed_priority': 0.5,      # High speed priority\n",
    "            'quality_priority': 0.3,    # Medium quality priority\n",
    "            'cost_priority': 0.2,       # Low cost priority (speed more important)\n",
    "            'context_length': 4000,     # Typical support context\n",
    "        },\n",
    "        'monthly_queries': 10000,\n",
    "        'avg_tokens': 1500\n",
    "    },\n",
    "    {\n",
    "        'name': 'Research Analysis',\n",
    "        'requirements': {\n",
    "            'speed_priority': 0.2,      # Low speed priority\n",
    "            'quality_priority': 0.6,    # High quality priority\n",
    "            'cost_priority': 0.2,       # Medium cost priority\n",
    "            'context_length': 50000,    # Long research documents\n",
    "        },\n",
    "        'monthly_queries': 500,\n",
    "        'avg_tokens': 8000\n",
    "    },\n",
    "    {\n",
    "        'name': 'Budget-Conscious Startup',\n",
    "        'requirements': {\n",
    "            'speed_priority': 0.3,      # Medium speed priority\n",
    "            'quality_priority': 0.2,    # Medium quality priority\n",
    "            'cost_priority': 0.5,       # High cost priority\n",
    "            'context_length': 8000,     # Standard context\n",
    "        },\n",
    "        'monthly_queries': 2000,\n",
    "        'avg_tokens': 3000\n",
    "    },\n",
    "    {\n",
    "        'name': 'Premium Enterprise Service',\n",
    "        'requirements': {\n",
    "            'speed_priority': 0.3,      # Medium speed priority\n",
    "            'quality_priority': 0.7,    # Highest quality priority\n",
    "            'cost_priority': 0.0,       # No cost constraints\n",
    "            'context_length': 100000,   # Very long contexts\n",
    "        },\n",
    "        'monthly_queries': 1000,\n",
    "        'avg_tokens': 12000\n",
    "    },\n",
    "    {\n",
    "        'name': 'Real-time Chat Application',\n",
    "        'requirements': {\n",
    "            'speed_priority': 0.6,      # Highest speed priority\n",
    "            'quality_priority': 0.3,    # Medium quality priority\n",
    "            'cost_priority': 0.1,       # Low cost priority\n",
    "            'context_length': 2000,     # Short contexts for speed\n",
    "        },\n",
    "        'monthly_queries': 20000,\n",
    "        'avg_tokens': 800\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‹ Created {len(optimization_scenarios)} optimization scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Optimization Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate optimization recommendations\n",
    "optimization_report = optimizer.create_optimization_report(optimization_scenarios)\n",
    "\n",
    "print(\"ðŸŽ¯ Model Optimization Recommendations:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if not optimization_report.empty:\n",
    "    for _, row in optimization_report.iterrows():\n",
    "        print(f\"\\nðŸ“Š {row['scenario']}\")\n",
    "        print(f\"   Recommended: {row['recommended_model']}\")\n",
    "        print(f\"   Reason: {row['reason']}\")\n",
    "        print(f\"   Estimated Monthly Cost: ${row['estimated_cost']:.2f}\")\n",
    "        print(f\"   Speed Score: {row['speed']:.2f} | Quality Score: {row['quality']:.2f}\")\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\nðŸ“‹ Summary Table:\")\n",
    "    display_cols = ['scenario', 'recommended_model', 'estimated_cost', 'speed', 'quality']\n",
    "    print(optimization_report[display_cols].to_string(index=False))\n",
    "else:\n",
    "    print(\"âŒ No optimization report generated. Check model availability.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Multi-Model RAG System\n",
    "\n",
    "Let's build a sophisticated RAG system that can route queries to different models based on query characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from enum import Enum\n",
    "\n",
    "class QueryType(Enum):\n",
    "    SIMPLE = \"simple\"\n",
    "    COMPLEX = \"complex\"\n",
    "    ANALYTICAL = \"analytical\"\n",
    "    CODE = \"code\"\n",
    "    FACTUAL = \"factual\"\n",
    "    CREATIVE = \"creative\"\n",
    "\n",
    "class MultiModelRAGSystem:\n",
    "    \"\"\"Advanced RAG system with intelligent model routing\"\"\"\n",
    "    \n",
    "    def __init__(self, comparator: LLMComparator):\n",
    "        self.comparator = comparator\n",
    "        self.routing_rules = self.setup_routing_rules()\n",
    "        self.fallback_chain = self.setup_fallback_chain()\n",
    "        self.query_cache = {}\n",
    "    \n",
    "    def setup_routing_rules(self) -> Dict[QueryType, List[str]]:\n",
    "        \"\"\"Define which models to use for different query types\"\"\"\n",
    "        return {\n",
    "            QueryType.SIMPLE: ['gpt-3.5-turbo', 'gemini-pro', 'claude-3-sonnet'],\n",
    "            QueryType.COMPLEX: ['gpt-4-turbo', 'claude-3-opus', 'claude-3-sonnet'],\n",
    "            QueryType.ANALYTICAL: ['claude-3-opus', 'gpt-4-turbo', 'claude-3-sonnet'],\n",
    "            QueryType.CODE: ['gpt-4-turbo', 'claude-3-opus', 'gpt-3.5-turbo'],\n",
    "            QueryType.FACTUAL: ['gpt-3.5-turbo', 'gemini-pro', 'claude-3-sonnet'],\n",
    "            QueryType.CREATIVE: ['gpt-4-turbo', 'claude-3-opus', 'claude-3-sonnet']\n",
    "        }\n",
    "    \n",
    "    def setup_fallback_chain(self) -> List[str]:\n",
    "        \"\"\"Define fallback order if primary models fail\"\"\"\n",
    "        return ['gpt-3.5-turbo', 'gemini-pro', 'claude-3-sonnet', 'gpt-4-turbo']\n",
    "    \n",
    "    def classify_query(self, query: str) -> QueryType:\n",
    "        \"\"\"Classify query to determine appropriate model\"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Code-related patterns\n",
    "        code_patterns = ['code', 'function', 'algorithm', 'programming', 'debug', 'syntax']\n",
    "        if any(pattern in query_lower for pattern in code_patterns):\n",
    "            return QueryType.CODE\n",
    "        \n",
    "        # Analytical patterns\n",
    "        analytical_patterns = ['analyze', 'compare', 'evaluate', 'assess', 'critique', 'implications']\n",
    "        if any(pattern in query_lower for pattern in analytical_patterns):\n",
    "            return QueryType.ANALYTICAL\n",
    "        \n",
    "        # Creative patterns\n",
    "        creative_patterns = ['create', 'generate', 'write', 'design', 'brainstorm', 'imagine']\n",
    "        if any(pattern in query_lower for pattern in creative_patterns):\n",
    "            return QueryType.CREATIVE\n",
    "        \n",
    "        # Complex patterns (multiple questions, compound queries)\n",
    "        if len(query.split('?')) > 2 or len(query.split(' and ')) > 2:\n",
    "            return QueryType.COMPLEX\n",
    "        \n",
    "        # Simple factual patterns\n",
    "        factual_patterns = ['what is', 'who is', 'when did', 'where is', 'how many']\n",
    "        if any(pattern in query_lower for pattern in factual_patterns):\n",
    "            return QueryType.FACTUAL\n",
    "        \n",
    "        # Default to simple for short queries\n",
    "        if len(query.split()) < 10:\n",
    "            return QueryType.SIMPLE\n",
    "        \n",
    "        return QueryType.COMPLEX\n",
    "    \n",
    "    def select_model(self, query_type: QueryType) -> str:\n",
    "        \"\"\"Select the best available model for the query type\"\"\"\n",
    "        preferred_models = self.routing_rules.get(query_type, self.fallback_chain)\n",
    "        \n",
    "        # Find first available model\n",
    "        for model in preferred_models:\n",
    "            if model in self.comparator.models:\n",
    "                return model\n",
    "        \n",
    "        # Fallback to any available model\n",
    "        available_models = list(self.comparator.models.keys())\n",
    "        return available_models[0] if available_models else None\n",
    "    \n",
    "    def query_with_fallback(self, query: str, context: str, max_retries: int = 3) -> Dict:\n",
    "        \"\"\"Query with automatic fallback on failures\"\"\"\n",
    "        query_type = self.classify_query(query)\n",
    "        \n",
    "        # Try preferred models first\n",
    "        attempted_models = []\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                # Select model (avoid already attempted models)\n",
    "                available_models = [m for m in self.routing_rules.get(query_type, self.fallback_chain) \n",
    "                                  if m in self.comparator.models and m not in attempted_models]\n",
    "                \n",
    "                if not available_models:\n",
    "                    # Try fallback chain\n",
    "                    available_models = [m for m in self.fallback_chain \n",
    "                                      if m in self.comparator.models and m not in attempted_models]\n",
    "                \n",
    "                if not available_models:\n",
    "                    break  # No more models to try\n",
    "                \n",
    "                selected_model = available_models[0]\n",
    "                attempted_models.append(selected_model)\n",
    "                \n",
    "                print(f\"ðŸŽ¯ Routing {query_type.value} query to {selected_model}\")\n",
    "                \n",
    "                # Execute query\n",
    "                start_time = time.time()\n",
    "                model = self.comparator.models[selected_model]\n",
    "                prompt = self.comparator.create_rag_prompt(query, context)\n",
    "                \n",
    "                response = model.invoke([HumanMessage(content=prompt)])\n",
    "                response_time = time.time() - start_time\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'model_used': selected_model,\n",
    "                    'query_type': query_type.value,\n",
    "                    'response': response.content,\n",
    "                    'response_time': response_time,\n",
    "                    'attempt': attempt + 1\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ {selected_model} failed (attempt {attempt + 1}): {e}\")\n",
    "                continue\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': 'All models failed',\n",
    "            'attempted_models': attempted_models,\n",
    "            'query_type': query_type.value\n",
    "        }\n",
    "    \n",
    "    def batch_process(self, queries: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Process multiple queries with optimal model routing\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, query_data in enumerate(queries):\n",
    "            print(f\"\\nðŸ”„ Processing query {i+1}/{len(queries)}\")\n",
    "            \n",
    "            result = self.query_with_fallback(\n",
    "                query_data['query'],\n",
    "                query_data['context']\n",
    "            )\n",
    "            \n",
    "            result['original_query'] = query_data['query']\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_routing_stats(self, results: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Generate routing statistics\"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for result in results:\n",
    "            if result['success']:\n",
    "                stats.append({\n",
    "                    'query_type': result['query_type'],\n",
    "                    'model_used': result['model_used'],\n",
    "                    'response_time': result['response_time'],\n",
    "                    'attempt': result['attempt']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "# Initialize multi-model system\n",
    "multi_rag = MultiModelRAGSystem(comparator)\n",
    "print(\"âœ… Multi-Model RAG System initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Multi-Model Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create diverse test queries\n",
    "test_queries = [\n",
    "    {\n",
    "        'query': 'What is photosynthesis?',\n",
    "        'context': 'Photosynthesis is the process by which plants convert light energy into chemical energy. During photosynthesis, plants use sunlight, water, and carbon dioxide to produce glucose and oxygen.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Analyze the economic implications of renewable energy adoption and compare it with traditional fossil fuel infrastructure investments.',\n",
    "        'context': 'The transition to renewable energy requires significant upfront investments but offers long-term economic benefits. Traditional fossil fuel infrastructure has high operational costs and environmental externalities.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Write a Python function that implements binary search algorithm.',\n",
    "        'context': 'Binary search is an efficient algorithm for finding an item from a sorted list of items. It works by repeatedly dividing the search space in half.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'Create a creative story about a time-traveling scientist.',\n",
    "        'context': 'Science fiction often explores themes of time travel and scientific discovery. Time travel stories can examine cause and effect, paradoxes, and the nature of destiny.'\n",
    "    },\n",
    "    {\n",
    "        'query': 'How many planets are in our solar system and when was Pluto reclassified?',\n",
    "        'context': 'The solar system contains eight planets since Pluto was reclassified as a dwarf planet in 2006 by the International Astronomical Union.'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"ðŸ§ª Testing with {len(test_queries)} diverse queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process queries through multi-model system\n",
    "print(\"ðŸš€ Starting multi-model routing test...\")\n",
    "routing_results = multi_rag.batch_process(test_queries)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nðŸ“Š Routing Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for result in routing_results:\n",
    "    if result['success']:\n",
    "        print(f\"\\nâœ… Query: {result['original_query'][:60]}...\")\n",
    "        print(f\"   Type: {result['query_type']} | Model: {result['model_used']}\")\n",
    "        print(f\"   Time: {result['response_time']:.2f}s | Attempt: {result['attempt']}\")\n",
    "        print(f\"   Response: {result['response'][:100]}...\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ Failed Query: {result['original_query'][:60]}...\")\n",
    "        print(f\"   Error: {result['error']}\")\n",
    "\n",
    "# Generate routing statistics\n",
    "if any(r['success'] for r in routing_results):\n",
    "    stats_df = multi_rag.get_routing_stats(routing_results)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ Routing Statistics:\")\n",
    "    print(stats_df.groupby(['query_type', 'model_used']).size().unstack(fill_value=0))\n",
    "    \n",
    "    print(\"\\nâ±ï¸ Average Response Times by Model:\")\n",
    "    print(stats_df.groupby('model_used')['response_time'].mean().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Production-Ready Integration Patterns\n",
    "\n",
    "Let's implement production-ready patterns for LLM integration including rate limiting, error handling, and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import defaultdict, deque\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "class ProductionRAGManager:\n",
    "    \"\"\"Production-ready RAG manager with monitoring and reliability features\"\"\"\n",
    "    \n",
    "    def __init__(self, multi_rag: MultiModelRAGSystem):\n",
    "        self.multi_rag = multi_rag\n",
    "        self.rate_limiter = RateLimiter()\n",
    "        self.circuit_breaker = CircuitBreaker()\n",
    "        self.monitor = RAGMonitor()\n",
    "        self.cache = QueryCache()\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "    \n",
    "    def query(self, query: str, context: str, **kwargs) -> Dict:\n",
    "        \"\"\"Production query with full safety features\"\"\"\n",
    "        query_id = f\"q_{datetime.now().timestamp()}\"\n",
    "        \n",
    "        try:\n",
    "            # Check cache first\n",
    "            cached_result = self.cache.get(query, context)\n",
    "            if cached_result:\n",
    "                self.logger.info(f\"Cache hit for query {query_id}\")\n",
    "                return cached_result\n",
    "            \n",
    "            # Check rate limits\n",
    "            if not self.rate_limiter.can_proceed(kwargs.get('user_id', 'default')):\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': 'Rate limit exceeded',\n",
    "                    'query_id': query_id\n",
    "                }\n",
    "            \n",
    "            # Check circuit breaker\n",
    "            if not self.circuit_breaker.can_call():\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': 'Service temporarily unavailable',\n",
    "                    'query_id': query_id\n",
    "                }\n",
    "            \n",
    "            # Execute query\n",
    "            start_time = time.time()\n",
    "            result = self.multi_rag.query_with_fallback(query, context)\n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            # Update circuit breaker\n",
    "            if result['success']:\n",
    "                self.circuit_breaker.record_success()\n",
    "                \n",
    "                # Cache successful result\n",
    "                self.cache.set(query, context, result)\n",
    "            else:\n",
    "                self.circuit_breaker.record_failure()\n",
    "            \n",
    "            # Record metrics\n",
    "            self.monitor.record_query(\n",
    "                query_id=query_id,\n",
    "                success=result['success'],\n",
    "                model_used=result.get('model_used'),\n",
    "                execution_time=execution_time,\n",
    "                query_type=result.get('query_type')\n",
    "            )\n",
    "            \n",
    "            result['query_id'] = query_id\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Query {query_id} failed: {e}\")\n",
    "            self.circuit_breaker.record_failure()\n",
    "            \n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'query_id': query_id\n",
    "            }\n",
    "    \n",
    "    def get_health_status(self) -> Dict:\n",
    "        \"\"\"Get system health status\"\"\"\n",
    "        return {\n",
    "            'circuit_breaker_state': self.circuit_breaker.state,\n",
    "            'cache_hit_rate': self.cache.get_hit_rate(),\n",
    "            'total_queries': self.monitor.get_total_queries(),\n",
    "            'success_rate': self.monitor.get_success_rate(),\n",
    "            'avg_response_time': self.monitor.get_avg_response_time()\n",
    "        }\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter\"\"\"\n",
    "    \n",
    "    def __init__(self, requests_per_minute: int = 60):\n",
    "        self.requests_per_minute = requests_per_minute\n",
    "        self.buckets = defaultdict(lambda: {\n",
    "            'tokens': requests_per_minute,\n",
    "            'last_update': datetime.now()\n",
    "        })\n",
    "    \n",
    "    def can_proceed(self, user_id: str) -> bool:\n",
    "        \"\"\"Check if request can proceed\"\"\"\n",
    "        now = datetime.now()\n",
    "        bucket = self.buckets[user_id]\n",
    "        \n",
    "        # Refill tokens based on time passed\n",
    "        time_passed = (now - bucket['last_update']).total_seconds()\n",
    "        tokens_to_add = (time_passed / 60) * self.requests_per_minute\n",
    "        \n",
    "        bucket['tokens'] = min(\n",
    "            self.requests_per_minute,\n",
    "            bucket['tokens'] + tokens_to_add\n",
    "        )\n",
    "        bucket['last_update'] = now\n",
    "        \n",
    "        if bucket['tokens'] >= 1:\n",
    "            bucket['tokens'] -= 1\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"Circuit breaker for fault tolerance\"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 5, timeout_seconds: int = 60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout_seconds = timeout_seconds\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN\n",
    "    \n",
    "    def can_call(self) -> bool:\n",
    "        \"\"\"Check if calls are allowed\"\"\"\n",
    "        if self.state == 'CLOSED':\n",
    "            return True\n",
    "        \n",
    "        if self.state == 'OPEN':\n",
    "            if self._should_attempt_reset():\n",
    "                self.state = 'HALF_OPEN'\n",
    "                return True\n",
    "            return False\n",
    "        \n",
    "        if self.state == 'HALF_OPEN':\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def record_success(self):\n",
    "        \"\"\"Record successful call\"\"\"\n",
    "        self.failure_count = 0\n",
    "        self.state = 'CLOSED'\n",
    "    \n",
    "    def record_failure(self):\n",
    "        \"\"\"Record failed call\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = datetime.now()\n",
    "        \n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.state = 'OPEN'\n",
    "    \n",
    "    def _should_attempt_reset(self) -> bool:\n",
    "        \"\"\"Check if enough time has passed to attempt reset\"\"\"\n",
    "        if self.last_failure_time is None:\n",
    "            return False\n",
    "        \n",
    "        return (datetime.now() - self.last_failure_time).total_seconds() > self.timeout_seconds\n",
    "\n",
    "class QueryCache:\n",
    "    \"\"\"Simple in-memory cache with TTL\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):\n",
    "        self.max_size = max_size\n",
    "        self.ttl_seconds = ttl_seconds\n",
    "        self.cache = {}\n",
    "        self.access_times = deque()\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _generate_key(self, query: str, context: str) -> str:\n",
    "        \"\"\"Generate cache key\"\"\"\n",
    "        import hashlib\n",
    "        content = f\"{query}:{context}\"\n",
    "        return hashlib.md5(content.encode()).hexdigest()\n",
    "    \n",
    "    def get(self, query: str, context: str) -> Optional[Dict]:\n",
    "        \"\"\"Get cached result\"\"\"\n",
    "        key = self._generate_key(query, context)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            \n",
    "            # Check TTL\n",
    "            if (datetime.now() - entry['timestamp']).total_seconds() < self.ttl_seconds:\n",
    "                self.hits += 1\n",
    "                return entry['result']\n",
    "            else:\n",
    "                del self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, query: str, context: str, result: Dict):\n",
    "        \"\"\"Set cached result\"\"\"\n",
    "        key = self._generate_key(query, context)\n",
    "        \n",
    "        # Remove oldest entries if cache is full\n",
    "        while len(self.cache) >= self.max_size and self.access_times:\n",
    "            oldest_key = self.access_times.popleft()\n",
    "            self.cache.pop(oldest_key, None)\n",
    "        \n",
    "        self.cache[key] = {\n",
    "            'result': result,\n",
    "            'timestamp': datetime.now()\n",
    "        }\n",
    "        self.access_times.append(key)\n",
    "    \n",
    "    def get_hit_rate(self) -> float:\n",
    "        \"\"\"Get cache hit rate\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        return self.hits / total if total > 0 else 0.0\n",
    "\n",
    "class RAGMonitor:\n",
    "    \"\"\"Monitor RAG system performance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queries = []\n",
    "        self.model_usage = defaultdict(int)\n",
    "    \n",
    "    def record_query(self, query_id: str, success: bool, model_used: str,\n",
    "                    execution_time: float, query_type: str):\n",
    "        \"\"\"Record query metrics\"\"\"\n",
    "        self.queries.append({\n",
    "            'query_id': query_id,\n",
    "            'timestamp': datetime.now(),\n",
    "            'success': success,\n",
    "            'model_used': model_used,\n",
    "            'execution_time': execution_time,\n",
    "            'query_type': query_type\n",
    "        })\n",
    "        \n",
    "        if model_used:\n",
    "            self.model_usage[model_used] += 1\n",
    "    \n",
    "    def get_total_queries(self) -> int:\n",
    "        \"\"\"Get total number of queries\"\"\"\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def get_success_rate(self) -> float:\n",
    "        \"\"\"Get overall success rate\"\"\"\n",
    "        if not self.queries:\n",
    "            return 0.0\n",
    "        \n",
    "        successful = sum(1 for q in self.queries if q['success'])\n",
    "        return successful / len(self.queries)\n",
    "    \n",
    "    def get_avg_response_time(self) -> float:\n",
    "        \"\"\"Get average response time\"\"\"\n",
    "        if not self.queries:\n",
    "            return 0.0\n",
    "        \n",
    "        total_time = sum(q['execution_time'] for q in self.queries if q['success'])\n",
    "        successful_queries = sum(1 for q in self.queries if q['success'])\n",
    "        \n",
    "        return total_time / successful_queries if successful_queries > 0 else 0.0\n",
    "    \n",
    "    def get_model_usage_stats(self) -> Dict:\n",
    "        \"\"\"Get model usage statistics\"\"\"\n",
    "        return dict(self.model_usage)\n",
    "\n",
    "# Initialize production manager\n",
    "prod_manager = ProductionRAGManager(multi_rag)\n",
    "print(\"âœ… Production RAG Manager initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Production Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test production features\n",
    "print(\"ðŸ§ª Testing Production RAG Manager\")\n",
    "\n",
    "# Test queries with different users\n",
    "test_production_queries = [\n",
    "    {\n",
    "        'query': 'What is artificial intelligence?',\n",
    "        'context': 'Artificial intelligence is the simulation of human intelligence in machines.',\n",
    "        'user_id': 'user1'\n",
    "    },\n",
    "    {\n",
    "        'query': 'How does machine learning work?',\n",
    "        'context': 'Machine learning is a method of data analysis that automates analytical model building.',\n",
    "        'user_id': 'user1'\n",
    "    },\n",
    "    {\n",
    "        'query': 'What is artificial intelligence?',  # Same query to test caching\n",
    "        'context': 'Artificial intelligence is the simulation of human intelligence in machines.',\n",
    "        'user_id': 'user2'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Execute test queries\n",
    "production_results = []\n",
    "for i, query_data in enumerate(test_production_queries):\n",
    "    print(f\"\\nðŸ“¤ Query {i+1}: {query_data['query'][:40]}...\")\n",
    "    \n",
    "    result = prod_manager.query(\n",
    "        query_data['query'],\n",
    "        query_data['context'],\n",
    "        user_id=query_data['user_id']\n",
    "    )\n",
    "    \n",
    "    production_results.append(result)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"âœ… Success | Model: {result.get('model_used', 'cache')} | ID: {result['query_id']}\")\n",
    "    else:\n",
    "        print(f\"âŒ Failed | Error: {result['error']} | ID: {result['query_id']}\")\n",
    "\n",
    "# Display health status\n",
    "print(\"\\nðŸ¥ System Health Status:\")\n",
    "health = prod_manager.get_health_status()\n",
    "for key, value in health.items():\n",
    "    print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization of model performance\n",
    "if not optimization_report.empty:\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Model recommendations by scenario\n",
    "    model_counts = optimization_report['recommended_model'].value_counts()\n",
    "    ax1.pie(model_counts.values, labels=model_counts.index, autopct='%1.1f%%')\n",
    "    ax1.set_title('Model Recommendations Distribution')\n",
    "    \n",
    "    # Cost vs Quality scatter\n",
    "    ax2.scatter(optimization_report['estimated_cost'], optimization_report['quality'], \n",
    "               c=optimization_report['speed'], cmap='viridis', s=100)\n",
    "    ax2.set_xlabel('Estimated Monthly Cost ($)')\n",
    "    ax2.set_ylabel('Quality Score')\n",
    "    ax2.set_title('Cost vs Quality (Color = Speed)')\n",
    "    \n",
    "    # Speed vs Quality by model\n",
    "    for model in optimization_report['recommended_model'].unique():\n",
    "        model_data = optimization_report[optimization_report['recommended_model'] == model]\n",
    "        ax3.scatter(model_data['speed'], model_data['quality'], label=model, s=100)\n",
    "    ax3.set_xlabel('Speed Score')\n",
    "    ax3.set_ylabel('Quality Score')\n",
    "    ax3.set_title('Speed vs Quality by Model')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Cost comparison\n",
    "    scenario_costs = optimization_report.set_index('scenario')['estimated_cost']\n",
    "    scenario_costs.plot(kind='bar', ax=ax4)\n",
    "    ax4.set_title('Estimated Monthly Costs by Scenario')\n",
    "    ax4.set_ylabel('Cost ($)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Display optimization insights\n",
    "    print(\"\\nðŸ’¡ Key Insights:\")\n",
    "    print(f\"   â€¢ Most recommended model: {model_counts.index[0]} ({model_counts.iloc[0]} scenarios)\")\n",
    "    print(f\"   â€¢ Highest cost scenario: {scenario_costs.idxmax()} (${scenario_costs.max():.2f}/month)\")\n",
    "    print(f\"   â€¢ Most cost-effective scenario: {scenario_costs.idxmin()} (${scenario_costs.min():.2f}/month)\")\n",
    "    \n",
    "    cost_range = scenario_costs.max() - scenario_costs.min()\n",
    "    print(f\"   â€¢ Cost variation: ${cost_range:.2f}/month ({cost_range/scenario_costs.mean()*100:.1f}% of average)\")\n",
    "else:\n",
    "    print(\"âš ï¸ No data available for visualization. Make sure models are properly configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. Model Selection Strategy\n",
    "- **Context Window**: Critical for RAG applications with long documents\n",
    "- **Cost Structure**: Balance input/output token costs with query volume\n",
    "- **Query Classification**: Route different query types to optimal models\n",
    "\n",
    "### 2. Production Considerations\n",
    "- **Rate Limiting**: Prevent abuse and manage costs\n",
    "- **Circuit Breaker**: Handle model failures gracefully\n",
    "- **Caching**: Reduce costs and improve response times\n",
    "- **Monitoring**: Track performance and costs continuously\n",
    "\n",
    "### 3. Multi-Model Architecture Benefits\n",
    "- **Cost Optimization**: Use cheaper models for simple queries\n",
    "- **Reliability**: Fallback options when primary models fail\n",
    "- **Specialization**: Leverage model strengths for specific tasks\n",
    "\n",
    "### 4. Best Practices\n",
    "- Start with a single reliable model, then add complexity\n",
    "- Monitor costs and performance metrics continuously\n",
    "- Implement proper error handling and retry logic\n",
    "- Use caching strategically to reduce API calls\n",
    "- Plan for model deprecation and migration\n",
    "\n",
    "---\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "1. **Cost vs Quality Trade-offs**: When is it worth paying 10x more for a premium model?\n",
    "\n",
    "2. **Model Routing Logic**: How would you handle edge cases in query classification?\n",
    "\n",
    "3. **Production Monitoring**: What metrics are most important for a production RAG system?\n",
    "\n",
    "4. **Vendor Lock-in**: How do you balance using proprietary APIs vs open source models?\n",
    "\n",
    "5. **Future Proofing**: How do you prepare for new model releases and capability changes?\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different model routing strategies\n",
    "- Implement cost tracking and budgeting\n",
    "- Add support for streaming responses\n",
    "- Explore model fine-tuning for specific domains\n",
    "- Consider edge deployment for latency-critical applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}