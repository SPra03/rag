{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini: Document processing (PDF) — 5‑minute intro\n",
        "\n",
        "Goal: show a minimal PDF → text → clean → chunk → save flow so you can plug it into RAG later.\n",
        "\n",
        "Steps we’ll do now:\n",
        "- Load a sample PDF (`data/sample.pdf`)\n",
        "- Extract text with a lightweight library\n",
        "- Clean whitespace and normalize\n",
        "- Chunk into small slices with overlap\n",
        "- Save chunks to JSONL for later indexing\n",
        "\n",
        "Keep this simple: this is not about perfect parsing—just a quick, practical baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal deps; run once per environment\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(package: str):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "pip_install(\"pypdf\")\n",
        "\n",
        "print(\"Ready ✔\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from pypdf import PdfReader\n",
        "\n",
        "PDF_PATH = Path(\"data/sample.pdf\")\n",
        "assert PDF_PATH.exists(), f\"Missing file: {PDF_PATH}\"\n",
        "\n",
        "reader = PdfReader(str(PDF_PATH))\n",
        "num_pages = len(reader.pages)\n",
        "\n",
        "pages_text = []\n",
        "for i in range(num_pages):\n",
        "    page = reader.pages[i]\n",
        "    text = page.extract_text() or \"\"\n",
        "    pages_text.append(text)\n",
        "\n",
        "raw_text = \"\\n\\n\".join(pages_text)\n",
        "print({\"pages\": num_pages, \"chars\": len(raw_text)})\n",
        "raw_text[:500]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def basic_clean(text: str) -> str:\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    # strip odd nulls or control chars\n",
        "    text = re.sub(r\"[\\x00-\\x08\\x0B-\\x1F\\x7F]\", \"\", text)\n",
        "    # trim\n",
        "    return text.strip()\n",
        "\n",
        "clean_text = basic_clean(raw_text)\n",
        "print({\"chars_before\": len(raw_text), \"chars_after\": len(clean_text)})\n",
        "clean_text[:500]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "    chunks: List[str] = []\n",
        "    start = 0\n",
        "    n = len(text)\n",
        "    while start < n:\n",
        "        end = min(start + chunk_size, n)\n",
        "        chunks.append(text[start:end])\n",
        "        if end == n:\n",
        "            break\n",
        "        start = end - overlap\n",
        "    return chunks\n",
        "\n",
        "chunks = chunk_text(clean_text, chunk_size=700, overlap=100)\n",
        "print({\"num_chunks\": len(chunks), \"first_len\": len(chunks[0]) if chunks else 0})\n",
        "chunks[:2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_PATH = Path(\"data/sample_chunks.jsonl\")\n",
        "\n",
        "# quick preview\n",
        "for i, ch in enumerate(chunks[:3]):\n",
        "    print(f\"--- chunk {i} ({len(ch)} chars) ---\\n{ch[:300]}\\n\")\n",
        "\n",
        "with OUT_PATH.open(\"w\", encoding=\"utf-8\") as f:\n",
        "    for i, ch in enumerate(chunks):\n",
        "        rec = {\"id\": f\"sample-{i}\", \"text\": ch}\n",
        "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Saved {len(chunks)} chunks -> {OUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LangChain loader setup\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(package: str):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Core packages for loaders and splitters live in community package\n",
        "pip_install(\"langchain\")\n",
        "pip_install(\"langchain-community\")\n",
        "\n",
        "print(\"LangChain ready ✔\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "PDF_PATH = Path(\"data/sample.pdf\")\n",
        "loader = PyPDFLoader(str(PDF_PATH))\n",
        "docs = loader.load()  # one Document per page\n",
        "\n",
        "# add simple metadata\n",
        "for i, d in enumerate(docs):\n",
        "    d.metadata.update({\n",
        "        \"doc_id\": f\"sample_pdf\",\n",
        "        \"source\": str(PDF_PATH),\n",
        "        \"page\": d.metadata.get(\"page\", i),\n",
        "        \"type\": \"pdf\",\n",
        "        \"module\": \"doc_processing_min\",\n",
        "        \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    })\n",
        "\n",
        "len(docs), docs[0].metadata, docs[0].page_content[:300]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# quick glance at a couple of pages\n",
        "for i, d in enumerate(docs[:2]):\n",
        "    print({\"i\": i, \"chars\": len(d.page_content), \"meta\": {k: d.metadata[k] for k in [\"doc_id\", \"page\", \"type\", \"module\"]}})\n",
        "    print(d.page_content[:300].replace(\"\\n\", \" \") + \"\\n---\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make documents ready for next step: recursive chunking\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=700,\n",
        "    chunk_overlap=100,\n",
        "    add_start_index=True,\n",
        ")\n",
        "\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "print({\"pages\": len(docs), \"chunks\": len(chunked_docs)})\n",
        "chunked_docs[:2]\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
