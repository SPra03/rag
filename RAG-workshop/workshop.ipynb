{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f9830b",
   "metadata": {},
   "source": [
    "# Hands-on Coding Excersises\n",
    "\n",
    "### üéØ Goal: TO understand the fundamental concepts and see them in action.\n",
    "\n",
    "### What are we going to do? \n",
    "We will walk through the code step-by-step for the first 45 minutes. In the last 15 minutes, everyone can start working on the shared piece of code and start ececuting it. \n",
    "\n",
    "### What are we not going to do?\n",
    "We are not going to code in parallel to the excercise by the instructor. \n",
    "We should also refrain from deep diving individual complex codes or other packages. \n",
    "\n",
    "Note: We have people at various skill levels. So the focus is to fundamentally understand and see the concepts in action. We all have high level coding skills. So, it is expected we can later deep dive into code and other parameters etc. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fb8d2",
   "metadata": {},
   "source": [
    "# Module 1: Introduction & Problem Statement\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand the fundamental limitations of standalone LLMs\n",
    "- Recognize when RAG is the right solution\n",
    "- Visualize the complete RAG workflow\n",
    "- Experience hands-on examples of LLM limitations and RAG solutions\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### What is RAG?\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances Large Language Models (LLMs) by providing them with relevant external information before generating responses.\n",
    "\n",
    "Think of it like an open-book exam:\n",
    "- **Without RAG**: LLM answers from memory only (closed-book exam)\n",
    "- **With RAG**: LLM gets relevant documents first, then answers (open-book exam)\n",
    "\n",
    "### Why Do We Need RAG?\n",
    "\n",
    "#### üö´ LLM Limitations\n",
    "1. **Knowledge Cutoff Dates**: LLMs are trained on data up to a certain date\n",
    "2. **Hallucinations**: LLMs can generate confident-sounding but incorrect information\n",
    "3. **No Domain-Specific Knowledge**: Limited knowledge about your company/domain\n",
    "4. **No Real-time Information**: Cannot access current events or dynamic data\n",
    "5. **Context Length Limits**: Cannot process entire large documents\n",
    "\n",
    "#### ‚úÖ How RAG Helps\n",
    "1. **Fresh Information**: Access to up-to-date external data\n",
    "2. **Grounded Responses**: Answers based on provided evidence\n",
    "3. **Domain Expertise**: Include your specific documents and data\n",
    "4. **Source Attribution**: Know where information comes from\n",
    "5. **Cost Effective**: No need to retrain models with new data\n",
    "\n",
    "### 2025 Research Insights üî¨\n",
    "- **Mathematical Proof**: OpenAI researchers proved LLM hallucinations are mathematically inevitable (Sept 2025)\n",
    "- **Current Best**: Anthropic Claude 3.7 has the lowest hallucination rate at 17%\n",
    "- **RAG Impact**: Properly implemented RAG can reduce hallucinations by 49-67%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60edd6",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup (LlmUtils)\n",
    "Let's Set up and get started!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e0881",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d5dce4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import llmutils\n",
    "..\n",
    "..\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,  # Low temperature for more consistent results\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"üìÖ Today's date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae4afe3",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Demonstrating LLM Limitations\n",
    "\n",
    "Let's see these limitations in action with real examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56631061",
   "metadata": {},
   "source": [
    "### Problem 1: Knowledge Cutoff Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1871d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test knowledge cutoff with recent events\n",
    "recent_questions = [\n",
    "    \"What happened in the 2024 US Presidential Election?\",\n",
    "    \"Who won the 2024 Nobel Prize in Physics?\",\n",
    "    \"What are the latest features in iPhone 16?\",\n",
    "    \"What is the current stock price of NVIDIA?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing Knowledge Cutoff Issues:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in recent_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=question)])\n",
    "        print(f\"ü§ñ LLM Response: {response.content[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435bb62f",
   "metadata": {},
   "source": [
    "### Problem 2: Hallucinations (Confident but Wrong Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2932e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with questions that might trigger hallucinations\n",
    "tricky_questions = [\n",
    "    \"What is the exact population of Atlantis?\",\n",
    "    \"Who is the CEO of DataScience Corp (a fictional company)?\",\n",
    "    \"What are the side effects of Imaginex (a made-up drug)?\",\n",
    "    \"What year was the Treaty of Fabrication signed?\"\n",
    "]\n",
    "\n",
    "print(\"üé≠ Testing Hallucination Tendencies:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in tricky_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=question)])\n",
    "        print(f\"ü§ñ LLM Response: {response.content}\")\n",
    "        print(\"‚ö†Ô∏è  Note: This response might be hallucinated since the question involves fictional entities!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d68836",
   "metadata": {},
   "source": [
    "### Problem 3: No Domain-Specific Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de99053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with company-specific questions\n",
    "company_questions = [\n",
    "    \"What is our company's return policy?\",\n",
    "    \"Who is the head of our marketing department?\",\n",
    "    \"What are the specs of our Model-X product?\",\n",
    "    \"What was discussed in last week's board meeting?\"\n",
    "]\n",
    "\n",
    "print(\"üè¢ Testing Domain-Specific Knowledge:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in company_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=question)])\n",
    "        print(f\"ü§ñ LLM Response: {response.content}\")\n",
    "        print(\"üìù Note: LLM cannot access company-specific information!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8bbdd",
   "metadata": {},
   "source": [
    "## üîß Exercise 2: Simple RAG Preview\n",
    "\n",
    "Now let's see how RAG can solve these problems with a basic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f12101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample company knowledge\n",
    "company_knowledge = \"\"\"\n",
    "COMPANY INFORMATION DATABASE\n",
    "============================\n",
    "\n",
    "COMPANY: TechCorp Solutions\n",
    "FOUNDED: 2018\n",
    "HEADQUARTERS: San Francisco, CA\n",
    "\n",
    "LEADERSHIP:\n",
    "- CEO: Sarah Johnson\n",
    "- CTO: Michael Chen  \n",
    "- Head of Marketing: Lisa Rodriguez\n",
    "- Head of Sales: David Kim\n",
    "\n",
    "PRODUCTS:\n",
    "- Model-X: AI-powered analytics platform\n",
    "  Specs: 99.9% uptime, supports 1M+ queries/sec, cloud-native\n",
    "- Model-Y: Customer service automation tool\n",
    "  Specs: 24/7 support, multi-language, integrates with 50+ platforms\n",
    "\n",
    "POLICIES:\n",
    "- Return Policy: 30-day money-back guarantee on all products\n",
    "- Support: 24/7 technical support for enterprise customers\n",
    "- Privacy: SOC2 Type II certified, GDPR compliant\n",
    "\n",
    "RECENT NEWS:\n",
    "- 2024-01-15: Launched Model-Y 2.0 with enhanced AI capabilities\n",
    "- 2024-02-10: Secured $50M Series B funding\n",
    "- 2024-03-05: Opened new office in London\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe38d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_query(question, knowledge_base):\n",
    "    \"\"\"\n",
    "    A simple RAG implementation:\n",
    "    1. Provide relevant context to the LLM\n",
    "    2. Ask the LLM to answer based on that context\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a prompt that includes our knowledge base\n",
    "    rag_prompt = f\"\"\"\n",
    "    You are a helpful assistant that answers questions based on the provided company information.\n",
    "    \n",
    "    COMPANY INFORMATION:\n",
    "    {knowledge_base}\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Please answer the question based ONLY on the information provided above. \n",
    "    If the information is not available, say \"I don't have that information in the company database.\"\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke([HumanMessage(content=rag_prompt)])\n",
    "    return response.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba5ed27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our simple RAG with the same company questions\n",
    "print(\"üöÄ Testing Simple RAG vs Standard LLM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is our company's return policy?\",\n",
    "    \"Who is the head of our marketing department?\",\n",
    "    \"What are the specs of our Model-X product?\",\n",
    "    \"When did we open our London office?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    \n",
    "    # Standard LLM response\n",
    "    print(\"\\nü§ñ Standard LLM (no context):\")\n",
    "    standard_response = llm.invoke([HumanMessage(content=question)])\n",
    "    print(f\"   {standard_response.content}\")\n",
    "    \n",
    "    # RAG response\n",
    "    print(\"\\nüîç RAG LLM (with company context):\")\n",
    "    rag_response = simple_rag_query(question, company_knowledge)\n",
    "    print(f\"   {rag_response}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa520b1",
   "metadata": {},
   "source": [
    "## üìä Exercise 3: RAG Workflow Visualization\n",
    "\n",
    "Let's understand the complete RAG workflow step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac70a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rag_workflow(user_question):\n",
    "    \"\"\"\n",
    "    Demonstrate the RAG workflow step by step\n",
    "    \"\"\"\n",
    "    print(\"üîÑ RAG WORKFLOW DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: User asks a question\n",
    "    print(f\"üìù STEP 1: User Question\")\n",
    "    print(f\"   '{user_question}'\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Retrieve relevant documents (simplified)\n",
    "    print(f\"üîç STEP 2: Document Retrieval\")\n",
    "    print(f\"   Searching knowledge base for relevant information...\")\n",
    "    \n",
    "    # Simple keyword matching for demonstration\n",
    "    question_words = user_question.lower().split()\n",
    "    relevant_lines = []\n",
    "    \n",
    "    for line in company_knowledge.split('\\n'):\n",
    "        if any(word in line.lower() for word in question_words if len(word) > 3):\n",
    "            relevant_lines.append(line.strip())\n",
    "    \n",
    "    print(f\"   Found {len(relevant_lines)} relevant lines:\")\n",
    "    for line in relevant_lines[:3]:  # Show first 3 relevant lines\n",
    "        if line:\n",
    "            print(f\"   - {line}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Augment the prompt\n",
    "    print(f\"üìù STEP 3: Prompt Augmentation\")\n",
    "    print(f\"   Combining retrieved context with user question...\")\n",
    "    print(f\"   Context + Question ‚Üí Enhanced Prompt\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    print(f\"ü§ñ STEP 4: Generation\")\n",
    "    print(f\"   LLM generates response based on provided context...\")\n",
    "    \n",
    "    response = simple_rag_query(user_question, company_knowledge)\n",
    "    print(f\"   Response: '{response}'\")\n",
    "    print()\n",
    "    \n",
    "    # Step 5: Return grounded answer\n",
    "    print(f\"‚úÖ STEP 5: Grounded Answer\")\n",
    "    print(f\"   Answer is based on actual company data, not LLM's training!\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the workflow\n",
    "sample_question = \"Who is our CTO?\"\n",
    "result = visualize_rag_workflow(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70299a",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### ‚ùå LLM Limitations We Observed:\n",
    "1. **Knowledge Cutoff**: Cannot answer questions about recent events\n",
    "2. **Hallucinations**: May provide confident but incorrect answers\n",
    "3. **No Domain Knowledge**: Cannot access company-specific information\n",
    "4. **Generic Responses**: Provides general answers without specific context\n",
    "\n",
    "### ‚úÖ RAG Benefits We Demonstrated:\n",
    "1. **Accurate Information**: Answers based on provided context\n",
    "2. **Domain-Specific**: Can access and use company knowledge\n",
    "3. **Grounded Responses**: Explicitly tells you when information isn't available\n",
    "4. **Improved Accuracy**: Significantly better performance on domain-specific questions\n",
    "\n",
    "### üîÑ RAG Workflow:\n",
    "1. **User Question** ‚Üí 2. **Retrieve Relevant Docs** ‚Üí 3. **Augment Prompt** ‚Üí 4. **Generate Response** ‚Üí 5. **Grounded Answer**\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In the next modules, we'll dive deeper into each component of the RAG system:\n",
    "- **Module 2**: How to load and process different types of documents\n",
    "- **Module 3**: Strategies for breaking documents into chunks\n",
    "- **Module 4**: Understanding embedding models for semantic search\n",
    "- And much more!\n",
    "\n",
    "The simple RAG we built here is just the beginning. Real-world RAG systems are much more sophisticated and powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f97935",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. In which scenarios would you prefer a standard LLM over RAG?\n",
    "2. What types of company data would be most valuable to include in a RAG system?\n",
    "3. How might RAG help with compliance and audit requirements?\n",
    "4. What are potential challenges with implementing RAG in a large organization?\n",
    "\n",
    "## üìù Optional Exercise\n",
    "\n",
    "Try creating your own knowledge base for a fictional company or organization and test the RAG system with domain-specific questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59c0d68",
   "metadata": {},
   "source": [
    "# Module 2: Document Types & Data Sources\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Use LangChain document loaders for various file formats as well as process it  \n",
    "- Use LangChain document loaders for various file formats\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### Why Document Processing Matters\n",
    "In RAG systems, the quality of your document processing directly impacts your final results. \n",
    "\n",
    "**Garbage In = Garbage Out!**\n",
    "\n",
    "### Document Types & Challenges\n",
    "\n",
    "| Document Type | Common Issues | Best Approach |\n",
    "|---------------|---------------|---------------|\n",
    "| **Plain Text** | Encoding, structure | Simple loaders |\n",
    "| **PDF** | Tables, images, layout | AI-powered parsing |\n",
    "| **HTML** | Noise, dynamic content | Smart cleaning |\n",
    "| **CSV/JSON** | Structure preservation | Specialized loaders |\n",
    "| **Images** | OCR accuracy | Multimodal models |\n",
    "| **Code** | Syntax preservation | Language-aware parsing |\n",
    "\n",
    "### 2025 Breakthroughs üöÄ\n",
    "- **LlamaParse**: AI-powered PDF parsing with vision-language models\n",
    "- **Hybrid Multimodal**: Combining traditional + AI approaches\n",
    "- **Markdown Intermediate**: Better structure preservation\n",
    "- **Unstructured Library**: Production-ready document processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0225b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "PDF_PATH = Path(\"data/sample.pdf\")\n",
    "loader = PyPDFLoader(str(PDF_PATH))\n",
    "docs = loader.load()  # one Document per page\n",
    "\n",
    "# add simple metadata\n",
    "for i, d in enumerate(docs):\n",
    "    d.metadata.update({\n",
    "        \"doc_id\": f\"sample_pdf\",\n",
    "        \"source\": str(PDF_PATH),\n",
    "        \"page\": d.metadata.get(\"page\", i),\n",
    "        \"type\": \"pdf\",\n",
    "        \"module\": \"doc_processing_min\",\n",
    "        \"ingested_at\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    })\n",
    "\n",
    "len(docs), docs[0].metadata, docs[0].page_content[:300]\n",
    "\n",
    "\n",
    "#print.... and show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1985cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code for loading all, but will not run it.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed32908e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e83863d",
   "metadata": {},
   "source": [
    "# Module 3: Chunking Strategies & Implementation\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand why chunking is essential for RAG systems\n",
    "- Implement different chunking strategies using LangChain\n",
    "- Compare fixed-size, semantic, and recursive chunking approaches\n",
    "- Optimize chunk sizes for different use cases\n",
    "- Preserve context and metadata across chunks\n",
    "- Apply 2025's latest semantic chunking techniques\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### Why Chunking is Critical üî™\n",
    "\n",
    "**Think of chunking like creating sections and subsections in a book:**\n",
    "- Writers don't write the entire book in one paragraph.\n",
    "- We have the relevant chapter/section/sub-sections and paragraphs.\n",
    "- RAG does the same with your documents\n",
    "\n",
    "### The Chunking Challenge\n",
    "| Too Small üìè | Just Right ‚úÖ | Too Large üìê |\n",
    "|--------------|---------------|---------------|\n",
    "| Loses context | Preserves meaning | Hard to match |\n",
    "| Poor retrieval | Good relevance | Noise |\n",
    "| Fast processing | Balanced | Slow processing |\n",
    "\n",
    "### 2025 Chunking Evolution üöÄ\n",
    "- **Semantic Chunking**: AI determines natural boundaries\n",
    "- **Adaptive Chunking**: Dynamic sizing based on content type\n",
    "- **Contextual Preservation**: Better metadata and overlap strategies\n",
    "- **Embedding-Based Splitting**: Use embeddings to find semantic breaks\n",
    "\n",
    "### Common Chunking Strategies\n",
    "1. **Fixed-Size**: Split by character/token count\n",
    "2. **Semantic**: Split by meaning and context\n",
    "3. **Recursive**: Try multiple separators hierarchically\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b17049",
   "metadata": {},
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    TokenTextSplitter,\n",
    "    MarkdownHeaderTextSplitter\n",
    ")\n",
    "from langchain.schema import Document\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "# For token counting\n",
    "import tiktoken\n",
    "\n",
    "# For semantic chunking (2025 approach)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üî™ Ready to chunk documents!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cc1e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "### will move this to a separate file..\n",
    " \n",
    "def analyze_chunks(chunks, method_name):\n",
    "    \"\"\"\n",
    "    Analyze and visualize chunk characteristics\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä {method_name} Analysis:\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        # Calculate statistics\n",
    "        chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "        \n",
    "        print(f\"   Avg chunk size: {np.mean(chunk_sizes):.0f} chars\")\n",
    "        print(f\"   Min chunk size: {min(chunk_sizes)} chars\")\n",
    "        print(f\"   Max chunk size: {max(chunk_sizes)} chars\")\n",
    "        print(f\"   Std deviation: {np.std(chunk_sizes):.0f} chars\")\n",
    "        \n",
    "        # Show first chunk preview\n",
    "        print(f\"\\nüìù First chunk preview:\")\n",
    "        print(f\"   {chunks[0].page_content[:150]}...\")\n",
    "        \n",
    "        return chunk_sizes\n",
    "    return []\n",
    "\n",
    "# Test different fixed-size approaches\n",
    "print(\"üî™ FIXED-SIZE CHUNKING EXPERIMENTS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the technical document for comparison\n",
    "test_doc = docs[0]  # API documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa29de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Character-based chunking\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separator=\"\\n\\n\"  # Split on paragraphs when possible\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_documents([test_doc])\n",
    "char_sizes = analyze_chunks(char_chunks, \"Character-based (500 chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f5904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Token-based chunking (more precise for LLMs)\n",
    "token_splitter = TokenTextSplitter(\n",
    "    chunk_size=100,  # 100 tokens\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_documents([test_doc])\n",
    "token_sizes = analyze_chunks(token_chunks, \"Token-based (100 tokens)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32276447",
   "metadata": {},
   "outputs": [],
   "source": [
    "## todo: add metadata section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module 4: Understanding Embedding Models\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand the history of embeddings\n",
    "- Create your first embeddings\n",
    "- Visualize embeddings\n",
    "- Convert the chunks into embeddings\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "\n",
    "\n",
    "**Modern text embeddings are similar but much more powerful:**\n",
    "- **Contextual**: Same word has different embeddings in different contexts\n",
    "- **Semantic**: Capture meaning, not just word co-occurrence\n",
    "\n",
    "### Embedding Evolution Timeline üìà\n",
    "| Era | Approach | Example | Context |\n",
    "|-----|----------|---------|----------|\n",
    "| 2013 | Static Word Vectors | Word2Vec | One vector per word |\n",
    "| 2018 | Contextualized | BERT | Different vectors per context |\n",
    "| 2019 | Sentence-Level | Sentence-BERT | Optimized for sentences |\n",
    "| 2023 | Large-Scale | OpenAI text-embedding-3 | Billions of parameters |\n",
    "| 2025 | Specialized | NV-Embed-v2, Stella | Domain & task optimized |\n",
    "\n",
    "### 2025 MTEB Leaderboard Leaders üèÜ\n",
    "- **NV-Embed-v2**: 72.31 score (NVIDIA, Mistral-7B based)\n",
    "- **Stella-1.5B**: Best open-source with commercial license\n",
    "- **text-embedding-3-large**: OpenAI's flagship (64.6% MTEB)\n",
    "- **EmbeddingGemma**: Google's best under 500M parameters\n",
    "- **Voyage-3**: Strong commercial performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e39ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all chunks\n",
    "import time\n",
    "\n",
    "print(\"Generating embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Extract text content from documents\n",
    "texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Generate embeddings in batch (more efficient)\n",
    "embeddings = embedding_model.embed_documents(texts)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Generated {len(embeddings)} embeddings in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")\n",
    "\n",
    "# Preview first embedding\n",
    "print(f\"First embedding (first 10 values): {embeddings[0][:10]}\")\n",
    "print(f\"Embedding magnitude: {sum(x**2 for x in embeddings[0])**0.5:.3f}\")  # Should be ~1.0 if normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5782ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## viz.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8acd584",
   "metadata": {},
   "source": [
    "# Module 4: Vector Stores & Databases\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand vector database architecture and why traditional databases aren't suitable for similarity search\n",
    "- Compare different vector store options (local vs cloud, open-source vs commercial)\n",
    "- Implement CRUD operations for vectors with metadata\n",
    "- Design effective metadata schemas for filtering and organization\n",
    "- Benchmark and optimize vector database performance\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### Why Vector Databases?\n",
    "\n",
    "Traditional databases excel at exact matches but struggle with **similarity search**:\n",
    "\n",
    "\n",
    "#### üö´ Traditional Database Limitations\n",
    "1. **No built-in similarity search**: SQL doesn't understand \"semantic closeness\"\n",
    "2. **Poor scalability**: Linear scan becomes prohibitive with millions of vectors\n",
    "\n",
    "#### ‚úÖ Vector Database Advantages\n",
    "1. **Approximate Nearest Neighbor (ANN)**: Fast similarity search with controllable accuracy\n",
    "3. **Metadata filtering**: Combine vector similarity with traditional filters\n",
    "4. **Horizontal scaling**: Built for production workloads with millions/billions of vectors\n",
    "\n",
    "### 2025 Vector Database Landscape üèÜ\n",
    "\n",
    "| Database | Query Latency | Cost | Best For |\n",
    "|----------|---------------|------|---------|\n",
    "| **Pinecone** | 23ms p95 | High | Enterprise, turnkey scale |\n",
    "| **Qdrant** | ~30ms | Low | Complex filters, self-hosted |\n",
    "| **Weaviate** | 34ms p95 | Medium | OSS flexibility, GraphQL |\n",
    "| **Milvus** | Lowest | Variable | GPU acceleration |\n",
    "| **Chroma** | 20ms p50 | Free | Fast prototyping |\n",
    "\n",
    "### Database Categories\n",
    "\n",
    "#### üè† Local/Embedded Options\n",
    "- **Chroma**: SQLite-based, perfect for development\n",
    "- **FAISS**: Meta's library, CPU/GPU optimized\n",
    "- **Hnswlib**: Pure HNSW implementation, very fast\n",
    "\n",
    "#### ‚òÅÔ∏è Cloud/Managed Options\n",
    "- **Pinecone**: Fully managed, highest performance\n",
    "- **Weaviate**: Open-source with cloud hosting\n",
    "- **Qdrant**: Rust-based, excellent cost/performance\n",
    "\n",
    "#### üóÑÔ∏è Traditional DB Extensions\n",
    "- **pgvector**: PostgreSQL extension\n",
    "- **Redis**: In-memory vector search\n",
    "- **Elasticsearch**: Dense vector search support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6a2f5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20db1538",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
