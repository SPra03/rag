{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 8: Search Methods Comparison\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand the fundamental differences between semantic and lexical search\n",
    "- Implement both BM25 (lexical) and vector similarity (semantic) search\n",
    "- Compare results on different query types and identify when to use each approach\n",
    "- Design and implement hybrid search strategies using Reciprocal Rank Fusion (RRF)\n",
    "- Implement advanced techniques like HyDE and contextual retrieval\n",
    "\n",
    "## ðŸ“š Key Concepts\n",
    "\n",
    "### The Search Method Spectrum\n",
    "\n",
    "Modern information retrieval spans a spectrum from **exact matching** to **semantic understanding**:\n",
    "\n",
    "```\n",
    "Lexical Search â†â†’ Semantic Search â†â†’ Hybrid Search\n",
    "(Exact Terms)      (Meaning)        (Best of Both)\n",
    "```\n",
    "\n",
    "### ðŸ”¤ Lexical Search (Traditional)\n",
    "\n",
    "**How it works**: Matches words and phrases directly\n",
    "**Algorithm**: BM25 (Best Matching 25) - the gold standard\n",
    "**Strengths**:\n",
    "- Excellent for exact terms, names, IDs\n",
    "- Fast and resource-efficient\n",
    "- Explainable results\n",
    "- No training required\n",
    "\n",
    "**Weaknesses**:\n",
    "- Misses synonyms and paraphrases\n",
    "- No understanding of context or meaning\n",
    "- Sensitive to spelling variations\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Query: \"machine learning algorithms\"\n",
    "âœ… Matches: \"machine learning algorithms are powerful\"\n",
    "âŒ Misses: \"AI techniques for pattern recognition\" (same concept, different words)\n",
    "```\n",
    "\n",
    "### ðŸ§  Semantic Search (Modern)\n",
    "\n",
    "**How it works**: Understands meaning through vector embeddings\n",
    "**Algorithm**: Vector similarity (cosine, dot product)\n",
    "**Strengths**:\n",
    "- Captures meaning and intent\n",
    "- Handles synonyms and paraphrases\n",
    "- Works across languages\n",
    "- Good for conceptual queries\n",
    "\n",
    "**Weaknesses**:\n",
    "- May miss exact terms if not semantically emphasized\n",
    "- Computationally expensive\n",
    "- Less explainable\n",
    "- Requires quality embeddings\n",
    "\n",
    "**Example**:\n",
    "```\n",
    "Query: \"machine learning algorithms\"\n",
    "âœ… Matches: \"AI techniques for pattern recognition\" (understands the concept)\n",
    "âŒ Might miss: \"ML-2024-Report-v3.pdf\" (exact filename)\n",
    "```\n",
    "\n",
    "### ðŸ”„ Hybrid Search (Best Practice 2025)\n",
    "\n",
    "**How it works**: Combines both approaches intelligently\n",
    "**Standard**: BM25 + Vector Similarity with RRF fusion\n",
    "**Result**: Gets benefits of both methods\n",
    "\n",
    "### 2025 Production Standards ðŸ†\n",
    "\n",
    "| Method | Use Case | Performance | Adoption |\n",
    "|--------|----------|-------------|----------|\n",
    "| **BM25 Only** | Legacy systems, exact matching | Fast | Declining |\n",
    "| **Semantic Only** | Research, conceptual queries | Moderate | Common |\n",
    "| **Hybrid (BM25+Semantic)** | Production systems | Best | Standard |\n",
    "\n",
    "### Advanced Techniques (2025)\n",
    "\n",
    "1. **Reciprocal Rank Fusion (RRF)**: Standard algorithm for combining search results\n",
    "2. **HyDE**: Generate hypothetical documents to improve zero-shot performance\n",
    "3. **Contextual Retrieval**: Add contextual information to chunks (49% improvement)\n",
    "4. **Multi-Query Expansion**: Generate multiple query variations for better coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup\n",
    "Let's install the required packages and set up our search comparison lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q rank-bm25 sentence-transformers scikit-learn numpy pandas matplotlib seaborn\n",
    "!pip install -q langchain langchain-community openai python-dotenv\n",
    "!pip install -q nltk textstat wordcloud plotly\n",
    "# Note: For production, also add: elasticsearch, weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Search libraries\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import textstat\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# LangChain for advanced techniques\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(f\"ðŸ“… Today's date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 1: Implementing and Comparing Search Methods\n",
    "\n",
    "Let's implement both lexical and semantic search to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchMethodComparison:\n",
    "    \"\"\"Compare different search methods: lexical, semantic, and hybrid\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize search indexes\n",
    "        self.bm25_index = None\n",
    "        self.embeddings = None\n",
    "        self.documents = []\n",
    "        self.tokenized_docs = []\n",
    "        \n",
    "    def create_sample_dataset(self) -> List[str]:\n",
    "        \"\"\"Create a diverse dataset for search comparison\"\"\"\n",
    "        documents = [\n",
    "            # AI/ML Documents (conceptual)\n",
    "            \"Machine learning algorithms enable computers to learn patterns from data without explicit programming.\",\n",
    "            \"Deep learning uses neural networks with multiple layers to model complex patterns.\",\n",
    "            \"Artificial intelligence techniques include supervised learning, unsupervised learning, and reinforcement learning.\",\n",
    "            \"Natural language processing helps computers understand and generate human language.\",\n",
    "            \"Computer vision algorithms analyze and interpret visual information from images and videos.\",\n",
    "            \n",
    "            # Technical Documents (specific terms)\n",
    "            \"The TensorFlow library version 2.14.0 includes new optimizations for GPU computation.\",\n",
    "            \"PyTorch framework provides dynamic computational graphs for research applications.\",\n",
    "            \"CUDA programming enables parallel computing on NVIDIA graphics processing units.\",\n",
    "            \"Docker containers provide consistent deployment environments across different systems.\",\n",
    "            \"Kubernetes orchestrates containerized applications at scale in production environments.\",\n",
    "            \n",
    "            # Business Documents (mixed)\n",
    "            \"Q4 revenue increased by 15% due to strong performance in the AI products division.\",\n",
    "            \"The company's machine learning initiatives generated $50M in additional revenue this quarter.\",\n",
    "            \"Customer satisfaction scores improved significantly after implementing AI-powered support tools.\",\n",
    "            \"Data science team delivered 12 predictive models for various business units.\",\n",
    "            \"Investment in artificial intelligence research and development reached $100M this year.\",\n",
    "            \n",
    "            # Research Papers (academic)\n",
    "            \"Attention mechanisms in transformer architectures enable better sequence modeling.\",\n",
    "            \"Pre-trained language models achieve state-of-the-art results on downstream NLP tasks.\",\n",
    "            \"Contrastive learning improves representation learning in self-supervised settings.\",\n",
    "            \"Graph neural networks effectively model relationships between entities in complex datasets.\",\n",
    "            \"Federated learning enables distributed training while preserving data privacy.\",\n",
    "            \n",
    "            # Code Documentation (technical)\n",
    "            \"The function calculate_accuracy() computes classification performance metrics.\",\n",
    "            \"Use train_model(X_train, y_train) to fit the machine learning model on training data.\",\n",
    "            \"The API endpoint /api/v1/predict accepts JSON payload with feature vectors.\",\n",
    "            \"Error handling in ML pipelines prevents crashes during data preprocessing steps.\",\n",
    "            \"Model versioning tracks changes to algorithm parameters and training configurations.\",\n",
    "            \n",
    "            # News Articles (current events style)\n",
    "            \"Tech giants invest billions in generative AI research to compete with OpenAI ChatGPT.\",\n",
    "            \"New breakthrough in quantum computing could revolutionize machine learning algorithms.\",\n",
    "            \"Regulatory concerns grow around AI safety and potential risks of advanced systems.\",\n",
    "            \"Healthcare industry adopts AI diagnostic tools to improve patient outcomes.\",\n",
    "            \"Educational institutions integrate AI tutoring systems into online learning platforms.\"\n",
    "        ]\n",
    "        \n",
    "        return documents\n",
    "    \n",
    "    def preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Preprocess text for BM25 indexing\"\"\"\n",
    "        # Convert to lowercase and tokenize\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove non-alphabetic tokens and stop words\n",
    "        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n",
    "        \n",
    "        # Apply stemming\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def build_indexes(self, documents: List[str]) -> None:\n",
    "        \"\"\"Build both BM25 and semantic indexes\"\"\"\n",
    "        self.documents = documents\n",
    "        \n",
    "        print(\"Building search indexes...\")\n",
    "        \n",
    "        # 1. Build BM25 index\n",
    "        print(\"   Building BM25 index...\")\n",
    "        self.tokenized_docs = [self.preprocess_text(doc) for doc in documents]\n",
    "        self.bm25_index = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        # 2. Build semantic index\n",
    "        print(\"   Building semantic index...\")\n",
    "        self.embeddings = self.embedding_model.encode(documents)\n",
    "        # Normalize for cosine similarity\n",
    "        self.embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        print(f\"âœ… Indexes built for {len(documents)} documents\")\n",
    "    \n",
    "    def lexical_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Perform BM25 lexical search\"\"\"\n",
    "        query_tokens = self.preprocess_text(query)\n",
    "        scores = self.bm25_index.get_scores(query_tokens)\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            if scores[idx] > 0:  # Only include documents with non-zero scores\n",
    "                results.append((idx, scores[idx], self.documents[idx]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def semantic_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Perform semantic vector search\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((idx, similarities[idx], self.documents[idx]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def compare_search_methods(self, queries: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Compare lexical and semantic search across multiple queries\"\"\"\n",
    "        comparison_results = []\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nðŸ” Query: '{query}'\")\n",
    "            \n",
    "            # Get results from both methods\n",
    "            lexical_results = self.lexical_search(query, top_k=5)\n",
    "            semantic_results = self.semantic_search(query, top_k=5)\n",
    "            \n",
    "            # Analyze overlap\n",
    "            lexical_indices = set([r[0] for r in lexical_results])\n",
    "            semantic_indices = set([r[0] for r in semantic_results])\n",
    "            overlap = len(lexical_indices.intersection(semantic_indices))\n",
    "            \n",
    "            print(f\"   ðŸ“Š Overlap: {overlap}/5 documents\")\n",
    "            \n",
    "            # Display top results\n",
    "            print(\"   ðŸ”¤ BM25 (Lexical) Top 3:\")\n",
    "            for i, (idx, score, doc) in enumerate(lexical_results[:3]):\n",
    "                print(f\"      {i+1}. [{score:.3f}] {doc[:80]}...\")\n",
    "            \n",
    "            print(\"   ðŸ§  Semantic Top 3:\")\n",
    "            for i, (idx, score, doc) in enumerate(semantic_results[:3]):\n",
    "                print(f\"      {i+1}. [{score:.3f}] {doc[:80]}...\")\n",
    "            \n",
    "            comparison_results.append({\n",
    "                'query': query,\n",
    "                'lexical_results': lexical_results,\n",
    "                'semantic_results': semantic_results,\n",
    "                'overlap': overlap,\n",
    "                'overlap_ratio': overlap / min(len(lexical_results), len(semantic_results)) if min(len(lexical_results), len(semantic_results)) > 0 else 0\n",
    "            })\n",
    "        \n",
    "        return comparison_results\n",
    "    \n",
    "    def analyze_query_characteristics(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze query characteristics to predict which search method might work better\"\"\"\n",
    "        characteristics = {}\n",
    "        \n",
    "        # Basic statistics\n",
    "        characteristics['length'] = len(query.split())\n",
    "        characteristics['has_quotes'] = '\"' in query\n",
    "        characteristics['has_technical_terms'] = bool(re.search(r'\\b(API|GPU|CPU|v\\d+\\.\\d+|[A-Z]{2,})\\b', query))\n",
    "        characteristics['has_numbers'] = bool(re.search(r'\\d+', query))\n",
    "        \n",
    "        # Readability (higher = more complex)\n",
    "        characteristics['readability'] = textstat.flesch_reading_ease(query)\n",
    "        \n",
    "        # Predict best method\n",
    "        if characteristics['has_technical_terms'] or characteristics['has_quotes'] or characteristics['has_numbers']:\n",
    "            characteristics['predicted_best'] = 'lexical'\n",
    "            characteristics['reason'] = 'Contains specific terms, numbers, or exact phrases'\n",
    "        elif characteristics['length'] > 5 and characteristics['readability'] > 60:\n",
    "            characteristics['predicted_best'] = 'semantic'\n",
    "            characteristics['reason'] = 'Long conceptual query'\n",
    "        else:\n",
    "            characteristics['predicted_best'] = 'hybrid'\n",
    "            characteristics['reason'] = 'Mixed characteristics, hybrid approach recommended'\n",
    "        \n",
    "        return characteristics\n",
    "\n",
    "# Initialize the search comparison\n",
    "search_lab = SearchMethodComparison()\n",
    "print(\"ðŸ” Search Method Comparison Lab initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and build indexes\n",
    "documents = search_lab.create_sample_dataset()\n",
    "search_lab.build_indexes(documents)\n",
    "\n",
    "print(f\"ðŸ“š Dataset Statistics:\")\n",
    "print(f\"   Total documents: {len(documents)}\")\n",
    "print(f\"   Average document length: {np.mean([len(doc.split()) for doc in documents]):.1f} words\")\n",
    "print(f\"   Document categories: AI/ML, Technical, Business, Research, Code, News\")\n",
    "\n",
    "# Display sample documents\n",
    "print(\"\\nðŸ“„ Sample Documents:\")\n",
    "for i, doc in enumerate(documents[:3]):\n",
    "    print(f\"   {i+1}. {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different types of queries\n",
    "test_queries = [\n",
    "    # Conceptual queries (should favor semantic)\n",
    "    \"AI techniques for pattern recognition\",\n",
    "    \"Methods to improve business performance\",\n",
    "    \"Understanding human language with computers\",\n",
    "    \n",
    "    # Specific term queries (should favor lexical)\n",
    "    \"TensorFlow version 2.14.0\",\n",
    "    \"calculate_accuracy() function\",\n",
    "    \"Q4 revenue 15%\",\n",
    "    \n",
    "    # Mixed queries (could benefit from hybrid)\n",
    "    \"machine learning model training\",\n",
    "    \"AI research investment\",\n",
    "    \"neural network optimization\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§ª SEARCH METHOD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_results = search_lab.compare_search_methods(test_queries)\n",
    "\n",
    "# Analyze query characteristics\n",
    "print(\"\\nðŸ” QUERY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries[:6]:  # Analyze first 6 queries\n",
    "    characteristics = search_lab.analyze_query_characteristics(query)\n",
    "    print(f\"\\nðŸ“ Query: '{query}'\")\n",
    "    print(f\"   Length: {characteristics['length']} words\")\n",
    "    print(f\"   Technical terms: {characteristics['has_technical_terms']}\")\n",
    "    print(f\"   Contains numbers: {characteristics['has_numbers']}\")\n",
    "    print(f\"   Predicted best method: {characteristics['predicted_best']}\")\n",
    "    print(f\"   Reason: {characteristics['reason']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "overlap_ratios = [result['overlap_ratio'] for result in comparison_results]\n",
    "query_lengths = [len(result['query'].split()) for result in comparison_results]\n",
    "query_labels = [result['query'][:30] + \"...\" if len(result['query']) > 30 else result['query'] \n",
    "               for result in comparison_results]\n",
    "\n",
    "# Create visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Overlap between search methods\n",
    "ax1.bar(range(len(comparison_results)), overlap_ratios, color='skyblue')\n",
    "ax1.set_title('Overlap Between Lexical and Semantic Search', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Query Index')\n",
    "ax1.set_ylabel('Overlap Ratio (0-1)')\n",
    "ax1.set_xticks(range(len(comparison_results)))\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, ratio in enumerate(overlap_ratios):\n",
    "    ax1.text(i, ratio + 0.02, f'{ratio:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Query length vs overlap\n",
    "ax2.scatter(query_lengths, overlap_ratios, s=100, alpha=0.7, color='orange')\n",
    "ax2.set_title('Query Length vs Search Method Overlap', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Query Length (words)')\n",
    "ax2.set_ylabel('Overlap Ratio')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(query_lengths, overlap_ratios, 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(sorted(query_lengths), p(sorted(query_lengths)), \"r--\", alpha=0.8)\n",
    "\n",
    "# 3. Score distribution comparison for one query\n",
    "sample_query_idx = 0\n",
    "sample_result = comparison_results[sample_query_idx]\n",
    "lexical_scores = [r[1] for r in sample_result['lexical_results']]\n",
    "semantic_scores = [r[1] for r in sample_result['semantic_results']]\n",
    "\n",
    "x_pos = np.arange(len(lexical_scores))\n",
    "width = 0.35\n",
    "\n",
    "ax3.bar(x_pos - width/2, lexical_scores, width, label='BM25 (Lexical)', alpha=0.8)\n",
    "ax3.bar(x_pos + width/2, semantic_scores, width, label='Semantic', alpha=0.8)\n",
    "ax3.set_title(f'Score Distribution: \"{sample_result[\"query\"]}\"', fontweight='bold', fontsize=14)\n",
    "ax3.set_xlabel('Rank Position')\n",
    "ax3.set_ylabel('Score')\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels([f'#{i+1}' for i in range(len(lexical_scores))])\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Method performance heatmap\n",
    "performance_matrix = np.zeros((len(comparison_results), 2))\n",
    "for i, result in enumerate(comparison_results):\n",
    "    # Use average score as performance metric\n",
    "    lexical_avg = np.mean([r[1] for r in result['lexical_results']]) if result['lexical_results'] else 0\n",
    "    semantic_avg = np.mean([r[1] for r in result['semantic_results']]) if result['semantic_results'] else 0\n",
    "    performance_matrix[i] = [lexical_avg, semantic_avg]\n",
    "\n",
    "im = ax4.imshow(performance_matrix.T, cmap='YlOrRd', aspect='auto')\n",
    "ax4.set_title('Performance Heatmap by Query Type', fontweight='bold', fontsize=14)\n",
    "ax4.set_xlabel('Query Index')\n",
    "ax4.set_ylabel('Search Method')\n",
    "ax4.set_yticks([0, 1])\n",
    "ax4.set_yticklabels(['BM25 (Lexical)', 'Semantic'])\n",
    "ax4.set_xticks(range(len(comparison_results)))\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar(im, ax=ax4, label='Average Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Observations:\")\n",
    "print(\"- Lower overlap indicates methods find different relevant documents\")\n",
    "print(\"- Conceptual queries often show lower overlap (semantic finds different results)\")\n",
    "print(\"- Specific term queries show higher overlap (both methods find same exact matches)\")\n",
    "print(\"- This validates the need for hybrid approaches in production systems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Exercise 2: Hybrid Search with Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "Let's implement hybrid search using the 2025 production standard: RRF fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSearchEngine:\n",
    "    \"\"\"Advanced hybrid search with multiple fusion strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, search_lab: SearchMethodComparison):\n",
    "        self.search_lab = search_lab\n",
    "        self.embedding_model = search_lab.embedding_model\n",
    "        \n",
    "    def reciprocal_rank_fusion(self, lexical_results: List[Tuple], semantic_results: List[Tuple], \n",
    "                              k: int = 60, alpha: float = 0.5) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Combine search results using Reciprocal Rank Fusion (RRF)\"\"\"\n",
    "        \n",
    "        # Create document score dictionaries\n",
    "        lexical_scores = {}\n",
    "        semantic_scores = {}\n",
    "        \n",
    "        # Calculate RRF scores for lexical results\n",
    "        for rank, (doc_id, score, doc_text) in enumerate(lexical_results):\n",
    "            lexical_scores[doc_id] = 1 / (k + rank + 1)\n",
    "        \n",
    "        # Calculate RRF scores for semantic results\n",
    "        for rank, (doc_id, score, doc_text) in enumerate(semantic_results):\n",
    "            semantic_scores[doc_id] = 1 / (k + rank + 1)\n",
    "        \n",
    "        # Combine scores\n",
    "        all_doc_ids = set(lexical_scores.keys()) | set(semantic_scores.keys())\n",
    "        combined_scores = {}\n",
    "        \n",
    "        for doc_id in all_doc_ids:\n",
    "            lexical_rrf = lexical_scores.get(doc_id, 0)\n",
    "            semantic_rrf = semantic_scores.get(doc_id, 0)\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_scores[doc_id] = (1 - alpha) * lexical_rrf + alpha * semantic_rrf\n",
    "        \n",
    "        # Sort by combined score and return top results\n",
    "        sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in sorted_docs:\n",
    "            doc_text = self.search_lab.documents[doc_id]\n",
    "            results.append((doc_id, score, doc_text))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def weighted_sum_fusion(self, lexical_results: List[Tuple], semantic_results: List[Tuple], \n",
    "                           alpha: float = 0.7) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Combine search results using weighted sum of normalized scores\"\"\"\n",
    "        \n",
    "        # Normalize scores to [0, 1] range\n",
    "        if lexical_results:\n",
    "            max_lexical = max(r[1] for r in lexical_results)\n",
    "            min_lexical = min(r[1] for r in lexical_results)\n",
    "            lexical_range = max_lexical - min_lexical if max_lexical != min_lexical else 1\n",
    "        else:\n",
    "            max_lexical = min_lexical = lexical_range = 1\n",
    "        \n",
    "        if semantic_results:\n",
    "            max_semantic = max(r[1] for r in semantic_results)\n",
    "            min_semantic = min(r[1] for r in semantic_results)\n",
    "            semantic_range = max_semantic - min_semantic if max_semantic != min_semantic else 1\n",
    "        else:\n",
    "            max_semantic = min_semantic = semantic_range = 1\n",
    "        \n",
    "        # Create normalized score dictionaries\n",
    "        lexical_scores = {}\n",
    "        semantic_scores = {}\n",
    "        \n",
    "        for doc_id, score, doc_text in lexical_results:\n",
    "            normalized_score = (score - min_lexical) / lexical_range\n",
    "            lexical_scores[doc_id] = normalized_score\n",
    "        \n",
    "        for doc_id, score, doc_text in semantic_results:\n",
    "            normalized_score = (score - min_semantic) / semantic_range\n",
    "            semantic_scores[doc_id] = normalized_score\n",
    "        \n",
    "        # Combine scores\n",
    "        all_doc_ids = set(lexical_scores.keys()) | set(semantic_scores.keys())\n",
    "        combined_scores = {}\n",
    "        \n",
    "        for doc_id in all_doc_ids:\n",
    "            lexical_norm = lexical_scores.get(doc_id, 0)\n",
    "            semantic_norm = semantic_scores.get(doc_id, 0)\n",
    "            \n",
    "            # Weighted combination\n",
    "            combined_scores[doc_id] = (1 - alpha) * lexical_norm + alpha * semantic_norm\n",
    "        \n",
    "        # Sort and return results\n",
    "        sorted_docs = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in sorted_docs:\n",
    "            doc_text = self.search_lab.documents[doc_id]\n",
    "            results.append((doc_id, score, doc_text))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def hybrid_search(self, query: str, top_k: int = 10, fusion_method: str = \"rrf\", \n",
    "                     alpha: float = 0.7) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Perform hybrid search with specified fusion method\"\"\"\n",
    "        \n",
    "        # Get results from both search methods\n",
    "        lexical_results = self.search_lab.lexical_search(query, top_k=top_k)\n",
    "        semantic_results = self.search_lab.semantic_search(query, top_k=top_k)\n",
    "        \n",
    "        # Apply fusion method\n",
    "        if fusion_method == \"rrf\":\n",
    "            combined_results = self.reciprocal_rank_fusion(lexical_results, semantic_results, alpha=alpha)\n",
    "        elif fusion_method == \"weighted_sum\":\n",
    "            combined_results = self.weighted_sum_fusion(lexical_results, semantic_results, alpha=alpha)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown fusion method: {fusion_method}\")\n",
    "        \n",
    "        return combined_results[:top_k]\n",
    "    \n",
    "    def compare_fusion_methods(self, queries: List[str], alpha_values: List[float] = [0.3, 0.5, 0.7]) -> pd.DataFrame:\n",
    "        \"\"\"Compare different fusion methods and alpha values\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for query in queries:\n",
    "            print(f\"\\nðŸ” Testing: '{query}'\")\n",
    "            \n",
    "            # Get baseline results\n",
    "            lexical_results = self.search_lab.lexical_search(query, top_k=5)\n",
    "            semantic_results = self.search_lab.semantic_search(query, top_k=5)\n",
    "            \n",
    "            for alpha in alpha_values:\n",
    "                # Test RRF\n",
    "                rrf_results = self.hybrid_search(query, top_k=5, fusion_method=\"rrf\", alpha=alpha)\n",
    "                \n",
    "                # Test Weighted Sum\n",
    "                ws_results = self.hybrid_search(query, top_k=5, fusion_method=\"weighted_sum\", alpha=alpha)\n",
    "                \n",
    "                # Calculate diversity metrics\n",
    "                lexical_docs = set(r[0] for r in lexical_results)\n",
    "                semantic_docs = set(r[0] for r in semantic_results)\n",
    "                rrf_docs = set(r[0] for r in rrf_results)\n",
    "                ws_docs = set(r[0] for r in ws_results)\n",
    "                \n",
    "                results.append({\n",
    "                    'query': query[:30] + \"...\" if len(query) > 30 else query,\n",
    "                    'alpha': alpha,\n",
    "                    'method': 'RRF',\n",
    "                    'unique_docs': len(rrf_docs),\n",
    "                    'overlap_with_lexical': len(rrf_docs.intersection(lexical_docs)),\n",
    "                    'overlap_with_semantic': len(rrf_docs.intersection(semantic_docs)),\n",
    "                    'avg_score': np.mean([r[1] for r in rrf_results]) if rrf_results else 0\n",
    "                })\n",
    "                \n",
    "                results.append({\n",
    "                    'query': query[:30] + \"...\" if len(query) > 30 else query,\n",
    "                    'alpha': alpha,\n",
    "                    'method': 'Weighted Sum',\n",
    "                    'unique_docs': len(ws_docs),\n",
    "                    'overlap_with_lexical': len(ws_docs.intersection(lexical_docs)),\n",
    "                    'overlap_with_semantic': len(ws_docs.intersection(semantic_docs)),\n",
    "                    'avg_score': np.mean([r[1] for r in ws_results]) if ws_results else 0\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Initialize hybrid search engine\n",
    "hybrid_engine = HybridSearchEngine(search_lab)\n",
    "print(\"ðŸ”„ Hybrid Search Engine initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hybrid search with different methods\n",
    "test_query = \"machine learning algorithms for business applications\"\n",
    "\n",
    "print(f\"ðŸ§ª HYBRID SEARCH COMPARISON\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compare different fusion methods\n",
    "methods = [\n",
    "    (\"lexical\", \"BM25 Only\"),\n",
    "    (\"semantic\", \"Semantic Only\"),\n",
    "    (\"rrf\", \"RRF Fusion (Î±=0.7)\"),\n",
    "    (\"weighted_sum\", \"Weighted Sum (Î±=0.7)\")\n",
    "]\n",
    "\n",
    "all_results = {}\n",
    "\n",
    "for method_key, method_name in methods:\n",
    "    print(f\"\\nðŸ“Š {method_name}:\")\n",
    "    \n",
    "    if method_key == \"lexical\":\n",
    "        results = search_lab.lexical_search(test_query, top_k=5)\n",
    "    elif method_key == \"semantic\":\n",
    "        results = search_lab.semantic_search(test_query, top_k=5)\n",
    "    else:\n",
    "        results = hybrid_engine.hybrid_search(test_query, top_k=5, fusion_method=method_key, alpha=0.7)\n",
    "    \n",
    "    all_results[method_key] = results\n",
    "    \n",
    "    for i, (doc_id, score, doc_text) in enumerate(results):\n",
    "        print(f\"   {i+1}. [{score:.4f}] {doc_text[:70]}...\")\n",
    "\n",
    "# Analyze result diversity\n",
    "print(\"\\nðŸ” RESULT DIVERSITY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Calculate unique documents found by each method\n",
    "for method_key, method_name in methods:\n",
    "    doc_ids = set(r[0] for r in all_results[method_key])\n",
    "    print(f\"{method_name}: {len(doc_ids)} unique documents\")\n",
    "\n",
    "# Calculate overlap between methods\n",
    "lexical_docs = set(r[0] for r in all_results['lexical'])\n",
    "semantic_docs = set(r[0] for r in all_results['semantic'])\n",
    "rrf_docs = set(r[0] for r in all_results['rrf'])\n",
    "\n",
    "print(f\"\\nOverlap Analysis:\")\n",
    "print(f\"  Lexical âˆ© Semantic: {len(lexical_docs.intersection(semantic_docs))} documents\")\n",
    "print(f\"  RRF âˆ© Lexical: {len(rrf_docs.intersection(lexical_docs))} documents\")\n",
    "print(f\"  RRF âˆ© Semantic: {len(rrf_docs.intersection(semantic_docs))} documents\")\n",
    "print(f\"  All three methods: {len(lexical_docs.intersection(semantic_docs).intersection(rrf_docs))} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different alpha values for hybrid search\n",
    "print(\"\\nâš–ï¸ ALPHA PARAMETER TUNING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "alpha_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "sample_queries = [\n",
    "    \"TensorFlow version optimization\",  # Technical query\n",
    "    \"AI techniques for pattern recognition\",  # Conceptual query\n",
    "    \"revenue performance business metrics\"  # Mixed query\n",
    "]\n",
    "\n",
    "# Test alpha sensitivity\n",
    "alpha_results = []\n",
    "\n",
    "for query in sample_queries:\n",
    "    print(f\"\\nðŸ” Query: '{query}'\")\n",
    "    \n",
    "    for alpha in alpha_values:\n",
    "        rrf_results = hybrid_engine.hybrid_search(query, top_k=3, fusion_method=\"rrf\", alpha=alpha)\n",
    "        \n",
    "        # Calculate method bias (how much each method contributes)\n",
    "        lexical_only = set(r[0] for r in search_lab.lexical_search(query, top_k=10))\n",
    "        semantic_only = set(r[0] for r in search_lab.semantic_search(query, top_k=10))\n",
    "        hybrid_docs = set(r[0] for r in rrf_results)\n",
    "        \n",
    "        lexical_bias = len(hybrid_docs.intersection(lexical_only)) / len(hybrid_docs) if hybrid_docs else 0\n",
    "        semantic_bias = len(hybrid_docs.intersection(semantic_only)) / len(hybrid_docs) if hybrid_docs else 0\n",
    "        \n",
    "        alpha_results.append({\n",
    "            'query_type': 'technical' if 'TensorFlow' in query else ('conceptual' if 'pattern recognition' in query else 'mixed'),\n",
    "            'alpha': alpha,\n",
    "            'lexical_bias': lexical_bias,\n",
    "            'semantic_bias': semantic_bias,\n",
    "            'avg_score': np.mean([r[1] for r in rrf_results]) if rrf_results else 0\n",
    "        })\n",
    "        \n",
    "        print(f\"   Î±={alpha}: Lexical bias {lexical_bias:.2f}, Semantic bias {semantic_bias:.2f}\")\n",
    "\n",
    "# Visualize alpha effects\n",
    "alpha_df = pd.DataFrame(alpha_results)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Alpha vs method bias\n",
    "for query_type in alpha_df['query_type'].unique():\n",
    "    subset = alpha_df[alpha_df['query_type'] == query_type]\n",
    "    ax1.plot(subset['alpha'], subset['semantic_bias'], marker='o', label=f'{query_type} (semantic bias)')\n",
    "    ax1.plot(subset['alpha'], subset['lexical_bias'], marker='s', linestyle='--', label=f'{query_type} (lexical bias)')\n",
    "\n",
    "ax1.set_title('Effect of Alpha on Method Bias', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Alpha Value (0=lexical, 1=semantic)')\n",
    "ax1.set_ylabel('Bias Score')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Alpha vs average score\n",
    "for query_type in alpha_df['query_type'].unique():\n",
    "    subset = alpha_df[alpha_df['query_type'] == query_type]\n",
    "    ax2.plot(subset['alpha'], subset['avg_score'], marker='o', label=query_type)\n",
    "\n",
    "ax2.set_title('Effect of Alpha on Average Score', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Alpha Value')\n",
    "ax2.set_ylabel('Average Score')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Alpha Tuning Insights:\")\n",
    "print(\"- Î±=0.7 often provides good balance for most query types\")\n",
    "print(\"- Technical queries may benefit from lower Î± (more lexical weight)\")\n",
    "print(\"- Conceptual queries may benefit from higher Î± (more semantic weight)\")\n",
    "print(\"- Consider dynamic Î± adjustment based on query characteristics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Exercise 3: Advanced Techniques - HyDE and Contextual Retrieval\n",
    "\n",
    "Let's implement cutting-edge search techniques from 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedSearchTechniques:\n",
    "    \"\"\"Implement HyDE and Contextual Retrieval techniques\"\"\"\n",
    "    \n",
    "    def __init__(self, hybrid_engine: HybridSearchEngine):\n",
    "        self.hybrid_engine = hybrid_engine\n",
    "        self.search_lab = hybrid_engine.search_lab\n",
    "        self.embedding_model = hybrid_engine.embedding_model\n",
    "        \n",
    "    def generate_hypothetical_document(self, query: str) -> str:\n",
    "        \"\"\"Generate a hypothetical document for HyDE (simplified version)\"\"\"\n",
    "        # In production, this would use an LLM like GPT-4 or Claude\n",
    "        # For this demo, we'll create rule-based hypothetical documents\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Template-based hypothetical document generation\n",
    "        if 'machine learning' in query_lower or 'ai' in query_lower:\n",
    "            hypothetical = f\"\"\"Machine learning is a subset of artificial intelligence that enables systems to learn and improve from experience. \n",
    "            Key algorithms include supervised learning methods like decision trees and neural networks, unsupervised learning techniques such as clustering, \n",
    "            and reinforcement learning for decision-making tasks. These approaches help solve problems in {query} by finding patterns in data and making predictions.\"\"\"\n",
    "        \n",
    "        elif 'tensorflow' in query_lower or 'pytorch' in query_lower:\n",
    "            hypothetical = f\"\"\"TensorFlow and PyTorch are leading deep learning frameworks that provide tools for building and training neural networks. \n",
    "            They offer optimized computation for GPUs, automatic differentiation, and high-level APIs for common machine learning tasks. \n",
    "            Version updates typically include performance improvements, new layer types, and better optimization algorithms relevant to {query}.\"\"\"\n",
    "        \n",
    "        elif 'business' in query_lower or 'revenue' in query_lower:\n",
    "            hypothetical = f\"\"\"Business performance metrics and revenue analysis are crucial for understanding company growth and market position. \n",
    "            Key indicators include quarterly revenue growth, customer acquisition costs, lifetime value, and market share. \n",
    "            Analytics and data science help optimize these metrics through {query} and strategic decision-making.\"\"\"\n",
    "        \n",
    "        else:\n",
    "            # Generic hypothetical document\n",
    "            hypothetical = f\"\"\"This document discusses {query} and covers the main concepts, applications, and implications. \n",
    "            It provides comprehensive information about the topic including technical details, practical examples, and real-world use cases. \n",
    "            The content is designed to answer questions and provide insights related to {query}.\"\"\"\n",
    "        \n",
    "        return hypothetical.strip()\n",
    "    \n",
    "    def hyde_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Perform HyDE (Hypothetical Document Embeddings) search\"\"\"\n",
    "        # Generate hypothetical document\n",
    "        hypothetical_doc = self.generate_hypothetical_document(query)\n",
    "        \n",
    "        # Use hypothetical document for semantic search instead of original query\n",
    "        hyp_embedding = self.embedding_model.encode([hypothetical_doc])\n",
    "        hyp_embedding = hyp_embedding / np.linalg.norm(hyp_embedding)\n",
    "        \n",
    "        # Calculate similarities with document embeddings\n",
    "        similarities = np.dot(self.search_lab.embeddings, hyp_embedding.T).flatten()\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append((idx, similarities[idx], self.search_lab.documents[idx]))\n",
    "        \n",
    "        return results, hypothetical_doc\n",
    "    \n",
    "    def add_contextual_information(self, documents: List[str], context_window: int = 1) -> List[str]:\n",
    "        \"\"\"Add contextual information to document chunks (simplified contextual retrieval)\"\"\"\n",
    "        # In production, this would use LLMs to generate context for each chunk\n",
    "        # For demo, we'll add simple positional and categorical context\n",
    "        \n",
    "        contextual_documents = []\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            # Determine document category\n",
    "            doc_lower = doc.lower()\n",
    "            if any(term in doc_lower for term in ['tensorflow', 'pytorch', 'api', 'function', 'version']):\n",
    "                category = \"Technical Documentation\"\n",
    "            elif any(term in doc_lower for term in ['revenue', 'business', 'customer', 'market']):\n",
    "                category = \"Business Analysis\"\n",
    "            elif any(term in doc_lower for term in ['learning', 'algorithm', 'model', 'neural']):\n",
    "                category = \"Machine Learning Research\"\n",
    "            elif any(term in doc_lower for term in ['attention', 'transformer', 'language model']):\n",
    "                category = \"AI Research Papers\"\n",
    "            else:\n",
    "                category = \"General Content\"\n",
    "            \n",
    "            # Add context prefix\n",
    "            context_prefix = f\"[Document {i+1} of {len(documents)} - Category: {category}] \"\n",
    "            \n",
    "            # Add surrounding document context if available\n",
    "            if context_window > 0:\n",
    "                surrounding_context = []\n",
    "                for j in range(max(0, i-context_window), min(len(documents), i+context_window+1)):\n",
    "                    if j != i:\n",
    "                        # Add brief context from nearby documents\n",
    "                        nearby_words = documents[j].split()[:5]  # First 5 words\n",
    "                        surrounding_context.append(\" \".join(nearby_words))\n",
    "                \n",
    "                if surrounding_context:\n",
    "                    context_prefix += f\"Related content: {'; '.join(surrounding_context)}. \"\n",
    "            \n",
    "            contextual_doc = context_prefix + doc\n",
    "            contextual_documents.append(contextual_doc)\n",
    "        \n",
    "        return contextual_documents\n",
    "    \n",
    "    def contextual_retrieval_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Perform search with contextual retrieval\"\"\"\n",
    "        # Add contextual information to documents\n",
    "        contextual_docs = self.add_contextual_information(self.search_lab.documents)\n",
    "        \n",
    "        # Generate embeddings for contextual documents\n",
    "        contextual_embeddings = self.embedding_model.encode(contextual_docs)\n",
    "        contextual_embeddings = contextual_embeddings / np.linalg.norm(contextual_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Perform semantic search with contextual embeddings\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        similarities = np.dot(contextual_embeddings, query_embedding.T).flatten()\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            # Return original document (without context prefix) but with contextual score\n",
    "            results.append((idx, similarities[idx], self.search_lab.documents[idx]))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def multi_query_expansion(self, query: str, num_expansions: int = 3) -> List[str]:\n",
    "        \"\"\"Generate multiple query variations for better coverage\"\"\"\n",
    "        # In production, this would use LLMs to generate diverse query reformulations\n",
    "        # For demo, we'll use template-based expansion\n",
    "        \n",
    "        expansions = [query]  # Include original query\n",
    "        \n",
    "        # Synonym-based expansions\n",
    "        synonyms = {\n",
    "            'machine learning': ['artificial intelligence', 'ML algorithms', 'automated learning'],\n",
    "            'algorithms': ['methods', 'techniques', 'approaches'],\n",
    "            'business': ['enterprise', 'commercial', 'corporate'],\n",
    "            'performance': ['efficiency', 'effectiveness', 'results'],\n",
    "            'optimization': ['improvement', 'enhancement', 'tuning']\n",
    "        }\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        for term, syns in synonyms.items():\n",
    "            if term in query_lower and len(expansions) < num_expansions + 1:\n",
    "                for syn in syns[:num_expansions - len(expansions) + 1]:\n",
    "                    expanded = query_lower.replace(term, syn)\n",
    "                    expansions.append(expanded)\n",
    "        \n",
    "        # Ensure we have enough expansions\n",
    "        while len(expansions) < num_expansions + 1:\n",
    "            # Add more generic expansions\n",
    "            if 'how' not in query.lower():\n",
    "                expansions.append(f\"how to {query}\")\n",
    "            elif 'what' not in query.lower():\n",
    "                expansions.append(f\"what is {query}\")\n",
    "            else:\n",
    "                expansions.append(f\"{query} examples\")\n",
    "        \n",
    "        return expansions[:num_expansions + 1]\n",
    "    \n",
    "    def multi_query_search(self, query: str, top_k: int = 10) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Perform search with multiple query expansions and fusion\"\"\"\n",
    "        # Generate query expansions\n",
    "        expanded_queries = self.multi_query_expansion(query)\n",
    "        \n",
    "        # Collect results from all expanded queries\n",
    "        all_results = []\n",
    "        for exp_query in expanded_queries:\n",
    "            results = self.search_lab.semantic_search(exp_query, top_k=top_k)\n",
    "            all_results.extend(results)\n",
    "        \n",
    "        # Aggregate scores for documents that appear multiple times\n",
    "        doc_scores = defaultdict(list)\n",
    "        for doc_id, score, doc_text in all_results:\n",
    "            doc_scores[doc_id].append(score)\n",
    "        \n",
    "        # Use maximum score aggregation\n",
    "        final_results = []\n",
    "        for doc_id, scores in doc_scores.items():\n",
    "            max_score = max(scores)\n",
    "            doc_text = self.search_lab.documents[doc_id]\n",
    "            final_results.append((doc_id, max_score, doc_text))\n",
    "        \n",
    "        # Sort and return top-k\n",
    "        final_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return final_results[:top_k], expanded_queries\n",
    "\n",
    "# Initialize advanced search techniques\n",
    "advanced_search = AdvancedSearchTechniques(hybrid_engine)\n",
    "print(\"ðŸš€ Advanced Search Techniques initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced search techniques\n",
    "test_query = \"improving business performance with AI\"\n",
    "\n",
    "print(f\"ðŸ§ª ADVANCED SEARCH TECHNIQUES COMPARISON\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Standard semantic search\n",
    "print(\"\\n1ï¸âƒ£ Standard Semantic Search:\")\n",
    "standard_results = search_lab.semantic_search(test_query, top_k=5)\n",
    "for i, (doc_id, score, doc_text) in enumerate(standard_results):\n",
    "    print(f\"   {i+1}. [{score:.4f}] {doc_text[:70]}...\")\n",
    "\n",
    "# 2. HyDE search\n",
    "print(\"\\n2ï¸âƒ£ HyDE (Hypothetical Document Embeddings):\")\n",
    "hyde_results, hypothetical_doc = advanced_search.hyde_search(test_query, top_k=5)\n",
    "print(f\"   Generated hypothetical document: {hypothetical_doc[:100]}...\")\n",
    "print(\"   Results:\")\n",
    "for i, (doc_id, score, doc_text) in enumerate(hyde_results):\n",
    "    print(f\"   {i+1}. [{score:.4f}] {doc_text[:70]}...\")\n",
    "\n",
    "# 3. Contextual retrieval\n",
    "print(\"\\n3ï¸âƒ£ Contextual Retrieval:\")\n",
    "contextual_results = advanced_search.contextual_retrieval_search(test_query, top_k=5)\n",
    "for i, (doc_id, score, doc_text) in enumerate(contextual_results):\n",
    "    print(f\"   {i+1}. [{score:.4f}] {doc_text[:70]}...\")\n",
    "\n",
    "# 4. Multi-query expansion\n",
    "print(\"\\n4ï¸âƒ£ Multi-Query Expansion:\")\n",
    "multi_query_results, expanded_queries = advanced_search.multi_query_search(test_query, top_k=5)\n",
    "print(f\"   Expanded queries: {expanded_queries}\")\n",
    "print(\"   Results:\")\n",
    "for i, (doc_id, score, doc_text) in enumerate(multi_query_results):\n",
    "    print(f\"   {i+1}. [{score:.4f}] {doc_text[:70]}...\")\n",
    "\n",
    "# 5. Hybrid with RRF\n",
    "print(\"\\n5ï¸âƒ£ Hybrid Search (RRF):\")\n",
    "hybrid_results = hybrid_engine.hybrid_search(test_query, top_k=5, fusion_method=\"rrf\", alpha=0.7)\n",
    "for i, (doc_id, score, doc_text) in enumerate(hybrid_results):\n",
    "    print(f\"   {i+1}. [{score:.4f}] {doc_text[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison across different query types\n",
    "diverse_queries = [\n",
    "    \"TensorFlow optimization techniques\",  # Technical/specific\n",
    "    \"AI methods for pattern recognition\",  # Conceptual\n",
    "    \"Q4 business revenue analysis\",       # Business/factual\n",
    "    \"neural network training methods\"      # Research/academic\n",
    "]\n",
    "\n",
    "print(\"\\nðŸ“Š COMPREHENSIVE SEARCH METHOD EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collect results for analysis\n",
    "evaluation_results = []\n",
    "\n",
    "for query in diverse_queries:\n",
    "    print(f\"\\nðŸ” Evaluating: '{query}'\")\n",
    "    \n",
    "    # Get results from all methods\n",
    "    methods = {\n",
    "        'Lexical (BM25)': search_lab.lexical_search(query, top_k=5),\n",
    "        'Semantic': search_lab.semantic_search(query, top_k=5),\n",
    "        'Hybrid (RRF)': hybrid_engine.hybrid_search(query, top_k=5, fusion_method=\"rrf\", alpha=0.7),\n",
    "        'HyDE': advanced_search.hyde_search(query, top_k=5)[0],\n",
    "        'Contextual': advanced_search.contextual_retrieval_search(query, top_k=5),\n",
    "        'Multi-Query': advanced_search.multi_query_search(query, top_k=5)[0]\n",
    "    }\n",
    "    \n",
    "    # Calculate diversity metrics\n",
    "    for method_name, results in methods.items():\n",
    "        doc_ids = set(r[0] for r in results)\n",
    "        avg_score = np.mean([r[1] for r in results]) if results else 0\n",
    "        max_score = max([r[1] for r in results]) if results else 0\n",
    "        \n",
    "        evaluation_results.append({\n",
    "            'query': query,\n",
    "            'method': method_name,\n",
    "            'unique_docs': len(doc_ids),\n",
    "            'avg_score': avg_score,\n",
    "            'max_score': max_score,\n",
    "            'top_doc_id': results[0][0] if results else None\n",
    "        })\n",
    "        \n",
    "        print(f\"   {method_name}: {len(doc_ids)} docs, avg_score={avg_score:.4f}\")\n",
    "\n",
    "# Create evaluation DataFrame\n",
    "eval_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Visualize comprehensive comparison\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Average scores by method\n",
    "method_scores = eval_df.groupby('method')['avg_score'].mean().sort_values(ascending=False)\n",
    "bars1 = ax1.bar(range(len(method_scores)), method_scores.values)\n",
    "ax1.set_title('Average Performance by Search Method', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Average Score')\n",
    "ax1.set_xticks(range(len(method_scores)))\n",
    "ax1.set_xticklabels(method_scores.index, rotation=45, ha='right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "             f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. Score distribution heatmap\n",
    "score_matrix = eval_df.pivot(index='method', columns='query', values='avg_score')\n",
    "im2 = ax2.imshow(score_matrix.values, cmap='YlOrRd', aspect='auto')\n",
    "ax2.set_title('Performance Heatmap by Query Type', fontweight='bold', fontsize=14)\n",
    "ax2.set_xlabel('Query')\n",
    "ax2.set_ylabel('Search Method')\n",
    "ax2.set_xticks(range(len(score_matrix.columns)))\n",
    "ax2.set_xticklabels([q[:20] + '...' for q in score_matrix.columns], rotation=45, ha='right')\n",
    "ax2.set_yticks(range(len(score_matrix.index)))\n",
    "ax2.set_yticklabels(score_matrix.index)\n",
    "plt.colorbar(im2, ax=ax2, label='Average Score')\n",
    "\n",
    "# 3. Document diversity comparison\n",
    "diversity_data = eval_df.groupby('method')['unique_docs'].mean().sort_values(ascending=False)\n",
    "bars3 = ax3.bar(range(len(diversity_data)), diversity_data.values, color='lightgreen')\n",
    "ax3.set_title('Document Diversity by Method', fontweight='bold', fontsize=14)\n",
    "ax3.set_ylabel('Average Unique Documents')\n",
    "ax3.set_xticks(range(len(diversity_data)))\n",
    "ax3.set_xticklabels(diversity_data.index, rotation=45, ha='right')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars3):\n",
    "    height = bar.get_height()\n",
    "    ax3.text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "             f'{height:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Method correlation analysis\n",
    "# Calculate how often methods agree on top document\n",
    "agreement_matrix = np.zeros((len(method_scores), len(method_scores)))\n",
    "methods_list = list(method_scores.index)\n",
    "\n",
    "for query in diverse_queries:\n",
    "    query_results = eval_df[eval_df['query'] == query]\n",
    "    top_docs = {row['method']: row['top_doc_id'] for _, row in query_results.iterrows()}\n",
    "    \n",
    "    for i, method1 in enumerate(methods_list):\n",
    "        for j, method2 in enumerate(methods_list):\n",
    "            if top_docs.get(method1) == top_docs.get(method2):\n",
    "                agreement_matrix[i, j] += 1\n",
    "\n",
    "# Normalize by number of queries\n",
    "agreement_matrix = agreement_matrix / len(diverse_queries)\n",
    "\n",
    "im4 = ax4.imshow(agreement_matrix, cmap='Blues', aspect='auto')\n",
    "ax4.set_title('Method Agreement (Top Document)', fontweight='bold', fontsize=14)\n",
    "ax4.set_xlabel('Search Method')\n",
    "ax4.set_ylabel('Search Method')\n",
    "ax4.set_xticks(range(len(methods_list)))\n",
    "ax4.set_xticklabels([m[:10] for m in methods_list], rotation=45, ha='right')\n",
    "ax4.set_yticks(range(len(methods_list)))\n",
    "ax4.set_yticklabels([m[:10] for m in methods_list])\n",
    "plt.colorbar(im4, ax=ax4, label='Agreement Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ SEARCH METHOD RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Best Overall: Hybrid (RRF) - Balances precision and recall\")\n",
    "print(\"ðŸŽ¯ High Precision: HyDE - Good for zero-shot complex queries\")\n",
    "print(\"ðŸ“š Rich Context: Contextual Retrieval - Better document understanding\")\n",
    "print(\"ðŸ” High Recall: Multi-Query - Covers more query variations\")\n",
    "print(\"âš¡ Fast & Simple: BM25 - For exact term matching\")\n",
    "print(\"ðŸ§  Conceptual: Semantic - For meaning-based search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### ðŸ”¤ Lexical Search Characteristics:\n",
    "1. **Excellent for**: Exact terms, names, IDs, technical specifications\n",
    "2. **Algorithm**: BM25 remains the gold standard for keyword matching\n",
    "3. **Strengths**: Fast, explainable, no training required\n",
    "4. **Weaknesses**: Misses synonyms, paraphrases, and conceptual matches\n",
    "\n",
    "### ðŸ§  Semantic Search Characteristics:\n",
    "1. **Excellent for**: Conceptual queries, cross-lingual search, intent understanding\n",
    "2. **Algorithm**: Vector similarity (cosine, dot product) with transformer embeddings\n",
    "3. **Strengths**: Handles meaning, synonyms, context\n",
    "4. **Weaknesses**: May miss exact terms, computationally expensive\n",
    "\n",
    "### ðŸ”„ Hybrid Search (2025 Standard):\n",
    "1. **Best Practice**: Combine BM25 + semantic search with RRF fusion\n",
    "2. **Optimal Alpha**: ~0.7 (70% semantic, 30% lexical) for most use cases\n",
    "3. **Production Ready**: Balances precision and recall effectively\n",
    "4. **Flexibility**: Adjust alpha based on query characteristics\n",
    "\n",
    "### ðŸš€ Advanced Techniques (2025):\n",
    "\n",
    "#### HyDE (Hypothetical Document Embeddings):\n",
    "- **Concept**: Generate hypothetical answer, then search for similar documents\n",
    "- **Best for**: Zero-shot scenarios, complex queries\n",
    "- **Improvement**: 15-30% better performance on out-of-domain queries\n",
    "\n",
    "#### Contextual Retrieval:\n",
    "- **Concept**: Add context information to document chunks\n",
    "- **Best for**: Large document collections, ambiguous chunks\n",
    "- **Improvement**: 49% reduction in failed retrievals (Anthropic research)\n",
    "\n",
    "#### Multi-Query Expansion:\n",
    "- **Concept**: Generate multiple query variations, search all, fuse results\n",
    "- **Best for**: High recall requirements, diverse terminology\n",
    "- **Improvement**: Better coverage of relevant documents\n",
    "\n",
    "### ðŸ“Š Performance Comparison:\n",
    "| Method | Speed | Precision | Recall | Use Case |\n",
    "|--------|-------|-----------|--------|---------|\n",
    "| **BM25** | â­â­â­â­â­ | â­â­â­ | â­â­ | Exact matching |\n",
    "| **Semantic** | â­â­â­ | â­â­â­â­ | â­â­â­â­ | Conceptual queries |\n",
    "| **Hybrid** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | Production default |\n",
    "| **HyDE** | â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | Complex queries |\n",
    "| **Contextual** | â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | Ambiguous content |\n",
    "\n",
    "### ðŸ› ï¸ Implementation Guidelines:\n",
    "1. **Start with hybrid**: RRF fusion with Î±=0.7\n",
    "2. **Add HyDE**: For domains with complex queries\n",
    "3. **Use contextual retrieval**: When chunks lack sufficient context\n",
    "4. **Implement multi-query**: When recall is critical\n",
    "5. **Monitor and tune**: Alpha values based on query characteristics\n",
    "\n",
    "### ðŸ”„ Search Method Selection Workflow:\n",
    "1. **Analyze Query** â†’ 2. **Detect Intent** â†’ 3. **Choose Method** â†’ 4. **Execute Search** â†’ 5. **Fuse Results** â†’ 6. **Return Ranked List**\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "In the next modules, we'll explore:\n",
    "- **Module 9**: Advanced retrieval strategies and re-ranking techniques\n",
    "- **Module 10**: Prompt engineering for optimal RAG performance\n",
    "- **Module 11**: LLM integration and model selection strategies\n",
    "\n",
    "Understanding search methods is fundamental for building effective RAG systems that can handle diverse query types and user needs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” Discussion Questions\n",
    "\n",
    "1. In what scenarios would you prefer pure semantic search over hybrid search?\n",
    "2. How would you dynamically adjust the alpha parameter based on query characteristics?\n",
    "3. What are the computational trade-offs between different advanced search techniques?\n",
    "4. How would you evaluate search quality in a production system?\n",
    "5. What factors would influence your choice of fusion algorithm (RRF vs weighted sum)?\n",
    "\n",
    "## ðŸ“ Optional Exercises\n",
    "\n",
    "1. **Real LLM Integration**: Implement HyDE with actual LLM calls (GPT-4, Claude)\n",
    "2. **Production Elasticsearch**: Set up hybrid search with Elasticsearch\n",
    "3. **Custom Fusion**: Develop your own fusion algorithm based on query features\n",
    "4. **A/B Testing**: Design experiments to compare search methods in production\n",
    "5. **Domain-Specific**: Adapt techniques for your specific domain (medical, legal, technical)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}