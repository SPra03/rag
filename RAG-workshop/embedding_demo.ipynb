{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mini: Embedding Generation â€” 5â€‘minute demo\n",
        "\n",
        "Goal: Take chunked documents and convert them to vector embeddings for similarity search.\n",
        "\n",
        "What we'll do:\n",
        "- Load chunked documents from our PDF processing\n",
        "- Set up a lightweight embedding model (HuggingFace or OpenAI)\n",
        "- Generate embeddings for each chunk\n",
        "- Preview the vector representations\n",
        "- Save embeddings with metadata for indexing\n",
        "\n",
        "Quick and practicalâ€”ready for vector search!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install embedding dependencies\n",
        "import sys, subprocess\n",
        "\n",
        "def pip_install(package: str):\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except ImportError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# For embedding models\n",
        "pip_install(\"langchain\")\n",
        "pip_install(\"langchain-community\") \n",
        "pip_install(\"sentence-transformers\")  # HuggingFace embeddings\n",
        "pip_install(\"langchain-huggingface\")   # LangChain HF integration\n",
        "\n",
        "print(\"Embedding setup ready âœ”\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup embedding model - using lightweight HuggingFace model\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "# Using a small, fast model good for demos\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"all-MiniLM-L6-v2\",  # 384 dimensions, fast\n",
        "    model_kwargs={'device': 'cpu'},  # CPU for compatibility\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "# Test it works\n",
        "test_text = \"This is a test sentence for embedding.\"\n",
        "test_embedding = embedding_model.embed_query(test_text)\n",
        "print(f\"Model loaded âœ” Embedding dimension: {len(test_embedding)}\")\n",
        "print(f\"Sample embedding (first 5 values): {test_embedding[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load chunked documents from our doc_processing work\n",
        "# Option 1: Re-run the LangChain chunking (if you have it)\n",
        "# Option 2: Load from saved JSONL and convert to Documents\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "from langchain.schema import Document\n",
        "\n",
        "# Try to load from JSONL first (simpler)\n",
        "jsonl_path = Path(\"data/sample_chunks.jsonl\")\n",
        "\n",
        "if jsonl_path.exists():\n",
        "    print(\"Loading from JSONL...\")\n",
        "    docs = []\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line_num, line in enumerate(f):\n",
        "            data = json.loads(line.strip())\n",
        "            doc = Document(\n",
        "                page_content=data[\"text\"],\n",
        "                metadata={\n",
        "                    \"chunk_id\": data[\"id\"],\n",
        "                    \"source\": \"data/sample.pdf\",\n",
        "                    \"chunk_index\": line_num,\n",
        "                    \"type\": \"pdf_chunk\"\n",
        "                }\n",
        "            )\n",
        "            docs.append(doc)\n",
        "else:\n",
        "    print(\"JSONL not found. Run doc_processing.ipynb first or create sample docs...\")\n",
        "    # Fallback: create sample documents\n",
        "    docs = [\n",
        "        Document(page_content=\"Sample document chunk 1 with some content.\", metadata={\"chunk_id\": \"sample-0\"}),\n",
        "        Document(page_content=\"Sample document chunk 2 with different content.\", metadata={\"chunk_id\": \"sample-1\"})\n",
        "    ]\n",
        "\n",
        "print(f\"Loaded {len(docs)} document chunks\")\n",
        "print(f\"First chunk preview: {docs[0].page_content[:200]}...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all chunks\n",
        "import time\n",
        "\n",
        "print(\"Generating embeddings...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Extract text content from documents\n",
        "texts = [doc.page_content for doc in docs]\n",
        "\n",
        "# Generate embeddings in batch (more efficient)\n",
        "embeddings = embedding_model.embed_documents(texts)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"Generated {len(embeddings)} embeddings in {end_time - start_time:.2f} seconds\")\n",
        "print(f\"Each embedding has {len(embeddings[0])} dimensions\")\n",
        "\n",
        "# Preview first embedding\n",
        "print(f\"First embedding (first 10 values): {embeddings[0][:10]}\")\n",
        "print(f\"Embedding magnitude: {sum(x**2 for x in embeddings[0])**0.5:.3f}\")  # Should be ~1.0 if normalized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick similarity demo - compare chunks\n",
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
        "\n",
        "# Compare first few chunks\n",
        "print(\"Similarity between chunks:\")\n",
        "for i in range(min(3, len(embeddings))):\n",
        "    for j in range(i+1, min(3, len(embeddings))):\n",
        "        sim = cosine_similarity(embeddings[i], embeddings[j])\n",
        "        print(f\"Chunk {i} â†” Chunk {j}: {sim:.3f}\")\n",
        "        print(f\"  Text {i}: {texts[i][:100]}...\")\n",
        "        print(f\"  Text {j}: {texts[j][:100]}...\")\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save embeddings with metadata for vector database\n",
        "import json\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "# Prepare data for saving\n",
        "embedding_data = []\n",
        "for i, (doc, embedding) in enumerate(zip(docs, embeddings)):\n",
        "    record = {\n",
        "        \"id\": doc.metadata.get(\"chunk_id\", f\"chunk-{i}\"),\n",
        "        \"text\": doc.page_content,\n",
        "        \"metadata\": doc.metadata,\n",
        "        \"embedding\": embedding,  # List of floats\n",
        "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "        \"embedding_dim\": len(embedding)\n",
        "    }\n",
        "    embedding_data.append(record)\n",
        "\n",
        "# Save as NPZ for efficient loading (vectors) + JSON for metadata\n",
        "vectors_path = Path(\"data/embeddings.npz\")\n",
        "metadata_path = Path(\"data/embeddings_metadata.json\")\n",
        "\n",
        "# Save vectors efficiently\n",
        "np.savez_compressed(\n",
        "    vectors_path,\n",
        "    embeddings=np.array(embeddings),\n",
        "    chunk_ids=[record[\"id\"] for record in embedding_data]\n",
        ")\n",
        "\n",
        "# Save metadata separately\n",
        "metadata_only = [\n",
        "    {k: v for k, v in record.items() if k != \"embedding\"}\n",
        "    for record in embedding_data\n",
        "]\n",
        "\n",
        "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(metadata_only, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Saved {len(embeddings)} embeddings:\")\n",
        "print(f\"  Vectors: {vectors_path}\")\n",
        "print(f\"  Metadata: {metadata_path}\")\n",
        "print(f\"Ready for vector search! ðŸš€\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
