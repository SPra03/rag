{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Working with Embeddings\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Generate and manipulate text embeddings programmatically\n",
    "- Understand and implement different similarity metrics\n",
    "- Visualize high-dimensional embedding spaces in 2D/3D\n",
    "- Build efficient semantic search functionality\n",
    "- Explore embedding neighborhoods and relationships\n",
    "- Implement batch processing for large datasets\n",
    "- Use 2025's latest visualization tools\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### What Are Embeddings Really? üî¢\n",
    "\n",
    "**Think of embeddings as coordinates in meaning-space:**\n",
    "- Each dimension captures some aspect of meaning\n",
    "- Similar texts have similar coordinates\n",
    "- Distance between points = semantic similarity\n",
    "- We can navigate this space mathematically\n",
    "\n",
    "### Similarity Metrics Comparison üìè\n",
    "| Metric | Formula | When to Use | Range |\n",
    "|--------|---------|-------------|-------|\n",
    "| **Cosine** | cos(Œ∏) = A¬∑B / (\\|A\\|\\|B\\|) | Text, normalized vectors | [-1, 1] |\n",
    "| **Dot Product** | A¬∑B | Unit vectors, fast computation | [-‚àû, ‚àû] |\n",
    "| **Euclidean** | ‚àöŒ£(Ai-Bi)¬≤ | When magnitude matters | [0, ‚àû] |\n",
    "| **Manhattan** | Œ£\\|Ai-Bi\\| | Robust to outliers | [0, ‚àû] |\n",
    "\n",
    "### 2025 Visualization Advances üé®\n",
    "- **UMAP dominance**: Faster, preserves global + local structure\n",
    "- **Interactive exploration**: Nomic Atlas for massive datasets\n",
    "- **3D visualization**: Real-time navigation of embedding spaces\n",
    "- **Contextual embeddings**: Dynamic embeddings based on context\n",
    "\n",
    "### Performance Optimization üöÄ\n",
    "- **Batch processing**: Process multiple texts simultaneously\n",
    "- **Vector operations**: Use NumPy/PyTorch for speed\n",
    "- **Memory management**: Handle large embedding datasets efficiently\n",
    "- **Caching strategies**: Store and reuse embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "Let's install the required packages for hands-on embedding work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers numpy matplotlib plotly\n",
    "!pip install -q scikit-learn umap-learn seaborn\n",
    "!pip install -q torch torchvision  # For efficient operations\n",
    "!pip install -q pandas ipywidgets  # For interactive widgets\n",
    "# Optional: !pip install -q nomic  # For advanced visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Embedding and ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# Torch for efficient operations\n",
    "import torch\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üî¢ Ready to work with embeddings!\")\n",
    "print(f\"üöÄ Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¢ Exercise 1: Generating and Inspecting Embeddings\n",
    "\n",
    "Let's start by generating embeddings and understanding their structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a good general-purpose embedding model\n",
    "print(\"ü§ñ Loading embedding model...\")\n",
    "model = SentenceTransformer('all-mpnet-base-v2')  # 768 dimensions, good performance\n",
    "print(f\"   Model: {model._modules['0'].auto_model.name_or_path}\")\n",
    "print(f\"   Dimensions: {model.get_sentence_embedding_dimension()}\")\n",
    "print(f\"   Max sequence length: {model.max_seq_length}\")\n",
    "\n",
    "# Sample texts covering different topics and styles\n",
    "sample_texts = [\n",
    "    \"Machine learning algorithms require large datasets for training\",\n",
    "    \"Deep learning models need substantial data to learn effectively\",  # Similar to above\n",
    "    \"The cat sat peacefully on the warm windowsill\",\n",
    "    \"A feline rested comfortably near the sunny window\",  # Similar to above\n",
    "    \"Financial markets experienced significant volatility yesterday\",\n",
    "    \"Stock prices fluctuated wildly during the trading session\",  # Similar to above\n",
    "    \"Python is a versatile programming language\",\n",
    "    \"The snake slithered silently through the tall grass\",  # Same word, different meaning\n",
    "    \"Climate change affects global weather patterns\",\n",
    "    \"Artificial neural networks mimic biological brain functions\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüìù Sample texts ({len(sample_texts)} items):\")\n",
    "for i, text in enumerate(sample_texts, 1):\n",
    "    print(f\"   {i:2d}. {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "print(\"\\nüîÑ Generating embeddings...\")\n",
    "start_time = time.time()\n",
    "embeddings = model.encode(sample_texts, convert_to_numpy=True)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"   ‚úÖ Generated {embeddings.shape[0]} embeddings in {end_time - start_time:.3f} seconds\")\n",
    "print(f\"   üìê Shape: {embeddings.shape}\")\n",
    "print(f\"   üíæ Memory usage: {embeddings.nbytes / 1024:.1f} KB\")\n",
    "print(f\"   üìä Data type: {embeddings.dtype}\")\n",
    "\n",
    "# Inspect embedding properties\n",
    "print(f\"\\nüîç Embedding Analysis:\")\n",
    "print(f\"   Value range: [{embeddings.min():.3f}, {embeddings.max():.3f}]\")\n",
    "print(f\"   Mean: {embeddings.mean():.3f}\")\n",
    "print(f\"   Standard deviation: {embeddings.std():.3f}\")\n",
    "print(f\"   Sparsity: {(embeddings == 0).mean()*100:.1f}% zeros\")\n",
    "\n",
    "# Show first embedding sample\n",
    "print(f\"\\nüìã First embedding (first 10 dimensions):\")\n",
    "print(f\"   Text: '{sample_texts[0]}'\")\n",
    "print(f\"   Vector: {embeddings[0][:10]}\")\n",
    "print(f\"   Norm (magnitude): {np.linalg.norm(embeddings[0]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding distribution\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Distribution of values across all embeddings\n",
    "all_values = embeddings.flatten()\n",
    "ax1.hist(all_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Distribution of All Embedding Values')\n",
    "ax1.set_xlabel('Embedding Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.axvline(all_values.mean(), color='red', linestyle='--', label=f'Mean: {all_values.mean():.3f}')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Embedding norms (magnitudes)\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "ax2.bar(range(len(norms)), norms, color='lightgreen', alpha=0.7)\n",
    "ax2.set_title('Embedding Magnitudes')\n",
    "ax2.set_xlabel('Text Index')\n",
    "ax2.set_ylabel('L2 Norm')\n",
    "ax2.axhline(norms.mean(), color='red', linestyle='--', label=f'Mean: {norms.mean():.3f}')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Heatmap of first few embeddings\n",
    "sns.heatmap(embeddings[:5, :50], annot=False, cmap='RdBu_r', center=0, ax=ax3)\n",
    "ax3.set_title('First 5 Embeddings (First 50 Dimensions)')\n",
    "ax3.set_xlabel('Dimension')\n",
    "ax3.set_ylabel('Text Index')\n",
    "\n",
    "# 4. Dimension variance\n",
    "dim_variance = np.var(embeddings, axis=0)\n",
    "ax4.plot(dim_variance, alpha=0.7, color='purple')\n",
    "ax4.set_title('Variance per Dimension')\n",
    "ax4.set_xlabel('Dimension Index')\n",
    "ax4.set_ylabel('Variance')\n",
    "ax4.axhline(dim_variance.mean(), color='red', linestyle='--', \n",
    "           label=f'Mean Variance: {dim_variance.mean():.6f}')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Key Observations:\")\n",
    "print(f\"   ‚Ä¢ Values roughly follow normal distribution around 0\")\n",
    "print(f\"   ‚Ä¢ Similar magnitude across different texts ({norms.std():.3f} std dev)\")\n",
    "print(f\"   ‚Ä¢ All dimensions contribute (no completely zero dimensions)\")\n",
    "print(f\"   ‚Ä¢ Dense representation (very few zero values)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìè Exercise 2: Understanding Similarity Metrics\n",
    "\n",
    "Let's explore different ways to measure similarity between embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_similarities(embeddings):\n",
    "    \"\"\"\n",
    "    Calculate similarity using different metrics\n",
    "    \"\"\"\n",
    "    # Cosine similarity (most common for text)\n",
    "    cosine_sim = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Dot product (for normalized vectors, same as cosine)\n",
    "    dot_product_sim = np.dot(embeddings, embeddings.T)\n",
    "    \n",
    "    # Euclidean distance (convert to similarity)\n",
    "    euclidean_dist = euclidean_distances(embeddings)\n",
    "    euclidean_sim = 1 / (1 + euclidean_dist)  # Convert distance to similarity\n",
    "    \n",
    "    # Manhattan distance (L1 norm)\n",
    "    manhattan_dist = np.sum(np.abs(embeddings[:, np.newaxis] - embeddings), axis=2)\n",
    "    manhattan_sim = 1 / (1 + manhattan_dist)\n",
    "    \n",
    "    return {\n",
    "        'cosine': cosine_sim,\n",
    "        'dot_product': dot_product_sim,\n",
    "        'euclidean': euclidean_sim,\n",
    "        'manhattan': manhattan_sim\n",
    "    }\n",
    "\n",
    "# Calculate similarities\n",
    "print(\"üìè SIMILARITY METRICS COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "similarities = calculate_all_similarities(embeddings)\n",
    "\n",
    "# Compare specific text pairs\n",
    "interesting_pairs = [\n",
    "    (0, 1, \"ML algorithms vs Deep learning (similar meaning)\"),\n",
    "    (2, 3, \"Cat on windowsill vs Feline near window (similar meaning)\"),\n",
    "    (4, 5, \"Financial volatility vs Stock fluctuation (similar meaning)\"),\n",
    "    (6, 7, \"Python programming vs Snake in grass (different meaning)\"),\n",
    "    (0, 2, \"ML algorithms vs Cat on windowsill (unrelated)\"),\n",
    "    (8, 9, \"Climate change vs Neural networks (unrelated)\")\n",
    "]\n",
    "\n",
    "print(\"\\nüîç Similarity Comparison for Key Pairs:\")\n",
    "print(f\"{'Pair':<50} {'Cosine':<8} {'Dot Prod':<9} {'Euclidean':<10} {'Manhattan':<10}\")\n",
    "print(\"-\" * 87)\n",
    "\n",
    "for i, j, description in interesting_pairs:\n",
    "    cosine = similarities['cosine'][i][j]\n",
    "    dot_prod = similarities['dot_product'][i][j]\n",
    "    euclidean = similarities['euclidean'][i][j]\n",
    "    manhattan = similarities['manhattan'][i][j]\n",
    "    \n",
    "    print(f\"{description:<50} {cosine:<8.3f} {dot_prod:<9.3f} {euclidean:<10.3f} {manhattan:<10.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrices\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metrics = ['cosine', 'dot_product', 'euclidean', 'manhattan']\n",
    "titles = ['Cosine Similarity', 'Dot Product', 'Euclidean Similarity', 'Manhattan Similarity']\n",
    "colormaps = ['Blues', 'Reds', 'Greens', 'Purples']\n",
    "\n",
    "for idx, (metric, title, cmap) in enumerate(zip(metrics, titles, colormaps)):\n",
    "    sim_matrix = similarities[metric]\n",
    "    \n",
    "    # Create heatmap\n",
    "    sns.heatmap(sim_matrix, \n",
    "                annot=True, \n",
    "                fmt='.2f', \n",
    "                cmap=cmap, \n",
    "                ax=axes[idx],\n",
    "                square=True,\n",
    "                cbar_kws={'label': 'Similarity'})\n",
    "    \n",
    "    axes[idx].set_title(f'{title}\\n(Range: {sim_matrix.min():.2f} to {sim_matrix.max():.2f})')\n",
    "    axes[idx].set_xlabel('Text Index')\n",
    "    axes[idx].set_ylabel('Text Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze correlations between metrics\n",
    "print(\"\\nüìä Correlation Between Similarity Metrics:\")\n",
    "correlation_data = {}\n",
    "for metric_name, sim_matrix in similarities.items():\n",
    "    # Get upper triangle (excluding diagonal)\n",
    "    mask = np.triu(np.ones_like(sim_matrix, dtype=bool), k=1)\n",
    "    correlation_data[metric_name] = sim_matrix[mask]\n",
    "\n",
    "corr_df = pd.DataFrame(correlation_data)\n",
    "correlation_matrix = corr_df.corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   ‚Ä¢ Cosine similarity focuses on direction, not magnitude\")\n",
    "print(\"   ‚Ä¢ Dot product includes magnitude information\")\n",
    "print(\"   ‚Ä¢ Euclidean considers absolute distance in space\")\n",
    "print(\"   ‚Ä¢ For normalized embeddings, cosine ‚âà dot product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé® Exercise 3: Visualizing Embedding Spaces\n",
    "\n",
    "Let's make the high-dimensional embeddings visible using dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for visualization\n",
    "print(\"üé® EMBEDDING SPACE VISUALIZATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create categories for coloring\n",
    "categories = [\n",
    "    'ML/AI', 'ML/AI',           # 0, 1: Machine learning texts\n",
    "    'Animals', 'Animals',       # 2, 3: Cat texts  \n",
    "    'Finance', 'Finance',       # 4, 5: Financial texts\n",
    "    'Programming', 'Animals',   # 6, 7: Python programming vs snake\n",
    "    'Environment', 'ML/AI'      # 8, 9: Climate change vs neural networks\n",
    "]\n",
    "\n",
    "colors = ['red', 'red', 'blue', 'blue', 'green', 'green', 'orange', 'blue', 'purple', 'red']\n",
    "\n",
    "# Method 1: PCA (Principal Component Analysis)\n",
    "print(\"\\nüîÑ Applying PCA...\")\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "pca_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "print(f\"   Explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "print(f\"   PC1: {pca.explained_variance_ratio_[0]:.3f}, PC2: {pca.explained_variance_ratio_[1]:.3f}\")\n",
    "\n",
    "# Method 2: UMAP (more advanced, preserves local and global structure)\n",
    "print(\"\\nüîÑ Applying UMAP...\")\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    metric='cosine',  # Use cosine distance for text embeddings\n",
    "    n_neighbors=5,    # Small number for small dataset\n",
    "    min_dist=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "umap_embeddings = umap_reducer.fit_transform(embeddings)\n",
    "\n",
    "print(\"   ‚úÖ UMAP transformation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualization\n",
    "def create_interactive_embedding_plot(embeddings_2d, method_name, texts, categories):\n",
    "    \"\"\"\n",
    "    Create an interactive plot of 2D embeddings\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Group by category for legend\n",
    "    unique_categories = list(set(categories))\n",
    "    category_colors = {\n",
    "        'ML/AI': 'red',\n",
    "        'Animals': 'blue', \n",
    "        'Finance': 'green',\n",
    "        'Programming': 'orange',\n",
    "        'Environment': 'purple'\n",
    "    }\n",
    "    \n",
    "    for category in unique_categories:\n",
    "        # Get indices for this category\n",
    "        indices = [i for i, cat in enumerate(categories) if cat == category]\n",
    "        \n",
    "        if indices:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=embeddings_2d[indices, 0],\n",
    "                y=embeddings_2d[indices, 1],\n",
    "                mode='markers+text',\n",
    "                name=category,\n",
    "                text=[str(i) for i in indices],\n",
    "                textposition='top center',\n",
    "                hovertext=[f\"{i}: {texts[i][:50]}...\" for i in indices],\n",
    "                hoverinfo='text',\n",
    "                marker=dict(\n",
    "                    size=12,\n",
    "                    color=category_colors.get(category, 'gray'),\n",
    "                    opacity=0.8,\n",
    "                    line=dict(color='black', width=1)\n",
    "                )\n",
    "            ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=f'{method_name} Embedding Visualization<br><sub>Hover for full text, numbers are text indices</sub>',\n",
    "        xaxis_title=f'{method_name} Dimension 1',\n",
    "        yaxis_title=f'{method_name} Dimension 2',\n",
    "        width=800,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create PCA visualization\n",
    "pca_fig = create_interactive_embedding_plot(pca_embeddings, 'PCA', sample_texts, categories)\n",
    "pca_fig.show()\n",
    "\n",
    "# Create UMAP visualization  \n",
    "umap_fig = create_interactive_embedding_plot(umap_embeddings, 'UMAP', sample_texts, categories)\n",
    "umap_fig.show()\n",
    "\n",
    "print(\"\\nüéØ Visualization Insights:\")\n",
    "print(\"   ‚Ä¢ Similar texts cluster together in 2D space\")\n",
    "print(\"   ‚Ä¢ UMAP often preserves local neighborhoods better than PCA\")\n",
    "print(\"   ‚Ä¢ Distance in 2D approximates semantic similarity\")\n",
    "print(\"   ‚Ä¢ Outliers may represent unique or ambiguous content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D visualization for even richer exploration\n",
    "print(\"\\nüåê Creating 3D UMAP Visualization...\")\n",
    "\n",
    "# Generate 3D UMAP\n",
    "umap_3d = umap.UMAP(\n",
    "    n_components=3,\n",
    "    metric='cosine',\n",
    "    n_neighbors=5,\n",
    "    min_dist=0.1,\n",
    "    random_state=42\n",
    ").fit_transform(embeddings)\n",
    "\n",
    "# Create 3D scatter plot\n",
    "fig_3d = go.Figure()\n",
    "\n",
    "unique_categories = list(set(categories))\n",
    "category_colors = {\n",
    "    'ML/AI': 'red',\n",
    "    'Animals': 'blue', \n",
    "    'Finance': 'green',\n",
    "    'Programming': 'orange',\n",
    "    'Environment': 'purple'\n",
    "}\n",
    "\n",
    "for category in unique_categories:\n",
    "    indices = [i for i, cat in enumerate(categories) if cat == category]\n",
    "    \n",
    "    if indices:\n",
    "        fig_3d.add_trace(go.Scatter3d(\n",
    "            x=umap_3d[indices, 0],\n",
    "            y=umap_3d[indices, 1], \n",
    "            z=umap_3d[indices, 2],\n",
    "            mode='markers+text',\n",
    "            name=category,\n",
    "            text=[str(i) for i in indices],\n",
    "            hovertext=[f\"{i}: {sample_texts[i][:60]}...\" for i in indices],\n",
    "            hoverinfo='text',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                color=category_colors.get(category, 'gray'),\n",
    "                opacity=0.8,\n",
    "                line=dict(color='black', width=1)\n",
    "            )\n",
    "        ))\n",
    "\n",
    "fig_3d.update_layout(\n",
    "    title='3D UMAP Embedding Visualization<br><sub>Rotate and zoom to explore the space</sub>',\n",
    "    scene=dict(\n",
    "        xaxis_title='UMAP Dimension 1',\n",
    "        yaxis_title='UMAP Dimension 2',\n",
    "        zaxis_title='UMAP Dimension 3'\n",
    "    ),\n",
    "    width=900,\n",
    "    height=700\n",
    ")\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "print(\"‚úÖ 3D visualization created! Rotate and zoom to explore the embedding space.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exercise 4: Building Semantic Search\n",
    "\n",
    "Now let's build a practical semantic search system using our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSearch:\n",
    "    def __init__(self, documents, model):\n",
    "        \"\"\"\n",
    "        Initialize semantic search with documents and embedding model\n",
    "        \"\"\"\n",
    "        self.documents = documents\n",
    "        self.model = model\n",
    "        self.embeddings = None\n",
    "        self.index_documents()\n",
    "    \n",
    "    def index_documents(self):\n",
    "        \"\"\"\n",
    "        Create embeddings for all documents\n",
    "        \"\"\"\n",
    "        print(f\"üîÑ Indexing {len(self.documents)} documents...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        self.embeddings = self.model.encode(self.documents, convert_to_tensor=True)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"   ‚úÖ Indexed in {end_time - start_time:.3f} seconds\")\n",
    "        print(f\"   üìê Embedding shape: {self.embeddings.shape}\")\n",
    "    \n",
    "    def search(self, query, top_k=5, return_scores=True):\n",
    "        \"\"\"\n",
    "        Search for most similar documents to query\n",
    "        \"\"\"\n",
    "        # Encode query\n",
    "        query_embedding = self.model.encode(query, convert_to_tensor=True)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cos_sim(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k results\n",
    "        top_indices = torch.topk(similarities, k=min(top_k, len(self.documents)))[1]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            doc_idx = idx.item()\n",
    "            score = similarities[doc_idx].item()\n",
    "            \n",
    "            result = {\n",
    "                'document': self.documents[doc_idx],\n",
    "                'index': doc_idx,\n",
    "                'score': score\n",
    "            }\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def explain_search(self, query, top_k=3):\n",
    "        \"\"\"\n",
    "        Search with detailed explanation\n",
    "        \"\"\"\n",
    "        print(f\"üîç Searching for: '{query}'\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        results = self.search(query, top_k)\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. Score: {result['score']:.4f}\")\n",
    "            print(f\"   Document: {result['document']}\")\n",
    "            print(f\"   Index: {result['index']}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Create semantic search system\n",
    "print(\"üîç BUILDING SEMANTIC SEARCH SYSTEM\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "search_engine = SemanticSearch(sample_texts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test semantic search with various queries\n",
    "test_queries = [\n",
    "    \"artificial intelligence and data\",\n",
    "    \"animal resting indoors\", \n",
    "    \"market volatility and trading\",\n",
    "    \"programming language for development\",\n",
    "    \"environmental issues and global warming\"\n",
    "]\n",
    "\n",
    "print(\"üß™ SEMANTIC SEARCH EXPERIMENTS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search_engine.explain_search(query, top_k=3)\n",
    "    print(\"\\n\" + \"=\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive search widget (if ipywidgets is available)\n",
    "try:\n",
    "    from ipywidgets import interact, widgets\n",
    "    from IPython.display import display, HTML\n",
    "    \n",
    "    def interactive_search(query=\"machine learning\", top_k=3):\n",
    "        if query.strip():\n",
    "            results = search_engine.search(query, top_k=top_k)\n",
    "            \n",
    "            html_output = f\"<h3>üîç Results for: '{query}'</h3>\"\n",
    "            \n",
    "            for i, result in enumerate(results, 1):\n",
    "                score_color = \"green\" if result['score'] > 0.7 else \"orange\" if result['score'] > 0.5 else \"red\"\n",
    "                \n",
    "                html_output += f\"\"\"\n",
    "                <div style=\"border: 1px solid #ddd; margin: 10px 0; padding: 10px; border-radius: 5px;\">\n",
    "                    <h4>{i}. <span style=\"color: {score_color};\">Score: {result['score']:.4f}</span></h4>\n",
    "                    <p><strong>Document:</strong> {result['document']}</p>\n",
    "                    <p><small><strong>Index:</strong> {result['index']}</small></p>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            display(HTML(html_output))\n",
    "        else:\n",
    "            display(HTML(\"<p>Please enter a search query.</p>\"))\n",
    "    \n",
    "    # Create interactive widget\n",
    "    print(\"\\nüéÆ Interactive Semantic Search:\")\n",
    "    interact(\n",
    "        interactive_search,\n",
    "        query=widgets.Text(\n",
    "            value=\"machine learning\",\n",
    "            placeholder=\"Enter your search query...\",\n",
    "            description=\"Query:\",\n",
    "            style={'description_width': 'initial'}\n",
    "        ),\n",
    "        top_k=widgets.IntSlider(\n",
    "            value=3,\n",
    "            min=1,\n",
    "            max=len(sample_texts),\n",
    "            description=\"Top K results:\",\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "    )\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n‚ö†Ô∏è  Interactive widgets not available. You can still use search_engine.explain_search(query) directly.\")\n",
    "    \n",
    "    # Alternative: simple function-based search\n",
    "    def try_search(query):\n",
    "        return search_engine.explain_search(query, top_k=3)\n",
    "    \n",
    "    print(\"\\nüí° Try: try_search('your query here')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Exercise 5: Efficient Batch Processing\n",
    "\n",
    "Let's explore how to handle larger datasets efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a larger dataset for performance testing\n",
    "import random\n",
    "\n",
    "# Base templates for generating varied content\n",
    "templates = {\n",
    "    'tech': [\n",
    "        \"The {technology} framework provides {feature} for developers\",\n",
    "        \"{language} programming language supports {capability} development\",\n",
    "        \"Implementing {concept} requires understanding of {skill}\",\n",
    "        \"Modern {tool} enables efficient {process} management\"\n",
    "    ],\n",
    "    'business': [\n",
    "        \"The company's {metric} increased by {percent}% this quarter\",\n",
    "        \"{department} team achieved {goal} through {strategy}\",\n",
    "        \"Market {condition} led to {outcome} in sales performance\",\n",
    "        \"Strategic {initiative} resulted in improved {result}\"\n",
    "    ],\n",
    "    'science': [\n",
    "        \"Research shows that {phenomenon} affects {system} significantly\",\n",
    "        \"The {study} demonstrates {finding} in {field} research\",\n",
    "        \"Scientists discovered {discovery} using {method} analysis\",\n",
    "        \"New {technology} enables better {measurement} of {variable}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Vocabulary for template filling\n",
    "vocabulary = {\n",
    "    'technology': ['React', 'Django', 'TensorFlow', 'Kubernetes', 'Docker'],\n",
    "    'feature': ['scalability', 'security', 'performance', 'flexibility', 'reliability'],\n",
    "    'language': ['Python', 'JavaScript', 'Go', 'Rust', 'TypeScript'],\n",
    "    'capability': ['web', 'mobile', 'data science', 'machine learning', 'backend'],\n",
    "    'concept': ['microservices', 'APIs', 'databases', 'caching', 'authentication'],\n",
    "    'skill': ['algorithms', 'design patterns', 'system architecture', 'testing', 'debugging'],\n",
    "    'tool': ['containers', 'orchestrators', 'CI/CD pipelines', 'monitoring systems', 'load balancers'],\n",
    "    'process': ['deployment', 'scaling', 'monitoring', 'backup', 'recovery'],\n",
    "    'metric': ['revenue', 'profit', 'efficiency', 'productivity', 'satisfaction'],\n",
    "    'percent': ['15', '23', '8', '31', '12'],\n",
    "    'department': ['Sales', 'Marketing', 'Engineering', 'Support', 'Operations'],\n",
    "    'goal': ['targets', 'objectives', 'milestones', 'KPIs', 'deliverables'],\n",
    "    'strategy': ['optimization', 'automation', 'collaboration', 'innovation', 'training'],\n",
    "    'condition': ['growth', 'volatility', 'stability', 'expansion', 'consolidation'],\n",
    "    'outcome': ['improvements', 'increases', 'gains', 'successes', 'achievements'],\n",
    "    'initiative': ['partnerships', 'investments', 'transformations', 'reorganizations', 'expansions'],\n",
    "    'result': ['efficiency', 'quality', 'speed', 'accuracy', 'customer satisfaction'],\n",
    "    'phenomenon': ['climate change', 'genetic variation', 'neural activity', 'chemical reactions', 'electromagnetic fields'],\n",
    "    'system': ['ecosystems', 'organisms', 'networks', 'processes', 'structures'],\n",
    "    'study': ['experiment', 'analysis', 'investigation', 'survey', 'trial'],\n",
    "    'finding': ['correlations', 'patterns', 'relationships', 'mechanisms', 'effects'],\n",
    "    'field': ['biology', 'chemistry', 'physics', 'psychology', 'neuroscience'],\n",
    "    'discovery': ['compounds', 'proteins', 'mechanisms', 'pathways', 'interactions'],\n",
    "    'method': ['statistical', 'computational', 'experimental', 'observational', 'theoretical'],\n",
    "    'measurement': ['monitoring', 'tracking', 'assessment', 'evaluation', 'quantification'],\n",
    "    'variable': ['temperature', 'pressure', 'concentration', 'activity', 'response']\n",
    "}\n",
    "\n",
    "def generate_text_dataset(num_texts=1000):\n",
    "    \"\"\"\n",
    "    Generate a synthetic dataset for performance testing\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    \n",
    "    for _ in range(num_texts):\n",
    "        # Choose random category and template\n",
    "        category = random.choice(list(templates.keys()))\n",
    "        template = random.choice(templates[category])\n",
    "        \n",
    "        # Fill template with random vocabulary\n",
    "        text = template\n",
    "        for placeholder in vocabulary:\n",
    "            if f'{{{placeholder}}}' in text:\n",
    "                replacement = random.choice(vocabulary[placeholder])\n",
    "                text = text.replace(f'{{{placeholder}}}', replacement)\n",
    "        \n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "# Generate test dataset\n",
    "print(\"üìä GENERATING LARGE DATASET FOR PERFORMANCE TESTING\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "large_dataset = generate_text_dataset(1000)\n",
    "print(f\"‚úÖ Generated {len(large_dataset)} synthetic texts\")\n",
    "print(f\"\\nüìù Sample texts:\")\n",
    "for i in range(5):\n",
    "    print(f\"   {i+1}. {large_dataset[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different batch processing approaches\n",
    "def benchmark_embedding_generation(texts, model, batch_sizes=[1, 16, 64, 128]):\n",
    "    \"\"\"\n",
    "    Benchmark embedding generation with different batch sizes\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Test different batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nüîÑ Testing batch size: {batch_size}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        if batch_size == 1:\n",
    "            # One at a time\n",
    "            embeddings = []\n",
    "            for text in texts[:100]:  # Limit for timing\n",
    "                emb = model.encode([text])\n",
    "                embeddings.append(emb[0])\n",
    "            embeddings = np.array(embeddings)\n",
    "        else:\n",
    "            # Batch processing\n",
    "            embeddings = model.encode(texts[:100], batch_size=batch_size)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        results[batch_size] = {\n",
    "            'time': processing_time,\n",
    "            'texts_per_second': 100 / processing_time,\n",
    "            'embedding_shape': embeddings.shape\n",
    "        }\n",
    "        \n",
    "        print(f\"   Time: {processing_time:.3f}s\")\n",
    "        print(f\"   Speed: {results[batch_size]['texts_per_second']:.1f} texts/second\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmark\n",
    "print(\"‚ö° BATCH PROCESSING PERFORMANCE BENCHMARK\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "batch_results = benchmark_embedding_generation(large_dataset, model)\n",
    "\n",
    "# Visualize results\n",
    "batch_sizes = list(batch_results.keys())\n",
    "processing_times = [batch_results[bs]['time'] for bs in batch_sizes]\n",
    "speeds = [batch_results[bs]['texts_per_second'] for bs in batch_sizes]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Processing time vs batch size\n",
    "ax1.plot(batch_sizes, processing_times, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Batch Size')\n",
    "ax1.set_ylabel('Processing Time (seconds)')\n",
    "ax1.set_title('Processing Time vs Batch Size\\n(100 texts)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# Throughput vs batch size\n",
    "ax2.plot(batch_sizes, speeds, 'ro-', linewidth=2, markersize=8)\n",
    "ax2.set_xlabel('Batch Size')\n",
    "ax2.set_ylabel('Texts per Second')\n",
    "ax2.set_title('Throughput vs Batch Size')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find optimal batch size\n",
    "optimal_batch_size = max(batch_results.keys(), key=lambda bs: batch_results[bs]['texts_per_second'])\n",
    "optimal_speed = batch_results[optimal_batch_size]['texts_per_second']\n",
    "\n",
    "print(f\"\\nüèÜ Optimal batch size: {optimal_batch_size}\")\n",
    "print(f\"   Speed: {optimal_speed:.1f} texts/second\")\n",
    "print(f\"   Speedup vs batch=1: {optimal_speed / batch_results[1]['texts_per_second']:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient processing for very large datasets\n",
    "def memory_efficient_embedding(texts, model, batch_size=64, max_memory_mb=500):\n",
    "    \"\"\"\n",
    "    Process embeddings in chunks to manage memory usage\n",
    "    \"\"\"\n",
    "    print(f\"üíæ Processing {len(texts)} texts with memory management...\")\n",
    "    \n",
    "    # Estimate memory per batch\n",
    "    sample_embedding = model.encode([texts[0]])\n",
    "    embedding_dim = sample_embedding.shape[1]\n",
    "    bytes_per_embedding = embedding_dim * 4  # float32\n",
    "    embeddings_per_mb = (1024 * 1024) / bytes_per_embedding\n",
    "    \n",
    "    max_embeddings = int(max_memory_mb * embeddings_per_mb)\n",
    "    effective_batch_size = min(batch_size, max_embeddings)\n",
    "    \n",
    "    print(f\"   Embedding dimensions: {embedding_dim}\")\n",
    "    print(f\"   Memory per embedding: {bytes_per_embedding} bytes\")\n",
    "    print(f\"   Effective batch size: {effective_batch_size}\")\n",
    "    print(f\"   Estimated memory usage: {effective_batch_size * bytes_per_embedding / 1024 / 1024:.1f} MB per batch\")\n",
    "    \n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in range(0, len(texts), effective_batch_size):\n",
    "        batch_texts = texts[i:i + effective_batch_size]\n",
    "        batch_embeddings = model.encode(batch_texts)\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "        \n",
    "        if (i // effective_batch_size + 1) % 10 == 0:\n",
    "            print(f\"   Processed {i + len(batch_texts)} / {len(texts)} texts...\")\n",
    "    \n",
    "    # Concatenate all embeddings\n",
    "    final_embeddings = np.vstack(all_embeddings)\n",
    "    \n",
    "    print(f\"   ‚úÖ Complete! Final shape: {final_embeddings.shape}\")\n",
    "    return final_embeddings\n",
    "\n",
    "# Test memory-efficient processing\n",
    "print(\"\\nüíæ MEMORY-EFFICIENT PROCESSING TEST\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Process first 500 texts efficiently\n",
    "efficient_embeddings = memory_efficient_embedding(\n",
    "    large_dataset[:500], \n",
    "    model, \n",
    "    batch_size=optimal_batch_size,\n",
    "    max_memory_mb=100  # Limit to 100MB\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"   Processed {efficient_embeddings.shape[0]} embeddings\")\n",
    "print(f\"   Dimensions: {efficient_embeddings.shape[1]}\")\n",
    "print(f\"   Memory usage: {efficient_embeddings.nbytes / 1024 / 1024:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### üî¢ Embedding Fundamentals:\n",
    "1. **Dense vectors**: Embeddings are dense, high-dimensional vectors (not sparse like TF-IDF)\n",
    "2. **Semantic coordinates**: Each dimension captures aspects of meaning\n",
    "3. **Normalized values**: Values typically range around [-1, 1] with normal distribution\n",
    "4. **Consistent magnitude**: Similar magnitude across different texts indicates good normalization\n",
    "\n",
    "### üìè Similarity Metrics:\n",
    "- **Cosine similarity**: Best for text, measures angle (direction) not magnitude\n",
    "- **Dot product**: Equivalent to cosine for normalized vectors, computationally faster\n",
    "- **Euclidean distance**: Considers absolute distance, sensitive to magnitude\n",
    "- **For RAG systems**: Cosine similarity is the standard choice\n",
    "\n",
    "### üé® Visualization Insights:\n",
    "1. **UMAP > t-SNE**: Better preserves both local and global structure\n",
    "2. **Semantic clustering**: Similar texts naturally cluster together\n",
    "3. **3D exploration**: Interactive 3D plots reveal more structure than 2D\n",
    "4. **Distance = similarity**: Closer points in visualization = higher semantic similarity\n",
    "\n",
    "### üîç Practical Search:\n",
    "- **Simple implementation**: Just compute cosine similarity between query and document embeddings\n",
    "- **Top-k retrieval**: Sort by similarity and return best matches\n",
    "- **Real-time capable**: Modern embeddings are fast enough for interactive search\n",
    "- **No keyword matching**: Semantic search finds conceptually similar content, not just word matches\n",
    "\n",
    "### ‚ö° Performance Optimization:\n",
    "1. **Batch processing**: 10-50x speedup over individual processing\n",
    "2. **Optimal batch size**: Usually 64-128 for good GPU utilization\n",
    "3. **Memory management**: Process in chunks for large datasets\n",
    "4. **PyTorch tensors**: Use GPU acceleration when available\n",
    "\n",
    "### üéØ Production Guidelines:\n",
    "- **Pre-compute embeddings**: Generate document embeddings offline\n",
    "- **Cache results**: Store embeddings to avoid recomputation\n",
    "- **Monitor memory**: Large embedding datasets can consume significant RAM\n",
    "- **Consider quantization**: Reduce precision for memory savings if needed\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In **Module 6**, we'll store these embeddings efficiently:\n",
    "- Vector database options (Chroma, Pinecone, Weaviate, Qdrant)\n",
    "- Indexing strategies for fast similarity search\n",
    "- CRUD operations with metadata\n",
    "- Performance optimization and scaling\n",
    "- Production deployment considerations\n",
    "\n",
    "You now have the foundational skills to work with embeddings in RAG systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. When might you choose euclidean distance over cosine similarity for your embeddings?\n",
    "2. How would you handle embedding drift when your document corpus changes over time?\n",
    "3. What strategies would you use to debug poor semantic search results?\n",
    "4. How would you implement incremental embedding updates for a production system?\n",
    "\n",
    "## üìù Optional Exercise\n",
    "\n",
    "**Advanced Challenge**: Build a \"semantic explorer\" that:\n",
    "1. Takes a large collection of your domain-specific texts\n",
    "2. Generates embeddings and creates an interactive 3D visualization\n",
    "3. Allows clicking on points to see the text and find similar documents\n",
    "4. Includes search functionality with result highlighting in the 3D space\n",
    "5. Shows clustering of different topics/themes in your domain\n",
    "\n",
    "This will give you hands-on experience with the practical aspects of working with embeddings at scale!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}