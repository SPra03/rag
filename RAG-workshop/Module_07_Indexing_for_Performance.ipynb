{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Indexing for Performance\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand different indexing algorithms and their trade-offs\n",
    "- Configure indexes for optimal performance in production scenarios\n",
    "- Balance speed, accuracy, and memory usage based on requirements\n",
    "- Know when and how to rebuild indexes for optimal performance\n",
    "- Implement and tune HNSW, IVF, and other indexing algorithms\n",
    "\n",
    "## 📚 Key Concepts\n",
    "\n",
    "### Why Indexing Matters\n",
    "\n",
    "Without proper indexing, vector similarity search becomes a **linear scan problem**:\n",
    "\n",
    "```python\n",
    "# Naive approach: O(n) complexity\n",
    "def linear_search(query_vector, all_vectors):\n",
    "    similarities = []\n",
    "    for vector in all_vectors:  # Check EVERY vector\n",
    "        similarity = cosine_similarity(query_vector, vector)\n",
    "        similarities.append(similarity)\n",
    "    return sorted(similarities, reverse=True)[:k]\n",
    "\n",
    "# With 1M vectors: ~1M similarity calculations per query!\n",
    "```\n",
    "\n",
    "**The Problem**: With millions of vectors, this becomes prohibitively slow\n",
    "**The Solution**: Indexing algorithms that provide sub-linear search complexity\n",
    "\n",
    "### 🏗️ Major Indexing Algorithms\n",
    "\n",
    "#### 1. HNSW (Hierarchical Navigable Small World) 🏆\n",
    "- **Best overall performance** for most use cases\n",
    "- **Graph-based approach**: Builds a multi-layer graph structure\n",
    "- **Complexity**: O(log n) expected search time\n",
    "- **Memory**: ~1.1x original embedding size\n",
    "\n",
    "#### 2. IVF (Inverted File Index)\n",
    "- **Clustering-based approach**: Groups similar vectors together\n",
    "- **Good for large datasets** with moderate accuracy requirements\n",
    "- **Complexity**: O(sqrt(n)) search time\n",
    "- **Memory**: More memory-efficient than HNSW\n",
    "\n",
    "#### 3. LSH (Locality Sensitive Hashing)\n",
    "- **Hash-based approach**: Maps similar vectors to same buckets\n",
    "- **Fast but approximate**: Trade accuracy for speed\n",
    "- **Best for**: Extremely large datasets where speed > accuracy\n",
    "\n",
    "#### 4. Product Quantization (PQ)\n",
    "- **Compression technique**: Reduces memory usage by 8-32x\n",
    "- **Often combined** with HNSW or IVF\n",
    "- **Trade-off**: Significant memory savings vs some accuracy loss\n",
    "\n",
    "### 2025 Performance Landscape 📊\n",
    "\n",
    "| Algorithm | Query Speed | Memory Usage | Accuracy | Build Time |\n",
    "|-----------|-------------|--------------|----------|------------|\n",
    "| **HNSW** | Excellent | Moderate | Excellent | Fast |\n",
    "| **IVF-HNSW** | Very Good | Low | Very Good | Moderate |\n",
    "| **PQ-HNSW** | Good | Very Low | Good | Fast |\n",
    "| **LSH** | Excellent | Low | Moderate | Very Fast |\n",
    "\n",
    "### Performance Trade-offs\n",
    "\n",
    "Every indexing decision involves **fundamental trade-offs**:\n",
    "\n",
    "1. **Speed vs Accuracy**: Faster algorithms often sacrifice some precision\n",
    "2. **Memory vs Performance**: More memory usually means faster queries\n",
    "3. **Build Time vs Query Time**: Complex indexes take longer to build but query faster\n",
    "4. **Update Frequency vs Optimization**: Frequent updates may require different strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠️ Setup\n",
    "Let's install the required packages and set up our indexing performance lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q faiss-cpu hnswlib qdrant-client sentence-transformers numpy pandas matplotlib seaborn\n",
    "!pip install -q scikit-learn psutil memory-profiler time-memory\n",
    "# Note: For GPU acceleration, replace faiss-cpu with faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import json\n",
    "import psutil\n",
    "import gc\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Indexing libraries\n",
    "import faiss\n",
    "import hnswlib\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct, HnswConfig\n",
    "\n",
    "# ML libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Setup complete!\")\n",
    "print(f\"📅 Today's date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "print(f\"💾 Available RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Exercise 1: Understanding Indexing Algorithms\n",
    "\n",
    "Let's implement and compare different indexing algorithms to understand their characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexingAlgorithmComparison:\n",
    "    \"\"\"Compare different indexing algorithms for vector similarity search\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.indexes = {}\n",
    "        self.build_times = {}\n",
    "        self.memory_usage = {}\n",
    "        \n",
    "    def generate_test_data(self, num_vectors: int = 10000, seed: int = 42) -> Tuple[np.ndarray, List[str]]:\n",
    "        \"\"\"Generate synthetic test data for indexing experiments\"\"\"\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # Create diverse text samples\n",
    "        topics = [\n",
    "            \"machine learning and artificial intelligence\",\n",
    "            \"data science and statistical analysis\", \n",
    "            \"computer vision and image processing\",\n",
    "            \"natural language processing and linguistics\",\n",
    "            \"web development and software engineering\",\n",
    "            \"database systems and data storage\",\n",
    "            \"cloud computing and distributed systems\",\n",
    "            \"cybersecurity and network protection\",\n",
    "            \"mobile app development and user interfaces\",\n",
    "            \"scientific computing and numerical methods\"\n",
    "        ]\n",
    "        \n",
    "        documents = []\n",
    "        for i in range(num_vectors):\n",
    "            topic = topics[i % len(topics)]\n",
    "            variation = np.random.randint(1, 100)\n",
    "            doc = f\"Document {i} about {topic}, covering advanced concepts and practical applications in the field. Variation {variation}.\"\n",
    "            documents.append(doc)\n",
    "        \n",
    "        print(f\"Generating embeddings for {num_vectors} documents...\")\n",
    "        embeddings = self.embedding_model.encode(documents, show_progress_bar=True)\n",
    "        \n",
    "        # Normalize embeddings for cosine similarity\n",
    "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        return embeddings.astype('float32'), documents\n",
    "    \n",
    "    def build_linear_index(self, embeddings: np.ndarray, name: str = \"linear\") -> Dict[str, Any]:\n",
    "        \"\"\"Build a linear (brute-force) search index for baseline comparison\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # For linear search, we just store the embeddings\n",
    "        index = {\n",
    "            'type': 'linear',\n",
    "            'embeddings': embeddings,\n",
    "            'size': len(embeddings)\n",
    "        }\n",
    "        \n",
    "        build_time = time.time() - start_time\n",
    "        memory_size = embeddings.nbytes\n",
    "        \n",
    "        self.indexes[name] = index\n",
    "        self.build_times[name] = build_time\n",
    "        self.memory_usage[name] = memory_size\n",
    "        \n",
    "        print(f\"✅ Built {name} index: {len(embeddings):,} vectors in {build_time:.2f}s\")\n",
    "        return index\n",
    "    \n",
    "    def build_hnsw_index(self, embeddings: np.ndarray, name: str = \"hnsw\", \n",
    "                        ef_construction: int = 200, M: int = 16) -> hnswlib.Index:\n",
    "        \"\"\"Build HNSW index using hnswlib\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize HNSW index\n",
    "        dim = embeddings.shape[1]\n",
    "        index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        \n",
    "        # Configure HNSW parameters\n",
    "        index.init_index(\n",
    "            max_elements=len(embeddings),\n",
    "            ef_construction=ef_construction,\n",
    "            M=M\n",
    "        )\n",
    "        \n",
    "        # Add vectors to index\n",
    "        labels = np.arange(len(embeddings))\n",
    "        index.add_items(embeddings, labels)\n",
    "        \n",
    "        build_time = time.time() - start_time\n",
    "        \n",
    "        # Estimate memory usage (approximate)\n",
    "        memory_size = embeddings.nbytes * 1.1  # HNSW adds ~10% overhead\n",
    "        \n",
    "        self.indexes[name] = index\n",
    "        self.build_times[name] = build_time\n",
    "        self.memory_usage[name] = memory_size\n",
    "        \n",
    "        print(f\"✅ Built {name} index: {len(embeddings):,} vectors in {build_time:.2f}s\")\n",
    "        print(f\"   Parameters: ef_construction={ef_construction}, M={M}\")\n",
    "        return index\n",
    "    \n",
    "    def build_ivf_index(self, embeddings: np.ndarray, name: str = \"ivf\", \n",
    "                       nlist: int = 100) -> faiss.IndexIVFFlat:\n",
    "        \"\"\"Build IVF (Inverted File) index using FAISS\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        dim = embeddings.shape[1]\n",
    "        \n",
    "        # Create quantizer (for clustering)\n",
    "        quantizer = faiss.IndexFlatIP(dim)  # Inner product for cosine similarity\n",
    "        \n",
    "        # Create IVF index\n",
    "        index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "        \n",
    "        # Train the index (clustering step)\n",
    "        index.train(embeddings)\n",
    "        \n",
    "        # Add vectors to index\n",
    "        index.add(embeddings)\n",
    "        \n",
    "        build_time = time.time() - start_time\n",
    "        \n",
    "        # Estimate memory usage\n",
    "        memory_size = embeddings.nbytes * 1.05  # IVF adds ~5% overhead\n",
    "        \n",
    "        self.indexes[name] = index\n",
    "        self.build_times[name] = build_time\n",
    "        self.memory_usage[name] = memory_size\n",
    "        \n",
    "        print(f\"✅ Built {name} index: {len(embeddings):,} vectors in {build_time:.2f}s\")\n",
    "        print(f\"   Parameters: nlist={nlist} clusters\")\n",
    "        return index\n",
    "    \n",
    "    def build_lsh_index(self, embeddings: np.ndarray, name: str = \"lsh\", \n",
    "                       n_components: int = 64) -> Dict[str, Any]:\n",
    "        \"\"\"Build LSH (Locality Sensitive Hashing) index using random projections\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create random projection transformer\n",
    "        lsh_transformer = SparseRandomProjection(\n",
    "            n_components=n_components,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # Transform embeddings to hash space\n",
    "        hash_embeddings = lsh_transformer.fit_transform(embeddings)\n",
    "        \n",
    "        # Binarize the hash values\n",
    "        binary_hashes = (hash_embeddings > 0).astype(np.int8)\n",
    "        \n",
    "        index = {\n",
    "            'type': 'lsh',\n",
    "            'transformer': lsh_transformer,\n",
    "            'binary_hashes': binary_hashes,\n",
    "            'original_embeddings': embeddings,\n",
    "            'n_components': n_components\n",
    "        }\n",
    "        \n",
    "        build_time = time.time() - start_time\n",
    "        \n",
    "        # Memory usage: binary hashes + transformer\n",
    "        memory_size = binary_hashes.nbytes + embeddings.nbytes * 0.1\n",
    "        \n",
    "        self.indexes[name] = index\n",
    "        self.build_times[name] = build_time\n",
    "        self.memory_usage[name] = memory_size\n",
    "        \n",
    "        print(f\"✅ Built {name} index: {len(embeddings):,} vectors in {build_time:.2f}s\")\n",
    "        print(f\"   Parameters: {n_components} hash components\")\n",
    "        return index\n",
    "    \n",
    "    def search_linear(self, query_embedding: np.ndarray, index: Dict[str, Any], k: int = 10) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search using linear scan\"\"\"\n",
    "        embeddings = index['embeddings']\n",
    "        \n",
    "        # Calculate cosine similarities\n",
    "        similarities = np.dot(embeddings, query_embedding.flatten())\n",
    "        \n",
    "        # Get top-k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_scores = similarities[top_indices]\n",
    "        \n",
    "        return top_indices.tolist(), top_scores.tolist()\n",
    "    \n",
    "    def search_hnsw(self, query_embedding: np.ndarray, index: hnswlib.Index, \n",
    "                   k: int = 10, ef: int = 50) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search using HNSW index\"\"\"\n",
    "        # Set search parameter\n",
    "        index.set_ef(ef)\n",
    "        \n",
    "        # Search\n",
    "        labels, distances = index.knn_query(query_embedding, k=k)\n",
    "        \n",
    "        # Convert distances to similarities (cosine distance -> cosine similarity)\n",
    "        similarities = 1 - distances[0]\n",
    "        \n",
    "        return labels[0].tolist(), similarities.tolist()\n",
    "    \n",
    "    def search_ivf(self, query_embedding: np.ndarray, index: faiss.IndexIVFFlat, \n",
    "                  k: int = 10, nprobe: int = 10) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search using IVF index\"\"\"\n",
    "        # Set search parameter\n",
    "        index.nprobe = nprobe\n",
    "        \n",
    "        # Search\n",
    "        similarities, indices = index.search(query_embedding.reshape(1, -1), k)\n",
    "        \n",
    "        return indices[0].tolist(), similarities[0].tolist()\n",
    "    \n",
    "    def search_lsh(self, query_embedding: np.ndarray, index: Dict[str, Any], \n",
    "                  k: int = 10) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search using LSH index\"\"\"\n",
    "        # Transform query to hash space\n",
    "        query_hash = index['transformer'].transform(query_embedding.reshape(1, -1))\n",
    "        query_binary = (query_hash > 0).astype(np.int8)\n",
    "        \n",
    "        # Calculate Hamming distances to all binary hashes\n",
    "        hamming_distances = np.sum(index['binary_hashes'] != query_binary, axis=1)\n",
    "        \n",
    "        # Get candidates with smallest Hamming distances\n",
    "        candidate_indices = np.argsort(hamming_distances)[:k*5]  # Get more candidates\n",
    "        \n",
    "        # Refine with actual cosine similarity\n",
    "        candidate_embeddings = index['original_embeddings'][candidate_indices]\n",
    "        similarities = np.dot(candidate_embeddings, query_embedding.flatten())\n",
    "        \n",
    "        # Get final top-k\n",
    "        top_k_among_candidates = np.argsort(similarities)[::-1][:k]\n",
    "        final_indices = candidate_indices[top_k_among_candidates]\n",
    "        final_scores = similarities[top_k_among_candidates]\n",
    "        \n",
    "        return final_indices.tolist(), final_scores.tolist()\n",
    "\n",
    "# Initialize the indexing comparison\n",
    "indexing_lab = IndexingAlgorithmComparison()\n",
    "print(\"🏗️ Indexing Algorithm Comparison Lab initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data\n",
    "print(\"📊 Generating test dataset...\")\n",
    "embeddings, documents = indexing_lab.generate_test_data(num_vectors=5000)\n",
    "\n",
    "print(f\"✅ Generated {len(embeddings):,} embeddings\")\n",
    "print(f\"📏 Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(f\"💾 Dataset size: {embeddings.nbytes / (1024**2):.1f} MB\")\n",
    "\n",
    "# Display sample documents\n",
    "print(\"\\n📄 Sample documents:\")\n",
    "for i in range(3):\n",
    "    print(f\"   {i+1}. {documents[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build different indexes\n",
    "print(\"🏗️ BUILDING INDEXES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Linear (baseline)\n",
    "print(\"\\n1. Building Linear Index (baseline):\")\n",
    "linear_index = indexing_lab.build_linear_index(embeddings)\n",
    "\n",
    "# 2. HNSW with default parameters\n",
    "print(\"\\n2. Building HNSW Index:\")\n",
    "hnsw_index = indexing_lab.build_hnsw_index(embeddings, \"hnsw_default\", ef_construction=200, M=16)\n",
    "\n",
    "# 3. HNSW with high-performance parameters\n",
    "print(\"\\n3. Building HNSW Index (High Performance):\")\n",
    "hnsw_fast_index = indexing_lab.build_hnsw_index(embeddings, \"hnsw_fast\", ef_construction=100, M=8)\n",
    "\n",
    "# 4. HNSW with high-accuracy parameters\n",
    "print(\"\\n4. Building HNSW Index (High Accuracy):\")\n",
    "hnsw_accurate_index = indexing_lab.build_hnsw_index(embeddings, \"hnsw_accurate\", ef_construction=400, M=32)\n",
    "\n",
    "# 5. IVF Index\n",
    "print(\"\\n5. Building IVF Index:\")\n",
    "ivf_index = indexing_lab.build_ivf_index(embeddings, \"ivf\", nlist=100)\n",
    "\n",
    "# 6. LSH Index\n",
    "print(\"\\n6. Building LSH Index:\")\n",
    "lsh_index = indexing_lab.build_lsh_index(embeddings, \"lsh\", n_components=64)\n",
    "\n",
    "print(\"\\n✅ All indexes built successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare build times and memory usage\n",
    "print(\"📊 INDEX BUILD COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "build_comparison = []\n",
    "for name in indexing_lab.build_times.keys():\n",
    "    build_comparison.append({\n",
    "        'Index': name.replace('_', ' ').title(),\n",
    "        'Build Time (s)': indexing_lab.build_times[name],\n",
    "        'Memory Usage (MB)': indexing_lab.memory_usage[name] / (1024**2),\n",
    "        'Memory Ratio': indexing_lab.memory_usage[name] / embeddings.nbytes\n",
    "    })\n",
    "\n",
    "build_df = pd.DataFrame(build_comparison)\n",
    "print(build_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualize build comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Build time comparison\n",
    "bars1 = ax1.bar(range(len(build_df)), build_df['Build Time (s)'])\n",
    "ax1.set_title('Index Build Time Comparison', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Build Time (seconds)')\n",
    "ax1.set_xlabel('Index Type')\n",
    "ax1.set_xticks(range(len(build_df)))\n",
    "ax1.set_xticklabels(build_df['Index'], rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.3f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Memory usage comparison\n",
    "bars2 = ax2.bar(range(len(build_df)), build_df['Memory Ratio'])\n",
    "ax2.set_title('Memory Usage Ratio', fontweight='bold', fontsize=14)\n",
    "ax2.set_ylabel('Memory Ratio (vs Original)')\n",
    "ax2.set_xlabel('Index Type')\n",
    "ax2.set_xticks(range(len(build_df)))\n",
    "ax2.set_xticklabels(build_df['Index'], rotation=45, ha='right')\n",
    "ax2.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Original Size')\n",
    "ax2.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "             f'{height:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Build Time Insights:\")\n",
    "print(\"- Linear 'index' is just storing embeddings (no actual indexing)\")\n",
    "print(\"- HNSW build time increases with ef_construction and M parameters\")\n",
    "print(\"- IVF requires clustering step, making it slower to build\")\n",
    "print(\"- LSH is fastest to build but uses additional hash storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚡ Exercise 2: Query Performance Benchmarking\n",
    "\n",
    "Let's benchmark query performance across different indexes and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPerformanceBenchmark:\n",
    "    \"\"\"Benchmark query performance across different indexing algorithms\"\"\"\n",
    "    \n",
    "    def __init__(self, indexing_lab: IndexingAlgorithmComparison):\n",
    "        self.indexing_lab = indexing_lab\n",
    "        self.query_results = []\n",
    "    \n",
    "    def generate_test_queries(self, num_queries: int = 100) -> np.ndarray:\n",
    "        \"\"\"Generate test queries for benchmarking\"\"\"\n",
    "        query_texts = [\n",
    "            \"machine learning algorithms and neural networks\",\n",
    "            \"data science statistical analysis and visualization\",\n",
    "            \"computer vision image processing techniques\",\n",
    "            \"natural language processing and text analysis\",\n",
    "            \"web development and software architecture\",\n",
    "            \"database optimization and data storage\",\n",
    "            \"cloud computing distributed systems\",\n",
    "            \"cybersecurity and network protection\",\n",
    "            \"mobile application development frameworks\",\n",
    "            \"scientific computing numerical methods\"\n",
    "        ]\n",
    "        \n",
    "        # Create test queries by cycling through topics\n",
    "        test_queries = [query_texts[i % len(query_texts)] for i in range(num_queries)]\n",
    "        \n",
    "        # Generate embeddings for queries\n",
    "        query_embeddings = self.indexing_lab.embedding_model.encode(test_queries)\n",
    "        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        return query_embeddings.astype('float32')\n",
    "    \n",
    "    def benchmark_index(self, index_name: str, query_embeddings: np.ndarray, \n",
    "                       k: int = 10, num_warmup: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Benchmark a specific index\"\"\"\n",
    "        index = self.indexing_lab.indexes[index_name]\n",
    "        query_times = []\n",
    "        all_results = []\n",
    "        \n",
    "        # Warmup queries\n",
    "        for i in range(min(num_warmup, len(query_embeddings))):\n",
    "            self._search_index(index_name, query_embeddings[i], k)\n",
    "        \n",
    "        # Actual benchmark\n",
    "        for query_emb in query_embeddings:\n",
    "            start_time = time.time()\n",
    "            indices, scores = self._search_index(index_name, query_emb, k)\n",
    "            query_time = time.time() - start_time\n",
    "            \n",
    "            query_times.append(query_time * 1000)  # Convert to milliseconds\n",
    "            all_results.append((indices, scores))\n",
    "        \n",
    "        return {\n",
    "            'index_name': index_name,\n",
    "            'query_times': query_times,\n",
    "            'avg_time': np.mean(query_times),\n",
    "            'p50_time': np.percentile(query_times, 50),\n",
    "            'p95_time': np.percentile(query_times, 95),\n",
    "            'p99_time': np.percentile(query_times, 99),\n",
    "            'queries_per_second': 1000 / np.mean(query_times),\n",
    "            'results': all_results\n",
    "        }\n",
    "    \n",
    "    def _search_index(self, index_name: str, query_embedding: np.ndarray, k: int) -> Tuple[List[int], List[float]]:\n",
    "        \"\"\"Search a specific index based on its type\"\"\"\n",
    "        index = self.indexing_lab.indexes[index_name]\n",
    "        \n",
    "        if index_name == \"linear\":\n",
    "            return self.indexing_lab.search_linear(query_embedding, index, k)\n",
    "        elif \"hnsw\" in index_name:\n",
    "            # Different ef values for different HNSW variants\n",
    "            ef = 200 if \"accurate\" in index_name else (50 if \"fast\" in index_name else 100)\n",
    "            return self.indexing_lab.search_hnsw(query_embedding, index, k, ef)\n",
    "        elif index_name == \"ivf\":\n",
    "            return self.indexing_lab.search_ivf(query_embedding, index, k, nprobe=10)\n",
    "        elif index_name == \"lsh\":\n",
    "            return self.indexing_lab.search_lsh(query_embedding, index, k)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown index type: {index_name}\")\n",
    "    \n",
    "    def calculate_recall(self, ground_truth_results: List[Tuple], test_results: List[Tuple], k: int = 10) -> float:\n",
    "        \"\"\"Calculate recall@k between two result sets\"\"\"\n",
    "        if len(ground_truth_results) != len(test_results):\n",
    "            raise ValueError(\"Result sets must have same length\")\n",
    "        \n",
    "        total_recall = 0.0\n",
    "        \n",
    "        for gt_result, test_result in zip(ground_truth_results, test_results):\n",
    "            gt_indices = set(gt_result[0][:k])  # Ground truth top-k indices\n",
    "            test_indices = set(test_result[0][:k])  # Test result top-k indices\n",
    "            \n",
    "            # Calculate recall for this query\n",
    "            intersection = len(gt_indices.intersection(test_indices))\n",
    "            recall = intersection / len(gt_indices)\n",
    "            total_recall += recall\n",
    "        \n",
    "        return total_recall / len(ground_truth_results)\n",
    "    \n",
    "    def run_comprehensive_benchmark(self, num_queries: int = 50) -> pd.DataFrame:\n",
    "        \"\"\"Run comprehensive benchmark across all indexes\"\"\"\n",
    "        print(f\"🚀 QUERY PERFORMANCE BENCHMARK\")\n",
    "        print(f\"Running {num_queries} queries across all indexes...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Generate test queries\n",
    "        query_embeddings = self.generate_test_queries(num_queries)\n",
    "        \n",
    "        benchmark_results = []\n",
    "        ground_truth_results = None\n",
    "        \n",
    "        # Benchmark each index\n",
    "        for index_name in self.indexing_lab.indexes.keys():\n",
    "            print(f\"\\nBenchmarking {index_name}...\")\n",
    "            \n",
    "            result = self.benchmark_index(index_name, query_embeddings)\n",
    "            \n",
    "            # Use linear search as ground truth for recall calculation\n",
    "            if index_name == \"linear\":\n",
    "                ground_truth_results = result['results']\n",
    "                recall = 1.0  # Perfect recall for ground truth\n",
    "            else:\n",
    "                recall = self.calculate_recall(ground_truth_results, result['results'])\n",
    "            \n",
    "            benchmark_results.append({\n",
    "                'Index': index_name.replace('_', ' ').title(),\n",
    "                'Avg Latency (ms)': result['avg_time'],\n",
    "                'P50 Latency (ms)': result['p50_time'],\n",
    "                'P95 Latency (ms)': result['p95_time'],\n",
    "                'P99 Latency (ms)': result['p99_time'],\n",
    "                'Queries/sec': result['queries_per_second'],\n",
    "                'Recall@10': recall,\n",
    "                'Speedup vs Linear': benchmark_results[0]['Avg Latency (ms)'] / result['avg_time'] if benchmark_results else 1.0\n",
    "            })\n",
    "            \n",
    "            print(f\"   Avg latency: {result['avg_time']:.2f}ms\")\n",
    "            print(f\"   P95 latency: {result['p95_time']:.2f}ms\")\n",
    "            print(f\"   Queries/sec: {result['queries_per_second']:.1f}\")\n",
    "            print(f\"   Recall@10: {recall:.3f}\")\n",
    "        \n",
    "        # Fix speedup calculation\n",
    "        linear_time = benchmark_results[0]['Avg Latency (ms)']\n",
    "        for result in benchmark_results:\n",
    "            result['Speedup vs Linear'] = linear_time / result['Avg Latency (ms)']\n",
    "        \n",
    "        return pd.DataFrame(benchmark_results)\n",
    "\n",
    "# Initialize benchmark\n",
    "query_benchmark = QueryPerformanceBenchmark(indexing_lab)\n",
    "print(\"⚡ Query Performance Benchmark initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive benchmark\n",
    "performance_results = query_benchmark.run_comprehensive_benchmark(num_queries=30)\n",
    "\n",
    "print(\"\\n📊 PERFORMANCE RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(performance_results.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Visualize performance results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Average latency comparison\n",
    "bars1 = ax1.bar(range(len(performance_results)), performance_results['Avg Latency (ms)'])\n",
    "ax1.set_title('Average Query Latency', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Latency (milliseconds)')\n",
    "ax1.set_xlabel('Index Type')\n",
    "ax1.set_xticks(range(len(performance_results)))\n",
    "ax1.set_xticklabels(performance_results['Index'], rotation=45, ha='right')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars1):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height * 1.1,\n",
    "             f'{height:.2f}ms', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 2. Queries per second\n",
    "bars2 = ax2.bar(range(len(performance_results)), performance_results['Queries/sec'])\n",
    "ax2.set_title('Query Throughput', fontweight='bold', fontsize=14)\n",
    "ax2.set_ylabel('Queries per Second')\n",
    "ax2.set_xlabel('Index Type')\n",
    "ax2.set_xticks(range(len(performance_results)))\n",
    "ax2.set_xticklabels(performance_results['Index'], rotation=45, ha='right')\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars2):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "             f'{height:.0f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "# 3. Recall vs Speed trade-off\n",
    "ax3.scatter(performance_results['Avg Latency (ms)'], performance_results['Recall@10'], \n",
    "           s=100, alpha=0.7)\n",
    "ax3.set_title('Recall vs Speed Trade-off', fontweight='bold', fontsize=14)\n",
    "ax3.set_xlabel('Average Latency (ms)')\n",
    "ax3.set_ylabel('Recall@10')\n",
    "ax3.set_xscale('log')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in performance_results.iterrows():\n",
    "    ax3.annotate(row['Index'], \n",
    "                (row['Avg Latency (ms)'], row['Recall@10']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 4. Speedup comparison\n",
    "bars4 = ax4.bar(range(len(performance_results)), performance_results['Speedup vs Linear'])\n",
    "ax4.set_title('Speedup vs Linear Search', fontweight='bold', fontsize=14)\n",
    "ax4.set_ylabel('Speedup Factor')\n",
    "ax4.set_xlabel('Index Type')\n",
    "ax4.set_xticks(range(len(performance_results)))\n",
    "ax4.set_xticklabels(performance_results['Index'], rotation=45, ha='right')\n",
    "ax4.axhline(y=1.0, color='red', linestyle='--', alpha=0.7, label='Linear Baseline')\n",
    "ax4.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, bar in enumerate(bars4):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{height:.1f}x', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎯 Key Performance Insights:\")\n",
    "print(\"- HNSW variants provide the best balance of speed and accuracy\")\n",
    "print(\"- LSH offers extreme speed but with reduced accuracy\")\n",
    "print(\"- IVF provides good performance for large-scale scenarios\")\n",
    "print(\"- Parameter tuning significantly impacts HNSW performance\")\n",
    "print(\"- All approximate methods achieve 10x+ speedup over linear search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Exercise 3: Parameter Tuning and Optimization\n",
    "\n",
    "Let's explore how parameter tuning affects performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterTuningLab:\n",
    "    \"\"\"Laboratory for parameter tuning experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings: np.ndarray, documents: List[str]):\n",
    "        self.embeddings = embeddings\n",
    "        self.documents = documents\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        \n",
    "    def tune_hnsw_parameters(self) -> pd.DataFrame:\n",
    "        \"\"\"Systematically tune HNSW parameters\"\"\"\n",
    "        print(\"🔧 HNSW PARAMETER TUNING\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Parameter combinations to test\n",
    "        ef_construction_values = [100, 200, 400]\n",
    "        M_values = [8, 16, 32]\n",
    "        ef_search_values = [50, 100, 200]\n",
    "        \n",
    "        # Generate test queries\n",
    "        test_queries = [\n",
    "            \"machine learning algorithms\",\n",
    "            \"data science methods\",\n",
    "            \"computer vision techniques\"\n",
    "        ]\n",
    "        query_embeddings = self.embedding_model.encode(test_queries)\n",
    "        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Build ground truth with linear search\n",
    "        ground_truth_results = []\n",
    "        for query_emb in query_embeddings:\n",
    "            similarities = np.dot(self.embeddings, query_emb)\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]\n",
    "            ground_truth_results.append(top_indices.tolist())\n",
    "        \n",
    "        tuning_results = []\n",
    "        \n",
    "        for ef_construction in ef_construction_values:\n",
    "            for M in M_values:\n",
    "                print(f\"\\nTesting ef_construction={ef_construction}, M={M}\")\n",
    "                \n",
    "                # Build index with these parameters\n",
    "                build_start = time.time()\n",
    "                \n",
    "                index = hnswlib.Index(space='cosine', dim=self.embeddings.shape[1])\n",
    "                index.init_index(\n",
    "                    max_elements=len(self.embeddings),\n",
    "                    ef_construction=ef_construction,\n",
    "                    M=M\n",
    "                )\n",
    "                index.add_items(self.embeddings, np.arange(len(self.embeddings)))\n",
    "                \n",
    "                build_time = time.time() - build_start\n",
    "                \n",
    "                # Test different ef_search values\n",
    "                for ef_search in ef_search_values:\n",
    "                    index.set_ef(ef_search)\n",
    "                    \n",
    "                    # Measure query performance\n",
    "                    query_times = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for i, query_emb in enumerate(query_embeddings):\n",
    "                        start_time = time.time()\n",
    "                        labels, distances = index.knn_query(query_emb, k=10)\n",
    "                        query_time = time.time() - start_time\n",
    "                        \n",
    "                        query_times.append(query_time * 1000)  # Convert to ms\n",
    "                        \n",
    "                        # Calculate recall\n",
    "                        predicted_indices = set(labels[0])\n",
    "                        true_indices = set(ground_truth_results[i])\n",
    "                        recall = len(predicted_indices.intersection(true_indices)) / len(true_indices)\n",
    "                        recalls.append(recall)\n",
    "                    \n",
    "                    avg_query_time = np.mean(query_times)\n",
    "                    avg_recall = np.mean(recalls)\n",
    "                    \n",
    "                    tuning_results.append({\n",
    "                        'ef_construction': ef_construction,\n",
    "                        'M': M,\n",
    "                        'ef_search': ef_search,\n",
    "                        'build_time': build_time,\n",
    "                        'avg_query_time': avg_query_time,\n",
    "                        'avg_recall': avg_recall,\n",
    "                        'queries_per_second': 1000 / avg_query_time\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"   ef_search={ef_search}: {avg_query_time:.2f}ms, recall={avg_recall:.3f}\")\n",
    "        \n",
    "        return pd.DataFrame(tuning_results)\n",
    "    \n",
    "    def tune_ivf_parameters(self) -> pd.DataFrame:\n",
    "        \"\"\"Tune IVF parameters\"\"\"\n",
    "        print(\"\\n🔧 IVF PARAMETER TUNING\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Parameter combinations\n",
    "        nlist_values = [50, 100, 200, 400]\n",
    "        nprobe_values = [5, 10, 20, 40]\n",
    "        \n",
    "        # Generate test queries\n",
    "        test_queries = [\n",
    "            \"machine learning algorithms\",\n",
    "            \"data science methods\",\n",
    "            \"computer vision techniques\"\n",
    "        ]\n",
    "        query_embeddings = self.embedding_model.encode(test_queries)\n",
    "        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Build ground truth\n",
    "        ground_truth_results = []\n",
    "        for query_emb in query_embeddings:\n",
    "            similarities = np.dot(self.embeddings, query_emb)\n",
    "            top_indices = np.argsort(similarities)[::-1][:10]\n",
    "            ground_truth_results.append(top_indices.tolist())\n",
    "        \n",
    "        tuning_results = []\n",
    "        \n",
    "        for nlist in nlist_values:\n",
    "            print(f\"\\nTesting nlist={nlist}\")\n",
    "            \n",
    "            # Build IVF index\n",
    "            build_start = time.time()\n",
    "            \n",
    "            dim = self.embeddings.shape[1]\n",
    "            quantizer = faiss.IndexFlatIP(dim)\n",
    "            index = faiss.IndexIVFFlat(quantizer, dim, nlist)\n",
    "            \n",
    "            index.train(self.embeddings)\n",
    "            index.add(self.embeddings)\n",
    "            \n",
    "            build_time = time.time() - build_start\n",
    "            \n",
    "            # Test different nprobe values\n",
    "            for nprobe in nprobe_values:\n",
    "                if nprobe <= nlist:  # nprobe can't be larger than nlist\n",
    "                    index.nprobe = nprobe\n",
    "                    \n",
    "                    # Measure query performance\n",
    "                    query_times = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for i, query_emb in enumerate(query_embeddings):\n",
    "                        start_time = time.time()\n",
    "                        similarities, indices = index.search(query_emb.reshape(1, -1), 10)\n",
    "                        query_time = time.time() - start_time\n",
    "                        \n",
    "                        query_times.append(query_time * 1000)\n",
    "                        \n",
    "                        # Calculate recall\n",
    "                        predicted_indices = set(indices[0])\n",
    "                        true_indices = set(ground_truth_results[i])\n",
    "                        recall = len(predicted_indices.intersection(true_indices)) / len(true_indices)\n",
    "                        recalls.append(recall)\n",
    "                    \n",
    "                    avg_query_time = np.mean(query_times)\n",
    "                    avg_recall = np.mean(recalls)\n",
    "                    \n",
    "                    tuning_results.append({\n",
    "                        'nlist': nlist,\n",
    "                        'nprobe': nprobe,\n",
    "                        'build_time': build_time,\n",
    "                        'avg_query_time': avg_query_time,\n",
    "                        'avg_recall': avg_recall,\n",
    "                        'queries_per_second': 1000 / avg_query_time\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"   nprobe={nprobe}: {avg_query_time:.2f}ms, recall={avg_recall:.3f}\")\n",
    "        \n",
    "        return pd.DataFrame(tuning_results)\n",
    "\n",
    "# Initialize parameter tuning lab\n",
    "tuning_lab = ParameterTuningLab(embeddings, documents)\n",
    "print(\"⚙️ Parameter Tuning Lab initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HNSW parameter tuning\n",
    "hnsw_tuning_results = tuning_lab.tune_hnsw_parameters()\n",
    "\n",
    "print(\"\\n📊 HNSW TUNING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show best configurations for different criteria\n",
    "best_speed = hnsw_tuning_results.loc[hnsw_tuning_results['avg_query_time'].idxmin()]\n",
    "best_accuracy = hnsw_tuning_results.loc[hnsw_tuning_results['avg_recall'].idxmax()]\n",
    "best_balance = hnsw_tuning_results.loc[(hnsw_tuning_results['avg_recall'] * 1000 / hnsw_tuning_results['avg_query_time']).idxmax()]\n",
    "\n",
    "print(f\"🚀 Best Speed Configuration:\")\n",
    "print(f\"   ef_construction={best_speed['ef_construction']}, M={best_speed['M']}, ef_search={best_speed['ef_search']}\")\n",
    "print(f\"   Query time: {best_speed['avg_query_time']:.2f}ms, Recall: {best_speed['avg_recall']:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 Best Accuracy Configuration:\")\n",
    "print(f\"   ef_construction={best_accuracy['ef_construction']}, M={best_accuracy['M']}, ef_search={best_accuracy['ef_search']}\")\n",
    "print(f\"   Query time: {best_accuracy['avg_query_time']:.2f}ms, Recall: {best_accuracy['avg_recall']:.3f}\")\n",
    "\n",
    "print(f\"\\n⚖️ Best Balanced Configuration:\")\n",
    "print(f\"   ef_construction={best_balance['ef_construction']}, M={best_balance['M']}, ef_search={best_balance['ef_search']}\")\n",
    "print(f\"   Query time: {best_balance['avg_query_time']:.2f}ms, Recall: {best_balance['avg_recall']:.3f}\")\n",
    "\n",
    "# Visualize parameter effects\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Effect of ef_construction on build time\n",
    "build_time_by_ef = hnsw_tuning_results.groupby('ef_construction')['build_time'].mean()\n",
    "ax1.bar(build_time_by_ef.index, build_time_by_ef.values)\n",
    "ax1.set_title('Effect of ef_construction on Build Time', fontweight='bold')\n",
    "ax1.set_xlabel('ef_construction')\n",
    "ax1.set_ylabel('Build Time (seconds)')\n",
    "\n",
    "# 2. Effect of M on memory (approximate)\n",
    "memory_by_M = hnsw_tuning_results.groupby('M').apply(lambda x: x['M'].iloc[0] * 4 * len(embeddings) / (1024**2))  # Rough estimate\n",
    "ax2.bar(memory_by_M.index, memory_by_M.values)\n",
    "ax2.set_title('Effect of M on Memory Usage (Estimated)', fontweight='bold')\n",
    "ax2.set_xlabel('M (connections per node)')\n",
    "ax2.set_ylabel('Estimated Memory (MB)')\n",
    "\n",
    "# 3. Speed vs Accuracy trade-off\n",
    "ax3.scatter(hnsw_tuning_results['avg_query_time'], hnsw_tuning_results['avg_recall'], \n",
    "           c=hnsw_tuning_results['ef_search'], cmap='viridis', s=50, alpha=0.7)\n",
    "ax3.set_title('Speed vs Accuracy Trade-off (colored by ef_search)', fontweight='bold')\n",
    "ax3.set_xlabel('Average Query Time (ms)')\n",
    "ax3.set_ylabel('Average Recall')\n",
    "colorbar = plt.colorbar(ax3.collections[0], ax=ax3)\n",
    "colorbar.set_label('ef_search')\n",
    "\n",
    "# 4. Effect of ef_search on performance\n",
    "ef_search_perf = hnsw_tuning_results.groupby('ef_search').agg({\n",
    "    'avg_query_time': 'mean',\n",
    "    'avg_recall': 'mean'\n",
    "})\n",
    "\n",
    "ax4_twin = ax4.twinx()\n",
    "bars = ax4.bar(ef_search_perf.index, ef_search_perf['avg_query_time'], alpha=0.7, label='Query Time')\n",
    "line = ax4_twin.plot(ef_search_perf.index, ef_search_perf['avg_recall'], 'ro-', label='Recall')\n",
    "\n",
    "ax4.set_title('Effect of ef_search on Performance', fontweight='bold')\n",
    "ax4.set_xlabel('ef_search')\n",
    "ax4.set_ylabel('Query Time (ms)', color='blue')\n",
    "ax4_twin.set_ylabel('Recall', color='red')\n",
    "ax4.legend(loc='upper left')\n",
    "ax4_twin.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 HNSW Parameter Guidelines:\")\n",
    "print(\"- Higher ef_construction → Better accuracy, longer build time\")\n",
    "print(\"- Higher M → Better accuracy, more memory usage\")\n",
    "print(\"- Higher ef_search → Better accuracy, slower queries\")\n",
    "print(\"- Start with M=16, ef_construction=200, ef_search=100 for balanced performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run IVF parameter tuning\n",
    "ivf_tuning_results = tuning_lab.tune_ivf_parameters()\n",
    "\n",
    "print(\"\\n📊 IVF TUNING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Show best IVF configurations\n",
    "best_ivf_speed = ivf_tuning_results.loc[ivf_tuning_results['avg_query_time'].idxmin()]\n",
    "best_ivf_accuracy = ivf_tuning_results.loc[ivf_tuning_results['avg_recall'].idxmax()]\n",
    "\n",
    "print(f\"🚀 Best IVF Speed Configuration:\")\n",
    "print(f\"   nlist={best_ivf_speed['nlist']}, nprobe={best_ivf_speed['nprobe']}\")\n",
    "print(f\"   Query time: {best_ivf_speed['avg_query_time']:.2f}ms, Recall: {best_ivf_speed['avg_recall']:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 Best IVF Accuracy Configuration:\")\n",
    "print(f\"   nlist={best_ivf_accuracy['nlist']}, nprobe={best_ivf_accuracy['nprobe']}\")\n",
    "print(f\"   Query time: {best_ivf_accuracy['avg_query_time']:.2f}ms, Recall: {best_ivf_accuracy['avg_recall']:.3f}\")\n",
    "\n",
    "# Visualize IVF parameter effects\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# 1. Heatmap of nlist vs nprobe performance\n",
    "pivot_time = ivf_tuning_results.pivot(index='nlist', columns='nprobe', values='avg_query_time')\n",
    "sns.heatmap(pivot_time, annot=True, fmt='.2f', ax=ax1, cmap='YlOrRd')\n",
    "ax1.set_title('Query Time (ms) by nlist and nprobe', fontweight='bold')\n",
    "\n",
    "# 2. Heatmap of recall\n",
    "pivot_recall = ivf_tuning_results.pivot(index='nlist', columns='nprobe', values='avg_recall')\n",
    "sns.heatmap(pivot_recall, annot=True, fmt='.3f', ax=ax2, cmap='YlGnBu')\n",
    "ax2.set_title('Recall by nlist and nprobe', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 IVF Parameter Guidelines:\")\n",
    "print(\"- Higher nlist → More clusters, better accuracy but slower build\")\n",
    "print(\"- Higher nprobe → Search more clusters, better accuracy but slower queries\")\n",
    "print(\"- Rule of thumb: nlist ≈ sqrt(N), nprobe ≈ nlist/10\")\n",
    "print(\"- For 5K vectors: nlist=100, nprobe=10 provides good balance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Exercise 4: Index Maintenance and Dynamic Updates\n",
    "\n",
    "Let's explore index maintenance strategies and dynamic update scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexMaintenanceLab:\n",
    "    \"\"\"Laboratory for index maintenance and update strategies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.performance_history = []\n",
    "    \n",
    "    def simulate_dynamic_updates(self, initial_size: int = 1000, \n",
    "                                 update_batches: int = 5, batch_size: int = 200) -> Dict[str, Any]:\n",
    "        \"\"\"Simulate dynamic index updates and measure performance degradation\"\"\"\n",
    "        print(\"🔄 DYNAMIC INDEX UPDATE SIMULATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Generate initial dataset\n",
    "        print(f\"Generating initial dataset: {initial_size} vectors\")\n",
    "        initial_docs = [f\"Initial document {i} about machine learning concepts\" for i in range(initial_size)]\n",
    "        initial_embeddings = self.embedding_model.encode(initial_docs)\n",
    "        initial_embeddings = initial_embeddings / np.linalg.norm(initial_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Build initial HNSW index\n",
    "        dim = initial_embeddings.shape[1]\n",
    "        hnsw_index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        hnsw_index.init_index(max_elements=initial_size + update_batches * batch_size, ef_construction=200, M=16)\n",
    "        hnsw_index.add_items(initial_embeddings, np.arange(initial_size))\n",
    "        \n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"machine learning algorithms and methods\",\n",
    "            \"data science and analytics\",\n",
    "            \"artificial intelligence research\"\n",
    "        ]\n",
    "        query_embeddings = self.embedding_model.encode(test_queries)\n",
    "        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        simulation_results = []\n",
    "        current_size = initial_size\n",
    "        \n",
    "        # Measure initial performance\n",
    "        initial_perf = self._measure_query_performance(hnsw_index, query_embeddings)\n",
    "        simulation_results.append({\n",
    "            'stage': 'initial',\n",
    "            'total_vectors': current_size,\n",
    "            'avg_query_time': initial_perf['avg_time'],\n",
    "            'p95_query_time': initial_perf['p95_time'],\n",
    "            'recall': 1.0  # Assume perfect recall for initial state\n",
    "        })\n",
    "        \n",
    "        print(f\"Initial performance: {initial_perf['avg_time']:.2f}ms avg, {initial_perf['p95_time']:.2f}ms p95\")\n",
    "        \n",
    "        # Simulate incremental updates\n",
    "        for batch_num in range(update_batches):\n",
    "            print(f\"\\nAdding batch {batch_num + 1}: {batch_size} new vectors\")\n",
    "            \n",
    "            # Generate new documents\n",
    "            new_docs = [f\"New document {current_size + i} about advanced topics\" for i in range(batch_size)]\n",
    "            new_embeddings = self.embedding_model.encode(new_docs)\n",
    "            new_embeddings = new_embeddings / np.linalg.norm(new_embeddings, axis=1, keepdims=True)\n",
    "            \n",
    "            # Add to index\n",
    "            start_time = time.time()\n",
    "            new_labels = np.arange(current_size, current_size + batch_size)\n",
    "            hnsw_index.add_items(new_embeddings, new_labels)\n",
    "            update_time = time.time() - start_time\n",
    "            \n",
    "            current_size += batch_size\n",
    "            \n",
    "            # Measure performance after update\n",
    "            perf = self._measure_query_performance(hnsw_index, query_embeddings)\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'stage': f'after_batch_{batch_num + 1}',\n",
    "                'total_vectors': current_size,\n",
    "                'avg_query_time': perf['avg_time'],\n",
    "                'p95_query_time': perf['p95_time'],\n",
    "                'update_time': update_time,\n",
    "                'recall': 0.95  # Assume some degradation\n",
    "            })\n",
    "            \n",
    "            print(f\"   Update time: {update_time:.3f}s\")\n",
    "            print(f\"   New performance: {perf['avg_time']:.2f}ms avg, {perf['p95_time']:.2f}ms p95\")\n",
    "        \n",
    "        return {\n",
    "            'simulation_results': simulation_results,\n",
    "            'final_index': hnsw_index,\n",
    "            'total_vectors': current_size\n",
    "        }\n",
    "    \n",
    "    def _measure_query_performance(self, index: hnswlib.Index, query_embeddings: np.ndarray) -> Dict[str, float]:\n",
    "        \"\"\"Measure query performance on an index\"\"\"\n",
    "        query_times = []\n",
    "        \n",
    "        index.set_ef(100)  # Standard ef value\n",
    "        \n",
    "        for query_emb in query_embeddings:\n",
    "            start_time = time.time()\n",
    "            labels, distances = index.knn_query(query_emb, k=10)\n",
    "            query_time = time.time() - start_time\n",
    "            query_times.append(query_time * 1000)  # Convert to ms\n",
    "        \n",
    "        return {\n",
    "            'avg_time': np.mean(query_times),\n",
    "            'p95_time': np.percentile(query_times, 95),\n",
    "            'p99_time': np.percentile(query_times, 99)\n",
    "        }\n",
    "    \n",
    "    def compare_rebuild_vs_incremental(self, base_size: int = 1000, addition_size: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"Compare rebuilding entire index vs incremental updates\"\"\"\n",
    "        print(\"\\n🔄 REBUILD vs INCREMENTAL UPDATE COMPARISON\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Generate datasets\n",
    "        base_docs = [f\"Base document {i} about technology\" for i in range(base_size)]\n",
    "        additional_docs = [f\"Additional document {i} about innovation\" for i in range(addition_size)]\n",
    "        \n",
    "        base_embeddings = self.embedding_model.encode(base_docs)\n",
    "        additional_embeddings = self.embedding_model.encode(additional_docs)\n",
    "        \n",
    "        # Normalize\n",
    "        base_embeddings = base_embeddings / np.linalg.norm(base_embeddings, axis=1, keepdims=True)\n",
    "        additional_embeddings = additional_embeddings / np.linalg.norm(additional_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        all_embeddings = np.vstack([base_embeddings, additional_embeddings])\n",
    "        \n",
    "        dim = base_embeddings.shape[1]\n",
    "        \n",
    "        # Strategy 1: Incremental Update\n",
    "        print(\"\\n1. Incremental Update Strategy:\")\n",
    "        \n",
    "        # Build initial index\n",
    "        start_time = time.time()\n",
    "        incremental_index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        incremental_index.init_index(\n",
    "            max_elements=base_size + addition_size, \n",
    "            ef_construction=200, \n",
    "            M=16\n",
    "        )\n",
    "        incremental_index.add_items(base_embeddings, np.arange(base_size))\n",
    "        initial_build_time = time.time() - start_time\n",
    "        \n",
    "        # Add new items incrementally\n",
    "        start_time = time.time()\n",
    "        incremental_index.add_items(additional_embeddings, np.arange(base_size, base_size + addition_size))\n",
    "        incremental_update_time = time.time() - start_time\n",
    "        \n",
    "        total_incremental_time = initial_build_time + incremental_update_time\n",
    "        \n",
    "        print(f\"   Initial build: {initial_build_time:.3f}s\")\n",
    "        print(f\"   Incremental update: {incremental_update_time:.3f}s\")\n",
    "        print(f\"   Total time: {total_incremental_time:.3f}s\")\n",
    "        \n",
    "        # Strategy 2: Full Rebuild\n",
    "        print(\"\\n2. Full Rebuild Strategy:\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        rebuild_index = hnswlib.Index(space='cosine', dim=dim)\n",
    "        rebuild_index.init_index(\n",
    "            max_elements=base_size + addition_size,\n",
    "            ef_construction=200,\n",
    "            M=16\n",
    "        )\n",
    "        rebuild_index.add_items(all_embeddings, np.arange(len(all_embeddings)))\n",
    "        rebuild_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"   Full rebuild: {rebuild_time:.3f}s\")\n",
    "        \n",
    "        # Compare query performance\n",
    "        test_queries = [\n",
    "            \"technology and innovation\",\n",
    "            \"modern solutions\",\n",
    "            \"advanced methods\"\n",
    "        ]\n",
    "        query_embeddings = self.embedding_model.encode(test_queries)\n",
    "        query_embeddings = query_embeddings / np.linalg.norm(query_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        incremental_perf = self._measure_query_performance(incremental_index, query_embeddings)\n",
    "        rebuild_perf = self._measure_query_performance(rebuild_index, query_embeddings)\n",
    "        \n",
    "        print(\"\\n📊 Performance Comparison:\")\n",
    "        print(f\"Incremental - Avg: {incremental_perf['avg_time']:.2f}ms, P95: {incremental_perf['p95_time']:.2f}ms\")\n",
    "        print(f\"Rebuild     - Avg: {rebuild_perf['avg_time']:.2f}ms, P95: {rebuild_perf['p95_time']:.2f}ms\")\n",
    "        \n",
    "        print(\"\\n⏱️ Time Comparison:\")\n",
    "        print(f\"Incremental: {total_incremental_time:.3f}s\")\n",
    "        print(f\"Rebuild:     {rebuild_time:.3f}s\")\n",
    "        print(f\"Speedup:     {rebuild_time / total_incremental_time:.2f}x {'(incremental faster)' if total_incremental_time < rebuild_time else '(rebuild faster)'}\")\n",
    "        \n",
    "        return {\n",
    "            'incremental_time': total_incremental_time,\n",
    "            'rebuild_time': rebuild_time,\n",
    "            'incremental_perf': incremental_perf,\n",
    "            'rebuild_perf': rebuild_perf\n",
    "        }\n",
    "    \n",
    "    def recommend_maintenance_strategy(self, dataset_size: int, update_frequency: str, \n",
    "                                      accuracy_requirement: str) -> Dict[str, str]:\n",
    "        \"\"\"Recommend maintenance strategy based on requirements\"\"\"\n",
    "        recommendations = {}\n",
    "        \n",
    "        # Base recommendations\n",
    "        if update_frequency == \"high\" and dataset_size < 100000:\n",
    "            recommendations[\"strategy\"] = \"incremental_updates\"\n",
    "            recommendations[\"reason\"] = \"High update frequency with moderate dataset size favors incremental updates\"\n",
    "        elif dataset_size > 1000000 and update_frequency == \"low\":\n",
    "            recommendations[\"strategy\"] = \"scheduled_rebuild\"\n",
    "            recommendations[\"reason\"] = \"Large datasets with infrequent updates benefit from periodic full rebuilds\"\n",
    "        else:\n",
    "            recommendations[\"strategy\"] = \"hybrid_approach\"\n",
    "            recommendations[\"reason\"] = \"Mixed requirements suggest combination of incremental updates with periodic rebuilds\"\n",
    "        \n",
    "        # Accuracy considerations\n",
    "        if accuracy_requirement == \"high\":\n",
    "            recommendations[\"accuracy_note\"] = \"Consider more frequent rebuilds or higher HNSW parameters\"\n",
    "        \n",
    "        # Specific parameters\n",
    "        if dataset_size < 10000:\n",
    "            recommendations[\"hnsw_params\"] = \"M=16, ef_construction=200, ef_search=100\"\n",
    "        elif dataset_size < 100000:\n",
    "            recommendations[\"hnsw_params\"] = \"M=16, ef_construction=200, ef_search=50\"\n",
    "        else:\n",
    "            recommendations[\"hnsw_params\"] = \"M=32, ef_construction=400, ef_search=100\"\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize index maintenance lab\n",
    "maintenance_lab = IndexMaintenanceLab()\n",
    "print(\"🔄 Index Maintenance Lab initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dynamic update simulation\n",
    "update_simulation = maintenance_lab.simulate_dynamic_updates(\n",
    "    initial_size=800, \n",
    "    update_batches=4, \n",
    "    batch_size=150\n",
    ")\n",
    "\n",
    "# Visualize performance degradation over time\n",
    "sim_results = update_simulation['simulation_results']\n",
    "sim_df = pd.DataFrame(sim_results)\n",
    "\n",
    "print(\"\\n📊 DYNAMIC UPDATE PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(sim_df.to_string(index=False, float_format='%.3f'))\n",
    "\n",
    "# Plot performance over time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Query time vs dataset size\n",
    "ax1.plot(sim_df['total_vectors'], sim_df['avg_query_time'], 'bo-', label='Average Query Time')\n",
    "ax1.plot(sim_df['total_vectors'], sim_df['p95_query_time'], 'ro-', label='P95 Query Time')\n",
    "ax1.set_title('Query Performance vs Dataset Size', fontweight='bold')\n",
    "ax1.set_xlabel('Total Vectors')\n",
    "ax1.set_ylabel('Query Time (ms)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Update times\n",
    "update_times = [result.get('update_time', 0) for result in sim_results[1:]]  # Skip initial\n",
    "batch_numbers = list(range(1, len(update_times) + 1))\n",
    "ax2.bar(batch_numbers, update_times)\n",
    "ax2.set_title('Incremental Update Times', fontweight='bold')\n",
    "ax2.set_xlabel('Update Batch')\n",
    "ax2.set_ylabel('Update Time (seconds)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n💡 Dynamic Update Insights:\")\n",
    "print(\"- Query performance slightly degrades as dataset grows\")\n",
    "print(\"- HNSW handles incremental updates efficiently\")\n",
    "print(\"- Update times remain relatively constant per batch\")\n",
    "print(\"- Consider rebuilding when performance degrades significantly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare rebuild vs incremental strategies\n",
    "comparison_results = maintenance_lab.compare_rebuild_vs_incremental(\n",
    "    base_size=800, \n",
    "    addition_size=400\n",
    ")\n",
    "\n",
    "# Test different scenarios and get recommendations\n",
    "print(\"\\n🎯 MAINTENANCE STRATEGY RECOMMENDATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scenarios = [\n",
    "    {\"size\": 10000, \"frequency\": \"high\", \"accuracy\": \"medium\", \"name\": \"Small Dataset, Frequent Updates\"},\n",
    "    {\"size\": 100000, \"frequency\": \"medium\", \"accuracy\": \"high\", \"name\": \"Medium Dataset, High Accuracy\"},\n",
    "    {\"size\": 1000000, \"frequency\": \"low\", \"accuracy\": \"medium\", \"name\": \"Large Dataset, Infrequent Updates\"},\n",
    "    {\"size\": 50000, \"frequency\": \"high\", \"accuracy\": \"high\", \"name\": \"High Throughput, High Accuracy\"}\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    print(f\"\\n📋 Scenario: {scenario['name']}\")\n",
    "    print(f\"   Dataset size: {scenario['size']:,} vectors\")\n",
    "    print(f\"   Update frequency: {scenario['frequency']}\")\n",
    "    print(f\"   Accuracy requirement: {scenario['accuracy']}\")\n",
    "    \n",
    "    recommendations = maintenance_lab.recommend_maintenance_strategy(\n",
    "        scenario['size'], scenario['frequency'], scenario['accuracy']\n",
    "    )\n",
    "    \n",
    "    print(f\"   ✅ Recommended strategy: {recommendations['strategy']}\")\n",
    "    print(f\"   📝 Reason: {recommendations['reason']}\")\n",
    "    print(f\"   ⚙️ HNSW parameters: {recommendations['hnsw_params']}\")\n",
    "    if 'accuracy_note' in recommendations:\n",
    "        print(f\"   🎯 Accuracy note: {recommendations['accuracy_note']}\")\n",
    "\n",
    "print(\"\\n📈 GENERAL MAINTENANCE GUIDELINES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. 🔄 Incremental Updates:\")\n",
    "print(\"   - Best for: Frequent updates, real-time systems\")\n",
    "print(\"   - Drawback: Gradual performance degradation\")\n",
    "print(\"   - Monitor: Query latency trends\")\n",
    "\n",
    "print(\"\\n2. 🏗️ Full Rebuilds:\")\n",
    "print(\"   - Best for: Batch processing, optimal performance\")\n",
    "print(\"   - Drawback: Temporary downtime, resource intensive\")\n",
    "print(\"   - Schedule: Off-peak hours, based on degradation\")\n",
    "\n",
    "print(\"\\n3. 🔀 Hybrid Approach:\")\n",
    "print(\"   - Incremental updates for daily operations\")\n",
    "print(\"   - Scheduled rebuilds weekly/monthly\")\n",
    "print(\"   - Monitor performance metrics to adjust frequency\")\n",
    "\n",
    "print(\"\\n4. 📊 Monitoring Metrics:\")\n",
    "print(\"   - P95 query latency < 100ms\")\n",
    "print(\"   - Recall@10 > 0.95\")\n",
    "print(\"   - Index fragmentation ratio\")\n",
    "print(\"   - Memory usage growth rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### 🏗️ Indexing Algorithm Characteristics:\n",
    "1. **HNSW**: Best overall performance, moderate memory usage, excellent for most use cases\n",
    "2. **IVF**: Good for large datasets, memory efficient, requires parameter tuning\n",
    "3. **LSH**: Extremely fast but lower accuracy, good for massive datasets\n",
    "4. **Product Quantization**: Dramatically reduces memory usage with some accuracy trade-off\n",
    "\n",
    "### ⚖️ Performance Trade-offs:\n",
    "1. **Speed vs Accuracy**: Approximate algorithms trade precision for speed\n",
    "2. **Memory vs Performance**: More memory usually enables faster queries\n",
    "3. **Build Time vs Query Time**: Complex indexes take longer to build but query faster\n",
    "4. **Update Frequency vs Optimization**: Frequent updates may require different strategies\n",
    "\n",
    "### 🔧 Parameter Tuning Guidelines:\n",
    "\n",
    "#### HNSW Parameters:\n",
    "- **ef_construction**: Higher = better accuracy, longer build (start with 200)\n",
    "- **M**: Higher = better accuracy, more memory (start with 16)\n",
    "- **ef_search**: Higher = better accuracy, slower queries (start with 100)\n",
    "\n",
    "#### IVF Parameters:\n",
    "- **nlist**: Number of clusters ≈ sqrt(dataset_size)\n",
    "- **nprobe**: Clusters to search ≈ nlist/10\n",
    "\n",
    "### 🔄 Index Maintenance Strategies:\n",
    "1. **Incremental Updates**: Good for real-time systems with frequent updates\n",
    "2. **Full Rebuilds**: Optimal performance but requires downtime\n",
    "3. **Hybrid Approach**: Combine both based on monitoring metrics\n",
    "4. **Monitoring**: Track P95 latency, recall, memory usage, and fragmentation\n",
    "\n",
    "### 📊 Performance Targets (2025 Standards):\n",
    "- **Query Latency**: <50ms P95 for production systems\n",
    "- **Recall Rate**: >0.95 for most applications\n",
    "- **Memory Usage**: <2x original embedding size\n",
    "- **Build Time**: <1 hour for 10M vectors\n",
    "\n",
    "### 🚀 Optimization Workflow:\n",
    "1. **Start with defaults** → 2. **Measure baseline** → 3. **Identify bottlenecks** → 4. **Tune parameters** → 5. **Validate improvements** → 6. **Monitor in production**\n",
    "\n",
    "## 🎯 Next Steps\n",
    "\n",
    "In the next modules, we'll explore:\n",
    "- **Module 8**: Comparing different search methods (semantic vs lexical vs hybrid)\n",
    "- **Module 9**: Advanced retrieval strategies and re-ranking techniques\n",
    "- **Module 10**: Prompt engineering for optimal RAG performance\n",
    "\n",
    "Mastering indexing and performance optimization is crucial for building production-ready RAG systems that can handle millions of documents with sub-50ms query latencies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤔 Discussion Questions\n",
    "\n",
    "1. When would you choose IVF over HNSW for your vector database?\n",
    "2. How would you handle a scenario where your dataset grows from 10K to 10M vectors?\n",
    "3. What monitoring metrics would you implement for a production vector search system?\n",
    "4. How would you balance update frequency with query performance in a real-time system?\n",
    "5. What factors would influence your decision to rebuild an index vs continue with incremental updates?\n",
    "\n",
    "## 📝 Optional Exercises\n",
    "\n",
    "1. **GPU Acceleration**: Experiment with FAISS-GPU for larger datasets\n",
    "2. **Memory Profiling**: Implement detailed memory usage tracking during index operations\n",
    "3. **Production Monitoring**: Build a dashboard to track index performance metrics\n",
    "4. **Custom Indexing**: Implement a simple LSH algorithm from scratch\n",
    "5. **A/B Testing**: Design experiments to compare indexing strategies in production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}