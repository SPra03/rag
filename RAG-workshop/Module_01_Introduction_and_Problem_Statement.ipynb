{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Introduction & Problem Statement\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Understand the fundamental limitations of standalone LLMs\n",
    "- Recognize when RAG is the right solution\n",
    "- Visualize the complete RAG workflow\n",
    "- Experience hands-on examples of LLM limitations and RAG solutions\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### What is RAG?\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances Large Language Models (LLMs) by providing them with relevant external information before generating responses.\n",
    "\n",
    "Think of it like an open-book exam:\n",
    "- **Without RAG**: LLM answers from memory only (closed-book exam)\n",
    "- **With RAG**: LLM gets relevant documents first, then answers (open-book exam)\n",
    "\n",
    "### Why Do We Need RAG?\n",
    "\n",
    "#### üö´ LLM Limitations\n",
    "1. **Knowledge Cutoff Dates**: LLMs are trained on data up to a certain date\n",
    "2. **Hallucinations**: LLMs can generate confident-sounding but incorrect information\n",
    "3. **No Domain-Specific Knowledge**: Limited knowledge about your company/domain\n",
    "4. **No Real-time Information**: Cannot access current events or dynamic data\n",
    "5. **Context Length Limits**: Cannot process entire large documents\n",
    "\n",
    "#### ‚úÖ How RAG Helps\n",
    "1. **Fresh Information**: Access to up-to-date external data\n",
    "2. **Grounded Responses**: Answers based on provided evidence\n",
    "3. **Domain Expertise**: Include your specific documents and data\n",
    "4. **Source Attribution**: Know where information comes from\n",
    "5. **Cost Effective**: No need to retrain models with new data\n",
    "\n",
    "### 2025 Research Insights üî¨\n",
    "- **Mathematical Proof**: OpenAI researchers proved LLM hallucinations are mathematically inevitable (Sept 2025)\n",
    "- **Current Best**: Anthropic Claude 3.7 has the lowest hallucination rate at 17%\n",
    "- **RAG Impact**: Properly implemented RAG can reduce hallucinations by 49-67%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "Let's install the required packages and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q openai langchain python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up OpenAI API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LangChain LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,  # Low temperature for more consistent results\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"üìÖ Today's date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Demonstrating LLM Limitations\n",
    "\n",
    "Let's see these limitations in action with real examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Knowledge Cutoff Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test knowledge cutoff with recent events\n",
    "recent_questions = [\n",
    "    \"What happened in the 2024 US Presidential Election?\",\n",
    "    \"Who won the 2024 Nobel Prize in Physics?\",\n",
    "    \"What are the latest features in iPhone 16?\",\n",
    "    \"What is the current stock price of NVIDIA?\"\n",
    "]\n",
    "\n",
    "print(\"üîç Testing Knowledge Cutoff Issues:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in recent_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=question)])\n",
    "        print(f\"ü§ñ LLM Response: {response.content[:200]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Hallucinations (Confident but Wrong Answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with questions that might trigger hallucinations\n",
    "tricky_questions = [\n",
    "    \"What is the exact population of Atlantis?\",\n",
    "    \"Who is the CEO of DataScience Corp (a fictional company)?\",\n",
    "    \"What are the side effects of Imaginex (a made-up drug)?\",\n",
    "    \"What year was the Treaty of Fabrication signed?\"\n",
    "]\n",
    "\n",
    "print(\"üé≠ Testing Hallucination Tendencies:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in tricky_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=question)])\n",
    "        print(f\"ü§ñ LLM Response: {response.content}\")\n",
    "        print(\"‚ö†Ô∏è  Note: This response might be hallucinated since the question involves fictional entities!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: No Domain-Specific Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with company-specific questions\n",
    "company_questions = [\n",
    "    \"What is our company's return policy?\",\n",
    "    \"Who is the head of our marketing department?\",\n",
    "    \"What are the specs of our Model-X product?\",\n",
    "    \"What was discussed in last week's board meeting?\"\n",
    "]\n",
    "\n",
    "print(\"üè¢ Testing Domain-Specific Knowledge:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for question in company_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    try:\n",
    "        response = llm.invoke([HumanMessage(content=question)])\n",
    "        print(f\"ü§ñ LLM Response: {response.content}\")\n",
    "        print(\"üìù Note: LLM cannot access company-specific information!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Exercise 2: Simple RAG Preview\n",
    "\n",
    "Now let's see how RAG can solve these problems with a basic example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some sample company knowledge\n",
    "company_knowledge = \"\"\"\n",
    "COMPANY INFORMATION DATABASE\n",
    "============================\n",
    "\n",
    "COMPANY: TechCorp Solutions\n",
    "FOUNDED: 2018\n",
    "HEADQUARTERS: San Francisco, CA\n",
    "\n",
    "LEADERSHIP:\n",
    "- CEO: Sarah Johnson\n",
    "- CTO: Michael Chen  \n",
    "- Head of Marketing: Lisa Rodriguez\n",
    "- Head of Sales: David Kim\n",
    "\n",
    "PRODUCTS:\n",
    "- Model-X: AI-powered analytics platform\n",
    "  Specs: 99.9% uptime, supports 1M+ queries/sec, cloud-native\n",
    "- Model-Y: Customer service automation tool\n",
    "  Specs: 24/7 support, multi-language, integrates with 50+ platforms\n",
    "\n",
    "POLICIES:\n",
    "- Return Policy: 30-day money-back guarantee on all products\n",
    "- Support: 24/7 technical support for enterprise customers\n",
    "- Privacy: SOC2 Type II certified, GDPR compliant\n",
    "\n",
    "RECENT NEWS:\n",
    "- 2024-01-15: Launched Model-Y 2.0 with enhanced AI capabilities\n",
    "- 2024-02-10: Secured $50M Series B funding\n",
    "- 2024-03-05: Opened new office in London\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìö Company Knowledge Base Created!\")\n",
    "print(f\"Knowledge base contains {len(company_knowledge.split())} words of information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag_query(question, knowledge_base):\n",
    "    \"\"\"\n",
    "    A simple RAG implementation:\n",
    "    1. Provide relevant context to the LLM\n",
    "    2. Ask the LLM to answer based on that context\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a prompt that includes our knowledge base\n",
    "    rag_prompt = f\"\"\"\n",
    "    You are a helpful assistant that answers questions based on the provided company information.\n",
    "    \n",
    "    COMPANY INFORMATION:\n",
    "    {knowledge_base}\n",
    "    \n",
    "    QUESTION: {question}\n",
    "    \n",
    "    Please answer the question based ONLY on the information provided above. \n",
    "    If the information is not available, say \"I don't have that information in the company database.\"\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get response from LLM\n",
    "    response = llm.invoke([HumanMessage(content=rag_prompt)])\n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ Simple RAG function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our simple RAG with the same company questions\n",
    "print(\"üöÄ Testing Simple RAG vs Standard LLM:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is our company's return policy?\",\n",
    "    \"Who is the head of our marketing department?\",\n",
    "    \"What are the specs of our Model-X product?\",\n",
    "    \"When did we open our London office?\"\n",
    "]\n",
    "\n",
    "for question in test_questions:\n",
    "    print(f\"\\n‚ùì Question: {question}\")\n",
    "    \n",
    "    # Standard LLM response\n",
    "    print(\"\\nü§ñ Standard LLM (no context):\")\n",
    "    standard_response = llm.invoke([HumanMessage(content=question)])\n",
    "    print(f\"   {standard_response.content}\")\n",
    "    \n",
    "    # RAG response\n",
    "    print(\"\\nüîç RAG LLM (with company context):\")\n",
    "    rag_response = simple_rag_query(question, company_knowledge)\n",
    "    print(f\"   {rag_response}\")\n",
    "    \n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exercise 3: RAG Workflow Visualization\n",
    "\n",
    "Let's understand the complete RAG workflow step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rag_workflow(user_question):\n",
    "    \"\"\"\n",
    "    Demonstrate the RAG workflow step by step\n",
    "    \"\"\"\n",
    "    print(\"üîÑ RAG WORKFLOW DEMONSTRATION\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: User asks a question\n",
    "    print(f\"üìù STEP 1: User Question\")\n",
    "    print(f\"   '{user_question}'\")\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Retrieve relevant documents (simplified)\n",
    "    print(f\"üîç STEP 2: Document Retrieval\")\n",
    "    print(f\"   Searching knowledge base for relevant information...\")\n",
    "    \n",
    "    # Simple keyword matching for demonstration\n",
    "    question_words = user_question.lower().split()\n",
    "    relevant_lines = []\n",
    "    \n",
    "    for line in company_knowledge.split('\\n'):\n",
    "        if any(word in line.lower() for word in question_words if len(word) > 3):\n",
    "            relevant_lines.append(line.strip())\n",
    "    \n",
    "    print(f\"   Found {len(relevant_lines)} relevant lines:\")\n",
    "    for line in relevant_lines[:3]:  # Show first 3 relevant lines\n",
    "        if line:\n",
    "            print(f\"   - {line}\")\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Augment the prompt\n",
    "    print(f\"üìù STEP 3: Prompt Augmentation\")\n",
    "    print(f\"   Combining retrieved context with user question...\")\n",
    "    print(f\"   Context + Question ‚Üí Enhanced Prompt\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Generate response\n",
    "    print(f\"ü§ñ STEP 4: Generation\")\n",
    "    print(f\"   LLM generates response based on provided context...\")\n",
    "    \n",
    "    response = simple_rag_query(user_question, company_knowledge)\n",
    "    print(f\"   Response: '{response}'\")\n",
    "    print()\n",
    "    \n",
    "    # Step 5: Return grounded answer\n",
    "    print(f\"‚úÖ STEP 5: Grounded Answer\")\n",
    "    print(f\"   Answer is based on actual company data, not LLM's training!\")\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the workflow\n",
    "sample_question = \"Who is our CTO?\"\n",
    "result = visualize_rag_workflow(sample_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Exercise 4: Comparing Accuracy\n",
    "\n",
    "Let's quantify the difference between standard LLM and RAG responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test questions with known correct answers\n",
    "qa_pairs = [\n",
    "    {\n",
    "        \"question\": \"Who is the CEO of TechCorp Solutions?\",\n",
    "        \"correct_answer\": \"Sarah Johnson\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the uptime guarantee for Model-X?\",\n",
    "        \"correct_answer\": \"99.9%\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How many days is the return policy?\",\n",
    "        \"correct_answer\": \"30 days\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"When was the Series B funding secured?\",\n",
    "        \"correct_answer\": \"2024-02-10\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def check_answer_accuracy(response, correct_answer):\n",
    "    \"\"\"Simple accuracy check\"\"\"\n",
    "    return correct_answer.lower() in response.lower()\n",
    "\n",
    "print(\"üìä ACCURACY COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "standard_correct = 0\n",
    "rag_correct = 0\n",
    "\n",
    "for qa in qa_pairs:\n",
    "    question = qa[\"question\"]\n",
    "    correct = qa[\"correct_answer\"]\n",
    "    \n",
    "    print(f\"\\n‚ùì {question}\")\n",
    "    print(f\"‚úÖ Correct answer: {correct}\")\n",
    "    \n",
    "    # Standard LLM\n",
    "    standard_response = llm.invoke([HumanMessage(content=question)]).content\n",
    "    standard_accurate = check_answer_accuracy(standard_response, correct)\n",
    "    standard_correct += standard_accurate\n",
    "    \n",
    "    print(f\"ü§ñ Standard LLM: {standard_response[:100]}...\")\n",
    "    print(f\"   Accurate: {'‚úÖ' if standard_accurate else '‚ùå'}\")\n",
    "    \n",
    "    # RAG\n",
    "    rag_response = simple_rag_query(question, company_knowledge)\n",
    "    rag_accurate = check_answer_accuracy(rag_response, correct)\n",
    "    rag_correct += rag_accurate\n",
    "    \n",
    "    print(f\"üîç RAG LLM: {rag_response[:100]}...\")\n",
    "    print(f\"   Accurate: {'‚úÖ' if rag_accurate else '‚ùå'}\")\n",
    "\n",
    "print(f\"\\nüìä FINAL RESULTS:\")\n",
    "print(f\"Standard LLM Accuracy: {standard_correct}/{len(qa_pairs)} ({standard_correct/len(qa_pairs)*100:.1f}%)\")\n",
    "print(f\"RAG LLM Accuracy: {rag_correct}/{len(qa_pairs)} ({rag_correct/len(qa_pairs)*100:.1f}%)\")\n",
    "print(f\"\\nüéØ Improvement: {(rag_correct-standard_correct)/len(qa_pairs)*100:.1f} percentage points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### ‚ùå LLM Limitations We Observed:\n",
    "1. **Knowledge Cutoff**: Cannot answer questions about recent events\n",
    "2. **Hallucinations**: May provide confident but incorrect answers\n",
    "3. **No Domain Knowledge**: Cannot access company-specific information\n",
    "4. **Generic Responses**: Provides general answers without specific context\n",
    "\n",
    "### ‚úÖ RAG Benefits We Demonstrated:\n",
    "1. **Accurate Information**: Answers based on provided context\n",
    "2. **Domain-Specific**: Can access and use company knowledge\n",
    "3. **Grounded Responses**: Explicitly tells you when information isn't available\n",
    "4. **Improved Accuracy**: Significantly better performance on domain-specific questions\n",
    "\n",
    "### üîÑ RAG Workflow:\n",
    "1. **User Question** ‚Üí 2. **Retrieve Relevant Docs** ‚Üí 3. **Augment Prompt** ‚Üí 4. **Generate Response** ‚Üí 5. **Grounded Answer**\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In the next modules, we'll dive deeper into each component of the RAG system:\n",
    "- **Module 2**: How to load and process different types of documents\n",
    "- **Module 3**: Strategies for breaking documents into chunks\n",
    "- **Module 4**: Understanding embedding models for semantic search\n",
    "- And much more!\n",
    "\n",
    "The simple RAG we built here is just the beginning. Real-world RAG systems are much more sophisticated and powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. In which scenarios would you prefer a standard LLM over RAG?\n",
    "2. What types of company data would be most valuable to include in a RAG system?\n",
    "3. How might RAG help with compliance and audit requirements?\n",
    "4. What are potential challenges with implementing RAG in a large organization?\n",
    "\n",
    "## üìù Optional Exercise\n",
    "\n",
    "Try creating your own knowledge base for a fictional company or organization and test the RAG system with domain-specific questions!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}