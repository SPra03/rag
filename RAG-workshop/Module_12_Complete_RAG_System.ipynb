{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 12: Complete RAG System - Putting It All Together\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Build a complete, production-ready RAG system\n",
    "- Integrate all components from previous modules\n",
    "- Implement comprehensive evaluation and monitoring\n",
    "- Deploy a web interface for your RAG system\n",
    "- Apply best practices for scalability and maintenance\n",
    "\n",
    "## System Architecture Overview\n",
    "\n",
    "Our complete RAG system will include:\n",
    "\n",
    "```\n",
    "┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n",
    "│   Document      │    │    Processing    │    │   Vector Store  │\n",
    "│   Ingestion     │───▶│    Pipeline      │───▶│   & Indexing    │\n",
    "│                 │    │                  │    │                 │\n",
    "└─────────────────┘    └──────────────────┘    └─────────────────┘\n",
    "                                                         │\n",
    "┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n",
    "│   User Query    │    │   Multi-Model    │    │   Retrieval     │\n",
    "│   Interface     │───▶│   LLM Router     │◄───│   System        │\n",
    "│                 │    │                  │    │                 │\n",
    "└─────────────────┘    └──────────────────┘    └─────────────────┘\n",
    "         │                       │                       \n",
    "         │              ┌──────────────────┐             \n",
    "         └─────────────▶│   Monitoring &   │             \n",
    "                        │   Analytics      │             \n",
    "                        │                  │             \n",
    "                        └──────────────────┘             \n",
    "```\n",
    "\n",
    "### Key Components Integration\n",
    "- **Document Processing**: Smart chunking, metadata enrichment\n",
    "- **Embedding & Storage**: Optimized vector databases with hybrid search\n",
    "- **Retrieval**: Multi-stage retrieval with re-ranking\n",
    "- **Generation**: Multi-model routing with cost optimization\n",
    "- **Monitoring**: Comprehensive metrics and evaluation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages for the complete system\n",
    "!pip install langchain langchain-openai langchain-anthropic langchain-google-genai\n",
    "!pip install langchain-community tiktoken chromadb sentence-transformers\n",
    "!pip install openai anthropic google-generativeai\n",
    "!pip install streamlit gradio fastapi uvicorn\n",
    "!pip install pandas numpy matplotlib seaborn plotly\n",
    "!pip install python-dotenv pypdf unstructured\n",
    "!pip install rank-bm25 scikit-learn nltk\n",
    "!pip install asyncio aiohttp pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional, Tuple, Union\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import Document, HumanMessage, SystemMessage\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Additional utilities\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "\n",
    "# Environment setup\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Download NLTK data\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ All packages imported successfully!\")\n",
    "print(f\"📅 System initialized at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Complete RAG Pipeline Architecture\n",
    "\n",
    "Let's build the core architecture that integrates all our previous components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RAGConfig:\n",
    "    \"\"\"Configuration for the RAG system\"\"\"\n",
    "    # Document processing\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "    max_chunks_per_doc: int = 100\n",
    "    \n",
    "    # Embedding settings\n",
    "    embedding_model: str = \"text-embedding-3-small\"\n",
    "    embedding_dimension: int = 1536\n",
    "    \n",
    "    # Retrieval settings\n",
    "    retrieval_k: int = 10\n",
    "    rerank_k: int = 5\n",
    "    similarity_threshold: float = 0.7\n",
    "    \n",
    "    # Generation settings\n",
    "    default_model: str = \"gpt-3.5-turbo\"\n",
    "    max_context_length: int = 4000\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    # System settings\n",
    "    enable_caching: bool = True\n",
    "    cache_ttl: int = 3600\n",
    "    enable_monitoring: bool = True\n",
    "    max_retries: int = 3\n",
    "\n",
    "class CompleteRAGSystem:\n",
    "    \"\"\"Complete RAG system integrating all components\"\"\"\n",
    "    \n",
    "    def __init__(self, config: RAGConfig = None):\n",
    "        self.config = config or RAGConfig()\n",
    "        self.documents = []\n",
    "        self.vector_store = None\n",
    "        self.bm25_retriever = None\n",
    "        self.embedding_model = None\n",
    "        self.llm_models = {}\n",
    "        self.query_cache = {} if self.config.enable_caching else None\n",
    "        self.metrics = RAGMetrics()\n",
    "        \n",
    "        self._initialize_components()\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize all RAG components\"\"\"\n",
    "        logger.info(\"Initializing RAG system components...\")\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        try:\n",
    "            if \"openai\" in self.config.embedding_model:\n",
    "                self.embedding_model = OpenAIEmbeddings(\n",
    "                    model=self.config.embedding_model\n",
    "                )\n",
    "            else:\n",
    "                self.embedding_model = HuggingFaceEmbeddings(\n",
    "                    model_name=self.config.embedding_model\n",
    "                )\n",
    "            logger.info(f\"✅ Initialized embedding model: {self.config.embedding_model}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize embeddings: {e}\")\n",
    "            # Fallback to sentence transformers\n",
    "            self.embedding_model = HuggingFaceEmbeddings(\n",
    "                model_name=\"all-MiniLM-L6-v2\"\n",
    "            )\n",
    "        \n",
    "        # Initialize LLM models\n",
    "        self._initialize_llms()\n",
    "        \n",
    "        logger.info(\"🚀 RAG system initialized successfully\")\n",
    "    \n",
    "    def _initialize_llms(self):\n",
    "        \"\"\"Initialize available LLM models\"\"\"\n",
    "        # OpenAI models\n",
    "        if os.getenv(\"OPENAI_API_KEY\"):\n",
    "            try:\n",
    "                self.llm_models['gpt-4-turbo'] = ChatOpenAI(\n",
    "                    model=\"gpt-4-turbo-preview\", temperature=self.config.temperature\n",
    "                )\n",
    "                self.llm_models['gpt-3.5-turbo'] = ChatOpenAI(\n",
    "                    model=\"gpt-3.5-turbo\", temperature=self.config.temperature\n",
    "                )\n",
    "                logger.info(\"✅ OpenAI models initialized\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to initialize OpenAI models: {e}\")\n",
    "        \n",
    "        # Anthropic models\n",
    "        if os.getenv(\"ANTHROPIC_API_KEY\"):\n",
    "            try:\n",
    "                self.llm_models['claude-3-sonnet'] = ChatAnthropic(\n",
    "                    model=\"claude-3-sonnet-20240229\", temperature=self.config.temperature\n",
    "                )\n",
    "                logger.info(\"✅ Anthropic models initialized\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to initialize Anthropic models: {e}\")\n",
    "        \n",
    "        # Google models\n",
    "        if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "            try:\n",
    "                self.llm_models['gemini-pro'] = ChatGoogleGenerativeAI(\n",
    "                    model=\"gemini-pro\", temperature=self.config.temperature\n",
    "                )\n",
    "                logger.info(\"✅ Google models initialized\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to initialize Google models: {e}\")\n",
    "        \n",
    "        if not self.llm_models:\n",
    "            logger.error(\"❌ No LLM models available. Please set API keys.\")\n",
    "    \n",
    "    def ingest_documents(self, file_paths: List[str], \n",
    "                        metadata_extractors: Dict[str, callable] = None) -> Dict:\n",
    "        \"\"\"Ingest and process documents with smart chunking\"\"\"\n",
    "        logger.info(f\"📚 Ingesting {len(file_paths)} documents...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        processed_docs = []\n",
    "        failed_docs = []\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            try:\n",
    "                # Load document\n",
    "                docs = self._load_document(file_path)\n",
    "                \n",
    "                # Extract metadata\n",
    "                if metadata_extractors:\n",
    "                    docs = self._extract_metadata(docs, metadata_extractors)\n",
    "                \n",
    "                # Smart chunking based on document type\n",
    "                chunks = self._smart_chunk_document(docs, file_path)\n",
    "                \n",
    "                processed_docs.extend(chunks)\n",
    "                logger.info(f\"✅ Processed {file_path}: {len(chunks)} chunks\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"❌ Failed to process {file_path}: {e}\")\n",
    "                failed_docs.append({'file': file_path, 'error': str(e)})\n",
    "        \n",
    "        # Store processed documents\n",
    "        self.documents.extend(processed_docs)\n",
    "        \n",
    "        # Build vector store and BM25 index\n",
    "        self._build_indices(processed_docs)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            'total_documents': len(file_paths),\n",
    "            'processed_successfully': len(file_paths) - len(failed_docs),\n",
    "            'failed_documents': failed_docs,\n",
    "            'total_chunks': len(processed_docs),\n",
    "            'processing_time': processing_time,\n",
    "            'average_chunks_per_doc': len(processed_docs) / max(len(file_paths) - len(failed_docs), 1)\n",
    "        }\n",
    "    \n",
    "    def _load_document(self, file_path: str) -> List[Document]:\n",
    "        \"\"\"Load document based on file type\"\"\"\n",
    "        file_ext = Path(file_path).suffix.lower()\n",
    "        \n",
    "        if file_ext == '.pdf':\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif file_ext in ['.txt', '.md']:\n",
    "            loader = TextLoader(file_path)\n",
    "        elif file_ext in ['.html', '.htm']:\n",
    "            loader = UnstructuredHTMLLoader(file_path)\n",
    "        else:\n",
    "            # Fallback to text loader\n",
    "            loader = TextLoader(file_path)\n",
    "        \n",
    "        return loader.load()\n",
    "    \n",
    "    def _extract_metadata(self, docs: List[Document], \n",
    "                         extractors: Dict[str, callable]) -> List[Document]:\n",
    "        \"\"\"Extract metadata using provided extractors\"\"\"\n",
    "        for doc in docs:\n",
    "            for key, extractor in extractors.items():\n",
    "                try:\n",
    "                    doc.metadata[key] = extractor(doc.page_content)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Metadata extraction failed for {key}: {e}\")\n",
    "        return docs\n",
    "    \n",
    "    def _smart_chunk_document(self, docs: List[Document], file_path: str) -> List[Document]:\n",
    "        \"\"\"Apply smart chunking based on document characteristics\"\"\"\n",
    "        # Analyze document to determine optimal chunking strategy\n",
    "        total_length = sum(len(doc.page_content) for doc in docs)\n",
    "        avg_paragraph_length = self._estimate_paragraph_length(docs)\n",
    "        \n",
    "        # Adaptive chunk size based on document characteristics\n",
    "        if avg_paragraph_length > self.config.chunk_size:\n",
    "            chunk_size = min(avg_paragraph_length, self.config.chunk_size * 2)\n",
    "        else:\n",
    "            chunk_size = self.config.chunk_size\n",
    "        \n",
    "        # Choose splitter based on document type\n",
    "        if any(ext in file_path.lower() for ext in ['.py', '.js', '.java']):\n",
    "            # Code-aware splitting\n",
    "            separators = [\"\\n\\nclass \", \"\\n\\ndef \", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        else:\n",
    "            # General text splitting\n",
    "            separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "        \n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=self.config.chunk_overlap,\n",
    "            separators=separators\n",
    "        )\n",
    "        \n",
    "        chunks = splitter.split_documents(docs)\n",
    "        \n",
    "        # Limit chunks per document\n",
    "        if len(chunks) > self.config.max_chunks_per_doc:\n",
    "            chunks = chunks[:self.config.max_chunks_per_doc]\n",
    "            logger.warning(f\"Limited {file_path} to {self.config.max_chunks_per_doc} chunks\")\n",
    "        \n",
    "        # Enrich chunk metadata\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk.metadata.update({\n",
    "                'chunk_id': f\"{Path(file_path).stem}_{i}\",\n",
    "                'source_file': file_path,\n",
    "                'chunk_index': i,\n",
    "                'total_chunks': len(chunks),\n",
    "                'chunk_length': len(chunk.page_content),\n",
    "                'ingestion_timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def _estimate_paragraph_length(self, docs: List[Document]) -> int:\n",
    "        \"\"\"Estimate average paragraph length\"\"\"\n",
    "        paragraphs = []\n",
    "        for doc in docs:\n",
    "            paragraphs.extend([p.strip() for p in doc.page_content.split('\\n\\n') if p.strip()])\n",
    "        \n",
    "        if paragraphs:\n",
    "            return sum(len(p) for p in paragraphs) // len(paragraphs)\n",
    "        return self.config.chunk_size\n",
    "    \n",
    "    def _build_indices(self, docs: List[Document]):\n",
    "        \"\"\"Build vector store and BM25 index\"\"\"\n",
    "        if not docs:\n",
    "            logger.warning(\"No documents to index\")\n",
    "            return\n",
    "        \n",
    "        logger.info(\"🔍 Building vector store and search indices...\")\n",
    "        \n",
    "        try:\n",
    "            # Build vector store\n",
    "            self.vector_store = Chroma.from_documents(\n",
    "                documents=docs,\n",
    "                embedding=self.embedding_model,\n",
    "                collection_name=\"rag_collection\",\n",
    "                persist_directory=\"./chroma_db\"\n",
    "            )\n",
    "            \n",
    "            # Build BM25 index for lexical search\n",
    "            corpus = [doc.page_content.lower() for doc in docs]\n",
    "            tokenized_corpus = [doc.split() for doc in corpus]\n",
    "            self.bm25_retriever = BM25Okapi(tokenized_corpus)\n",
    "            \n",
    "            logger.info(f\"✅ Built indices for {len(docs)} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to build indices: {e}\")\n",
    "    \n",
    "    def query(self, question: str, user_context: Dict = None) -> Dict:\n",
    "        \"\"\"Process query through complete RAG pipeline\"\"\"\n",
    "        start_time = time.time()\n",
    "        query_id = f\"q_{int(start_time * 1000)}\"\n",
    "        \n",
    "        logger.info(f\"🔍 Processing query: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Check cache first\n",
    "            if self.query_cache and question in self.query_cache:\n",
    "                cached_result = self.query_cache[question]\n",
    "                if (time.time() - cached_result['timestamp']) < self.config.cache_ttl:\n",
    "                    logger.info(\"💾 Returning cached result\")\n",
    "                    return cached_result['result']\n",
    "            \n",
    "            # Multi-stage retrieval\n",
    "            retrieved_docs = self._multi_stage_retrieval(question)\n",
    "            \n",
    "            if not retrieved_docs:\n",
    "                return {\n",
    "                    'query_id': query_id,\n",
    "                    'success': False,\n",
    "                    'error': 'No relevant documents found',\n",
    "                    'processing_time': time.time() - start_time\n",
    "                }\n",
    "            \n",
    "            # Select optimal model\n",
    "            model_name = self._select_optimal_model(question, retrieved_docs)\n",
    "            \n",
    "            # Generate response\n",
    "            response = self._generate_response(\n",
    "                question, retrieved_docs, model_name, user_context\n",
    "            )\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            result = {\n",
    "                'query_id': query_id,\n",
    "                'success': True,\n",
    "                'answer': response['answer'],\n",
    "                'sources': response['sources'],\n",
    "                'model_used': model_name,\n",
    "                'retrieved_chunks': len(retrieved_docs),\n",
    "                'processing_time': processing_time,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'confidence_score': response.get('confidence', 0.0)\n",
    "            }\n",
    "            \n",
    "            # Cache result\n",
    "            if self.query_cache:\n",
    "                self.query_cache[question] = {\n",
    "                    'result': result,\n",
    "                    'timestamp': time.time()\n",
    "                }\n",
    "            \n",
    "            # Record metrics\n",
    "            if self.config.enable_monitoring:\n",
    "                self.metrics.record_query(result)\n",
    "            \n",
    "            logger.info(f\"✅ Query processed successfully in {processing_time:.2f}s\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_result = {\n",
    "                'query_id': query_id,\n",
    "                'success': False,\n",
    "                'error': str(e),\n",
    "                'processing_time': time.time() - start_time\n",
    "            }\n",
    "            \n",
    "            logger.error(f\"❌ Query failed: {e}\")\n",
    "            \n",
    "            if self.config.enable_monitoring:\n",
    "                self.metrics.record_query(error_result)\n",
    "            \n",
    "            return error_result\n",
    "    \n",
    "    def _multi_stage_retrieval(self, question: str) -> List[Document]:\n",
    "        \"\"\"Multi-stage retrieval with hybrid search and re-ranking\"\"\"\n",
    "        # Stage 1: Hybrid retrieval (semantic + lexical)\n",
    "        semantic_docs = self._semantic_search(question, k=self.config.retrieval_k)\n",
    "        lexical_docs = self._lexical_search(question, k=self.config.retrieval_k)\n",
    "        \n",
    "        # Stage 2: Fusion and deduplication\n",
    "        fused_docs = self._fusion_retrieval(semantic_docs, lexical_docs)\n",
    "        \n",
    "        # Stage 3: Re-ranking\n",
    "        reranked_docs = self._rerank_documents(question, fused_docs)\n",
    "        \n",
    "        return reranked_docs[:self.config.rerank_k]\n",
    "    \n",
    "    def _semantic_search(self, query: str, k: int) -> List[Document]:\n",
    "        \"\"\"Semantic search using vector similarity\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            return self.vector_store.similarity_search(\n",
    "                query, k=k, score_threshold=self.config.similarity_threshold\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Semantic search failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _lexical_search(self, query: str, k: int) -> List[Document]:\n",
    "        \"\"\"Lexical search using BM25\"\"\"\n",
    "        if not self.bm25_retriever:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            query_tokens = query.lower().split()\n",
    "            scores = self.bm25_retriever.get_scores(query_tokens)\n",
    "            top_indices = np.argsort(scores)[::-1][:k]\n",
    "            \n",
    "            return [self.documents[i] for i in top_indices if i < len(self.documents)]\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Lexical search failed: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _fusion_retrieval(self, semantic_docs: List[Document], \n",
    "                         lexical_docs: List[Document]) -> List[Document]:\n",
    "        \"\"\"Fuse results from different retrieval methods\"\"\"\n",
    "        # Simple fusion: combine and deduplicate based on content similarity\n",
    "        all_docs = semantic_docs + lexical_docs\n",
    "        \n",
    "        if not all_docs:\n",
    "            return []\n",
    "        \n",
    "        # Deduplicate based on content hash\n",
    "        seen_hashes = set()\n",
    "        unique_docs = []\n",
    "        \n",
    "        for doc in all_docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_hashes:\n",
    "                seen_hashes.add(content_hash)\n",
    "                unique_docs.append(doc)\n",
    "        \n",
    "        return unique_docs\n",
    "    \n",
    "    def _rerank_documents(self, query: str, documents: List[Document]) -> List[Document]:\n",
    "        \"\"\"Re-rank documents using advanced similarity metrics\"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "        \n",
    "        # Calculate relevance scores\n",
    "        scores = []\n",
    "        for doc in documents:\n",
    "            # Combine multiple relevance signals\n",
    "            lexical_score = self._calculate_lexical_similarity(query, doc.page_content)\n",
    "            length_penalty = min(len(doc.page_content) / 1000, 1.0)  # Prefer substantial chunks\n",
    "            freshness_bonus = self._calculate_freshness_score(doc)\n",
    "            \n",
    "            combined_score = lexical_score * 0.7 + length_penalty * 0.2 + freshness_bonus * 0.1\n",
    "            scores.append((combined_score, doc))\n",
    "        \n",
    "        # Sort by score (descending)\n",
    "        scores.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        return [doc for score, doc in scores]\n",
    "    \n",
    "    def _calculate_lexical_similarity(self, query: str, text: str) -> float:\n",
    "        \"\"\"Calculate lexical similarity between query and text\"\"\"\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(stop_words='english')\n",
    "            tfidf_matrix = vectorizer.fit_transform([query, text])\n",
    "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "            return similarity\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def _calculate_freshness_score(self, doc: Document) -> float:\n",
    "        \"\"\"Calculate freshness score based on document metadata\"\"\"\n",
    "        if 'ingestion_timestamp' in doc.metadata:\n",
    "            try:\n",
    "                ingestion_time = datetime.fromisoformat(doc.metadata['ingestion_timestamp'])\n",
    "                time_diff = datetime.now() - ingestion_time\n",
    "                days_old = time_diff.days\n",
    "                return max(0.0, 1.0 - (days_old / 30))  # Decay over 30 days\n",
    "            except:\n",
    "                return 0.5\n",
    "        return 0.5\n",
    "    \n",
    "    def _select_optimal_model(self, question: str, documents: List[Document]) -> str:\n",
    "        \"\"\"Select optimal model based on query characteristics\"\"\"\n",
    "        if not self.llm_models:\n",
    "            raise Exception(\"No LLM models available\")\n",
    "        \n",
    "        # Analyze query complexity\n",
    "        query_length = len(question.split())\n",
    "        context_length = sum(len(doc.page_content) for doc in documents)\n",
    "        \n",
    "        # Simple model selection heuristics\n",
    "        if context_length > 8000 or query_length > 20:\n",
    "            # Complex query, prefer premium models\n",
    "            preferred_models = ['gpt-4-turbo', 'claude-3-opus', 'claude-3-sonnet']\n",
    "        else:\n",
    "            # Simple query, cost-effective models\n",
    "            preferred_models = ['gpt-3.5-turbo', 'gemini-pro', 'claude-3-sonnet']\n",
    "        \n",
    "        # Select first available model from preferred list\n",
    "        for model in preferred_models:\n",
    "            if model in self.llm_models:\n",
    "                return model\n",
    "        \n",
    "        # Fallback to any available model\n",
    "        return list(self.llm_models.keys())[0]\n",
    "    \n",
    "    def _generate_response(self, question: str, documents: List[Document], \n",
    "                          model_name: str, user_context: Dict = None) -> Dict:\n",
    "        \"\"\"Generate response using selected model\"\"\"\n",
    "        # Prepare context\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        \n",
    "        for i, doc in enumerate(documents):\n",
    "            source_info = {\n",
    "                'chunk_id': doc.metadata.get('chunk_id', f'chunk_{i}'),\n",
    "                'source_file': doc.metadata.get('source_file', 'unknown'),\n",
    "                'content_preview': doc.page_content[:200] + '...' if len(doc.page_content) > 200 else doc.page_content\n",
    "            }\n",
    "            sources.append(source_info)\n",
    "            \n",
    "            context_parts.append(f\"Source {i+1}:\\n{doc.page_content}\")\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Truncate context if too long\n",
    "        if len(context) > self.config.max_context_length:\n",
    "            context = context[:self.config.max_context_length] + \"\\n\\n[Context truncated...]\"\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = self._create_rag_prompt(question, context, user_context)\n",
    "        \n",
    "        # Generate response\n",
    "        model = self.llm_models[model_name]\n",
    "        \n",
    "        try:\n",
    "            if 'gpt' in model_name:\n",
    "                with get_openai_callback() as cb:\n",
    "                    response = model.invoke([HumanMessage(content=prompt)])\n",
    "                    # Could use token counts for confidence scoring\n",
    "            else:\n",
    "                response = model.invoke([HumanMessage(content=prompt)])\n",
    "            \n",
    "            return {\n",
    "                'answer': response.content,\n",
    "                'sources': sources,\n",
    "                'confidence': self._estimate_confidence(response.content, context)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Response generation failed with {model_name}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _create_rag_prompt(self, question: str, context: str, user_context: Dict = None) -> str:\n",
    "        \"\"\"Create optimized RAG prompt\"\"\"\n",
    "        base_prompt = f\"\"\"\n",
    "You are a helpful AI assistant that provides accurate answers based on the given context.\n",
    "\n",
    "Instructions:\n",
    "- Use ONLY the information provided in the context to answer the question\n",
    "- If the context doesn't contain enough information, clearly state this\n",
    "- Provide specific citations by referencing \"Source X\" when possible\n",
    "- Be concise but comprehensive in your response\n",
    "- If you're uncertain about any information, express appropriate caveats\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        \n",
    "        # Add user context if provided\n",
    "        if user_context:\n",
    "            context_info = \"\\n\".join([f\"- {k}: {v}\" for k, v in user_context.items()])\n",
    "            base_prompt = f\"User Context:\\n{context_info}\\n\\n{base_prompt}\"\n",
    "        \n",
    "        return base_prompt\n",
    "    \n",
    "    def _estimate_confidence(self, answer: str, context: str) -> float:\n",
    "        \"\"\"Estimate confidence based on answer characteristics\"\"\"\n",
    "        # Simple heuristics for confidence estimation\n",
    "        confidence = 0.5  # Base confidence\n",
    "        \n",
    "        # Check for uncertainty indicators\n",
    "        uncertainty_phrases = ['not sure', 'unclear', 'might be', 'possibly', 'don\\'t know']\n",
    "        if any(phrase in answer.lower() for phrase in uncertainty_phrases):\n",
    "            confidence -= 0.2\n",
    "        \n",
    "        # Check for citation patterns\n",
    "        if 'source' in answer.lower() and ('according to' in answer.lower() or 'states' in answer.lower()):\n",
    "            confidence += 0.2\n",
    "        \n",
    "        # Check answer length relative to context\n",
    "        if len(answer) > 50 and len(answer) < len(context) * 0.5:\n",
    "            confidence += 0.1\n",
    "        \n",
    "        return max(0.0, min(1.0, confidence))\n",
    "    \n",
    "    def get_system_stats(self) -> Dict:\n",
    "        \"\"\"Get comprehensive system statistics\"\"\"\n",
    "        return {\n",
    "            'documents_indexed': len(self.documents),\n",
    "            'vector_store_ready': self.vector_store is not None,\n",
    "            'bm25_ready': self.bm25_retriever is not None,\n",
    "            'available_models': list(self.llm_models.keys()),\n",
    "            'cache_size': len(self.query_cache) if self.query_cache else 0,\n",
    "            'total_queries': self.metrics.get_total_queries() if self.config.enable_monitoring else 0,\n",
    "            'avg_response_time': self.metrics.get_avg_response_time() if self.config.enable_monitoring else 0,\n",
    "            'success_rate': self.metrics.get_success_rate() if self.config.enable_monitoring else 0\n",
    "        }\n",
    "\n",
    "class RAGMetrics:\n",
    "    \"\"\"Comprehensive metrics tracking for RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.queries = []\n",
    "        self.model_usage = defaultdict(int)\n",
    "        self.error_counts = defaultdict(int)\n",
    "    \n",
    "    def record_query(self, result: Dict):\n",
    "        \"\"\"Record query result for metrics\"\"\"\n",
    "        self.queries.append({\n",
    "            **result,\n",
    "            'recorded_at': datetime.now()\n",
    "        })\n",
    "        \n",
    "        if result.get('success'):\n",
    "            model_used = result.get('model_used')\n",
    "            if model_used:\n",
    "                self.model_usage[model_used] += 1\n",
    "        else:\n",
    "            error = result.get('error', 'unknown_error')\n",
    "            self.error_counts[error] += 1\n",
    "    \n",
    "    def get_total_queries(self) -> int:\n",
    "        return len(self.queries)\n",
    "    \n",
    "    def get_success_rate(self) -> float:\n",
    "        if not self.queries:\n",
    "            return 0.0\n",
    "        successful = sum(1 for q in self.queries if q.get('success', False))\n",
    "        return successful / len(self.queries)\n",
    "    \n",
    "    def get_avg_response_time(self) -> float:\n",
    "        successful_queries = [q for q in self.queries if q.get('success', False)]\n",
    "        if not successful_queries:\n",
    "            return 0.0\n",
    "        \n",
    "        total_time = sum(q.get('processing_time', 0) for q in successful_queries)\n",
    "        return total_time / len(successful_queries)\n",
    "    \n",
    "    def get_model_usage_stats(self) -> Dict:\n",
    "        return dict(self.model_usage)\n",
    "    \n",
    "    def get_error_stats(self) -> Dict:\n",
    "        return dict(self.error_counts)\n",
    "\n",
    "print(\"✅ Complete RAG System architecture defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Document Ingestion and System Initialization\n",
    "\n",
    "Let's create sample documents and initialize our complete RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for testing\n",
    "sample_docs_dir = Path(\"./sample_documents\")\n",
    "sample_docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Document 1: AI and Machine Learning Overview\n",
    "ai_doc = \"\"\"\n",
    "# Artificial Intelligence and Machine Learning: A Comprehensive Guide\n",
    "\n",
    "## Introduction to AI\n",
    "Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans. The term may also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
    "\n",
    "## Machine Learning Fundamentals\n",
    "Machine Learning (ML) is a subset of AI that focuses on the development of computer programs that can access data and use it to learn for themselves. The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide.\n",
    "\n",
    "### Types of Machine Learning\n",
    "1. **Supervised Learning**: Learning with labeled data\n",
    "2. **Unsupervised Learning**: Finding patterns in data without labels\n",
    "3. **Reinforcement Learning**: Learning through interaction with environment\n",
    "\n",
    "## Deep Learning\n",
    "Deep Learning is a subset of machine learning that uses neural networks with three or more layers. These neural networks attempt to simulate the behavior of the human brain, though far from matching its ability, allowing it to \"learn\" from large amounts of data.\n",
    "\n",
    "### Applications of Deep Learning\n",
    "- Natural Language Processing (NLP)\n",
    "- Computer Vision\n",
    "- Speech Recognition\n",
    "- Autonomous Vehicles\n",
    "- Medical Diagnosis\n",
    "\n",
    "## Ethical Considerations\n",
    "As AI becomes more prevalent, it's crucial to consider the ethical implications:\n",
    "- Bias in AI systems\n",
    "- Privacy concerns\n",
    "- Job displacement\n",
    "- Transparency and explainability\n",
    "- Accountability\n",
    "\n",
    "## Future of AI\n",
    "The future of AI holds immense potential for transforming industries and society. Key areas of development include:\n",
    "- Artificial General Intelligence (AGI)\n",
    "- Quantum AI\n",
    "- AI in healthcare\n",
    "- AI in education\n",
    "- AI in climate change mitigation\n",
    "\n",
    "Artificial Intelligence and Machine Learning continue to evolve rapidly, with new breakthroughs happening regularly. Understanding these technologies is crucial for anyone looking to stay current in the modern technological landscape.\n",
    "\"\"\"\n",
    "\n",
    "# Document 2: Sustainable Energy Solutions\n",
    "energy_doc = \"\"\"\n",
    "# Sustainable Energy Solutions: Powering a Greener Future\n",
    "\n",
    "## Introduction\n",
    "Sustainable energy refers to energy that meets our current needs without compromising the ability of future generations to meet their needs. It encompasses renewable energy sources and energy efficiency measures that minimize environmental impact.\n",
    "\n",
    "## Renewable Energy Sources\n",
    "\n",
    "### Solar Energy\n",
    "Solar energy harnesses the power of the sun through photovoltaic (PV) panels or solar thermal systems. Benefits include:\n",
    "- Abundant and inexhaustible resource\n",
    "- No greenhouse gas emissions during operation\n",
    "- Declining costs and improving efficiency\n",
    "- Scalable from residential to utility scale\n",
    "\n",
    "### Wind Energy\n",
    "Wind energy converts the kinetic energy of moving air into electricity using wind turbines. Key advantages:\n",
    "- One of the fastest-growing energy sources globally\n",
    "- Cost-competitive with fossil fuels\n",
    "- Land can still be used for other purposes (farming)\n",
    "- Creates jobs in manufacturing and maintenance\n",
    "\n",
    "### Hydroelectric Power\n",
    "Hydroelectric power generates electricity by harnessing the energy of flowing or falling water. Characteristics:\n",
    "- Reliable and consistent energy source\n",
    "- Long lifespan of infrastructure\n",
    "- Can provide flood control and water storage\n",
    "- Environmental considerations for fish migration and ecosystems\n",
    "\n",
    "### Geothermal Energy\n",
    "Geothermal energy utilizes heat from the Earth's core to generate electricity or provide direct heating:\n",
    "- Available 24/7 regardless of weather conditions\n",
    "- Small land footprint\n",
    "- Low operational costs after installation\n",
    "- Limited to areas with geothermal activity\n",
    "\n",
    "## Energy Storage Solutions\n",
    "Energy storage is crucial for renewable energy integration:\n",
    "\n",
    "### Battery Technology\n",
    "- Lithium-ion batteries for grid-scale storage\n",
    "- Emerging technologies like solid-state batteries\n",
    "- Cost reductions and efficiency improvements\n",
    "\n",
    "### Pumped Hydro Storage\n",
    "- Most mature grid-scale storage technology\n",
    "- Uses excess energy to pump water uphill\n",
    "- Generates electricity when water flows back down\n",
    "\n",
    "### Hydrogen Storage\n",
    "- Green hydrogen produced from renewable electricity\n",
    "- Long-term storage capabilities\n",
    "- Applications in transportation and industry\n",
    "\n",
    "## Smart Grid Technology\n",
    "Smart grids integrate digital technology with electrical infrastructure:\n",
    "- Real-time monitoring and control\n",
    "- Demand response programs\n",
    "- Integration of distributed energy resources\n",
    "- Improved reliability and efficiency\n",
    "\n",
    "## Challenges and Solutions\n",
    "\n",
    "### Intermittency\n",
    "Challenge: Solar and wind energy are variable\n",
    "Solutions: Energy storage, grid flexibility, diverse renewable portfolio\n",
    "\n",
    "### Grid Integration\n",
    "Challenge: Integrating variable renewable sources\n",
    "Solutions: Smart grids, demand response, improved forecasting\n",
    "\n",
    "### Economic Considerations\n",
    "Challenge: Initial investment costs\n",
    "Solutions: Government incentives, improving economics, green financing\n",
    "\n",
    "## Global Impact and Future Outlook\n",
    "The transition to sustainable energy is essential for:\n",
    "- Combating climate change\n",
    "- Energy security and independence\n",
    "- Economic development and job creation\n",
    "- Public health improvements\n",
    "\n",
    "The future of sustainable energy looks promising with continued technological advancement, cost reductions, and increasing political and social support for clean energy transition.\n",
    "\"\"\"\n",
    "\n",
    "# Document 3: Data Science Best Practices\n",
    "datascience_doc = \"\"\"\n",
    "# Data Science Best Practices: A Comprehensive Guide\n",
    "\n",
    "## Introduction to Data Science\n",
    "Data Science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data. It combines domain expertise, programming skills, and knowledge of mathematics and statistics.\n",
    "\n",
    "## The Data Science Lifecycle\n",
    "\n",
    "### 1. Problem Definition\n",
    "- Clearly define business objectives\n",
    "- Identify success metrics\n",
    "- Understand stakeholder requirements\n",
    "- Assess feasibility and constraints\n",
    "\n",
    "### 2. Data Collection\n",
    "- Identify relevant data sources\n",
    "- Ensure data quality and integrity\n",
    "- Consider privacy and ethical implications\n",
    "- Document data lineage and metadata\n",
    "\n",
    "### 3. Data Exploration and Analysis\n",
    "- Perform exploratory data analysis (EDA)\n",
    "- Understand data distributions and patterns\n",
    "- Identify outliers and anomalies\n",
    "- Generate initial hypotheses\n",
    "\n",
    "### 4. Data Preprocessing\n",
    "- Handle missing data appropriately\n",
    "- Normalize and standardize features\n",
    "- Engineer relevant features\n",
    "- Address data quality issues\n",
    "\n",
    "### 5. Model Development\n",
    "- Select appropriate algorithms\n",
    "- Split data into train/validation/test sets\n",
    "- Perform cross-validation\n",
    "- Tune hyperparameters systematically\n",
    "\n",
    "### 6. Model Evaluation\n",
    "- Use appropriate evaluation metrics\n",
    "- Assess model performance on unseen data\n",
    "- Check for overfitting and underfitting\n",
    "- Validate business impact\n",
    "\n",
    "### 7. Deployment and Monitoring\n",
    "- Deploy models in production environment\n",
    "- Monitor model performance continuously\n",
    "- Implement feedback loops\n",
    "- Plan for model updates and maintenance\n",
    "\n",
    "## Best Practices for Data Scientists\n",
    "\n",
    "### Code Quality and Documentation\n",
    "- Write clean, readable, and modular code\n",
    "- Use version control (Git) effectively\n",
    "- Document code and methodology thoroughly\n",
    "- Follow coding standards and conventions\n",
    "\n",
    "### Reproducibility\n",
    "- Set random seeds for reproducible results\n",
    "- Use virtual environments and dependency management\n",
    "- Document computational environment\n",
    "- Maintain experiment tracking and logging\n",
    "\n",
    "### Collaboration and Communication\n",
    "- Create clear and compelling visualizations\n",
    "- Communicate findings to non-technical stakeholders\n",
    "- Collaborate effectively with cross-functional teams\n",
    "- Share knowledge and best practices\n",
    "\n",
    "### Ethical Considerations\n",
    "- Ensure data privacy and security\n",
    "- Address bias in data and algorithms\n",
    "- Consider societal impact of models\n",
    "- Maintain transparency and accountability\n",
    "\n",
    "## Common Pitfalls and How to Avoid Them\n",
    "\n",
    "### Data Leakage\n",
    "Problem: Using future information to predict past events\n",
    "Solution: Carefully design train/test splits and feature engineering\n",
    "\n",
    "### Overfitting\n",
    "Problem: Model performs well on training data but poorly on new data\n",
    "Solution: Use cross-validation, regularization, and simpler models\n",
    "\n",
    "### Selection Bias\n",
    "Problem: Training data not representative of target population\n",
    "Solution: Carefully design sampling strategies and validate assumptions\n",
    "\n",
    "### Ignoring Domain Knowledge\n",
    "Problem: Purely data-driven approach without subject matter expertise\n",
    "Solution: Collaborate with domain experts and validate results\n",
    "\n",
    "## Tools and Technologies\n",
    "\n",
    "### Programming Languages\n",
    "- Python: Versatile with rich ecosystem (pandas, scikit-learn, TensorFlow)\n",
    "- R: Statistical computing and data visualization\n",
    "- SQL: Database querying and data manipulation\n",
    "- Scala/Java: Big data processing with Spark\n",
    "\n",
    "### Data Processing and Storage\n",
    "- Apache Spark: Large-scale data processing\n",
    "- Hadoop: Distributed storage and computing\n",
    "- Cloud platforms: AWS, GCP, Azure\n",
    "- Databases: PostgreSQL, MongoDB, Cassandra\n",
    "\n",
    "### Machine Learning Frameworks\n",
    "- Scikit-learn: General-purpose machine learning\n",
    "- TensorFlow/Keras: Deep learning\n",
    "- PyTorch: Research-oriented deep learning\n",
    "- XGBoost: Gradient boosting\n",
    "\n",
    "### Visualization and BI Tools\n",
    "- Matplotlib/Seaborn: Python visualization\n",
    "- ggplot2: R visualization\n",
    "- Tableau: Business intelligence\n",
    "- Power BI: Microsoft's BI solution\n",
    "\n",
    "## Future Trends in Data Science\n",
    "- Automated Machine Learning (AutoML)\n",
    "- Explainable AI and interpretable models\n",
    "- Edge computing and real-time analytics\n",
    "- Integration with IoT and streaming data\n",
    "- Quantum computing applications\n",
    "\n",
    "Data Science continues to evolve rapidly with new tools, techniques, and applications emerging regularly. Staying current with best practices and emerging trends is essential for success in this dynamic field.\n",
    "\"\"\"\n",
    "\n",
    "# Write sample documents to files\n",
    "documents = {\n",
    "    \"ai_machine_learning.txt\": ai_doc,\n",
    "    \"sustainable_energy.txt\": energy_doc,\n",
    "    \"data_science_best_practices.txt\": datascience_doc\n",
    "}\n",
    "\n",
    "file_paths = []\n",
    "for filename, content in documents.items():\n",
    "    file_path = sample_docs_dir / filename\n",
    "    file_path.write_text(content.strip())\n",
    "    file_paths.append(str(file_path))\n",
    "\n",
    "print(f\"📝 Created {len(documents)} sample documents:\")\n",
    "for path in file_paths:\n",
    "    size = len(Path(path).read_text())\n",
    "    print(f\"   - {Path(path).name}: {size} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the complete RAG system\n",
    "print(\"🚀 Initializing Complete RAG System...\")\n",
    "\n",
    "# Create custom configuration\n",
    "rag_config = RAGConfig(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=150,\n",
    "    retrieval_k=8,\n",
    "    rerank_k=4,\n",
    "    similarity_threshold=0.6,\n",
    "    max_context_length=6000,\n",
    "    enable_caching=True,\n",
    "    enable_monitoring=True\n",
    ")\n",
    "\n",
    "# Initialize RAG system\n",
    "rag_system = CompleteRAGSystem(rag_config)\n",
    "\n",
    "print(f\"✅ RAG System initialized with config:\")\n",
    "print(f\"   - Chunk size: {rag_config.chunk_size}\")\n",
    "print(f\"   - Retrieval k: {rag_config.retrieval_k}\")\n",
    "print(f\"   - Available models: {list(rag_system.llm_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metadata extractors\n",
    "def extract_document_type(content: str) -> str:\n",
    "    \"\"\"Extract document type based on content patterns\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    if any(term in content_lower for term in ['machine learning', 'artificial intelligence', 'deep learning']):\n",
    "        return 'AI/ML'\n",
    "    elif any(term in content_lower for term in ['energy', 'renewable', 'solar', 'wind']):\n",
    "        return 'Energy'\n",
    "    elif any(term in content_lower for term in ['data science', 'statistics', 'analytics']):\n",
    "        return 'Data Science'\n",
    "    else:\n",
    "        return 'General'\n",
    "\n",
    "def extract_complexity_score(content: str) -> float:\n",
    "    \"\"\"Estimate content complexity based on various factors\"\"\"\n",
    "    # Simple heuristics for complexity\n",
    "    technical_terms = ['algorithm', 'methodology', 'framework', 'optimization', 'analysis']\n",
    "    complexity_indicators = ['complex', 'advanced', 'sophisticated', 'comprehensive']\n",
    "    \n",
    "    tech_count = sum(1 for term in technical_terms if term in content.lower())\n",
    "    complexity_count = sum(1 for term in complexity_indicators if term in content.lower())\n",
    "    \n",
    "    # Normalize to 0-1 scale\n",
    "    return min(1.0, (tech_count + complexity_count) / 10)\n",
    "\n",
    "def extract_key_topics(content: str) -> List[str]:\n",
    "    \"\"\"Extract key topics from content\"\"\"\n",
    "    # Simple keyword extraction (in production, use more sophisticated NLP)\n",
    "    topic_keywords = {\n",
    "        'Machine Learning': ['machine learning', 'supervised', 'unsupervised', 'neural network'],\n",
    "        'AI Ethics': ['ethical', 'bias', 'fairness', 'accountability'],\n",
    "        'Data Processing': ['data', 'preprocessing', 'cleaning', 'transformation'],\n",
    "        'Renewable Energy': ['solar', 'wind', 'renewable', 'sustainable'],\n",
    "        'Energy Storage': ['battery', 'storage', 'grid', 'hydrogen'],\n",
    "        'Best Practices': ['best practices', 'methodology', 'workflow', 'process']\n",
    "    }\n",
    "    \n",
    "    found_topics = []\n",
    "    content_lower = content.lower()\n",
    "    \n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        if any(keyword in content_lower for keyword in keywords):\n",
    "            found_topics.append(topic)\n",
    "    \n",
    "    return found_topics\n",
    "\n",
    "# Metadata extractors dictionary\n",
    "metadata_extractors = {\n",
    "    'document_type': extract_document_type,\n",
    "    'complexity_score': extract_complexity_score,\n",
    "    'key_topics': extract_key_topics\n",
    "}\n",
    "\n",
    "print(\"🔧 Defined metadata extractors for enhanced document processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest documents into the RAG system\n",
    "print(\"📚 Ingesting documents into RAG system...\")\n",
    "\n",
    "ingestion_result = rag_system.ingest_documents(\n",
    "    file_paths=file_paths,\n",
    "    metadata_extractors=metadata_extractors\n",
    ")\n",
    "\n",
    "print(\"\\n📊 Ingestion Results:\")\n",
    "for key, value in ingestion_result.items():\n",
    "    if key == 'failed_documents' and value:\n",
    "        print(f\"   ❌ {key}: {len(value)} failures\")\n",
    "        for failure in value:\n",
    "            print(f\"      - {failure['file']}: {failure['error']}\")\n",
    "    else:\n",
    "        print(f\"   ✅ {key}: {value}\")\n",
    "\n",
    "# Display system stats\n",
    "print(\"\\n🔍 System Statistics:\")\n",
    "stats = rag_system.get_system_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"   - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Comprehensive Query Testing\n",
    "\n",
    "Let's test our complete RAG system with various types of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive test queries\n",
    "test_queries = [\n",
    "    {\n",
    "        'query': 'What is machine learning and what are its main types?',\n",
    "        'category': 'Factual',\n",
    "        'expected_sources': ['AI/ML'],\n",
    "        'user_context': {'expertise_level': 'beginner'}\n",
    "    },\n",
    "    {\n",
    "        'query': 'Compare the advantages and challenges of solar energy versus wind energy for large-scale deployment.',\n",
    "        'category': 'Analytical',\n",
    "        'expected_sources': ['Energy'],\n",
    "        'user_context': {'expertise_level': 'intermediate', 'focus': 'renewable energy'}\n",
    "    },\n",
    "    {\n",
    "        'query': 'What are the best practices for avoiding overfitting in machine learning models?',\n",
    "        'category': 'Technical',\n",
    "        'expected_sources': ['Data Science', 'AI/ML'],\n",
    "        'user_context': {'expertise_level': 'advanced', 'role': 'data scientist'}\n",
    "    },\n",
    "    {\n",
    "        'query': 'How can energy storage solutions help with renewable energy integration?',\n",
    "        'category': 'Complex',\n",
    "        'expected_sources': ['Energy'],\n",
    "        'user_context': {'expertise_level': 'intermediate'}\n",
    "    },\n",
    "    {\n",
    "        'query': 'What ethical considerations should be addressed when developing AI systems?',\n",
    "        'category': 'Ethical',\n",
    "        'expected_sources': ['AI/ML'],\n",
    "        'user_context': {'expertise_level': 'intermediate', 'focus': 'ethics'}\n",
    "    },\n",
    "    {\n",
    "        'query': 'Explain the data science lifecycle and key steps involved.',\n",
    "        'category': 'Process',\n",
    "        'expected_sources': ['Data Science'],\n",
    "        'user_context': {'expertise_level': 'beginner', 'role': 'student'}\n",
    "    },\n",
    "    {\n",
    "        'query': 'What is quantum computing and how might it impact AI?',\n",
    "        'category': 'Edge Case',\n",
    "        'expected_sources': None,  # Should indicate insufficient information\n",
    "        'user_context': {'expertise_level': 'advanced'}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"🧪 Prepared {len(test_queries)} test queries across different categories\")\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(f\"   {i}. [{query['category']}] {query['query'][:50]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute comprehensive query testing\n",
    "print(\"🔬 Running comprehensive query tests...\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for i, test_query in enumerate(test_queries, 1):\n",
    "    print(f\"🔍 Test {i}: {test_query['category']} Query\")\n",
    "    print(f\"   Question: {test_query['query']}\")\n",
    "    \n",
    "    # Execute query\n",
    "    result = rag_system.query(\n",
    "        question=test_query['query'],\n",
    "        user_context=test_query.get('user_context')\n",
    "    )\n",
    "    \n",
    "    # Store result with test metadata\n",
    "    test_result = {\n",
    "        **result,\n",
    "        'test_category': test_query['category'],\n",
    "        'expected_sources': test_query.get('expected_sources'),\n",
    "        'original_query': test_query['query']\n",
    "    }\n",
    "    test_results.append(test_result)\n",
    "    \n",
    "    # Display results\n",
    "    if result['success']:\n",
    "        print(f\"   ✅ Success | Model: {result['model_used']} | Time: {result['processing_time']:.2f}s\")\n",
    "        print(f\"   📊 Confidence: {result['confidence_score']:.2f} | Sources: {result['retrieved_chunks']}\")\n",
    "        print(f\"   💬 Answer: {result['answer'][:150]}...\")\n",
    "        \n",
    "        # Show source information\n",
    "        if result['sources']:\n",
    "            print(f\"   📚 Sources:\")\n",
    "            for j, source in enumerate(result['sources'][:2], 1):  # Show first 2 sources\n",
    "                source_file = Path(source['source_file']).stem\n",
    "                print(f\"      {j}. {source_file} ({source['chunk_id']})\")\n",
    "    else:\n",
    "        print(f\"   ❌ Failed: {result['error']}\")\n",
    "    \n",
    "    print(\"   \" + \"-\"*60 + \"\\n\")\n",
    "\n",
    "print(f\"🎯 Completed {len(test_results)} query tests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: System Evaluation and Analytics\n",
    "\n",
    "Let's analyze the performance of our complete RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for RAG systems\"\"\"\n",
    "    \n",
    "    def __init__(self, test_results: List[Dict]):\n",
    "        self.test_results = test_results\n",
    "        self.successful_results = [r for r in test_results if r.get('success', False)]\n",
    "    \n",
    "    def calculate_performance_metrics(self) -> Dict:\n",
    "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
    "        if not self.test_results:\n",
    "            return {}\n",
    "        \n",
    "        total_queries = len(self.test_results)\n",
    "        successful_queries = len(self.successful_results)\n",
    "        \n",
    "        # Basic metrics\n",
    "        success_rate = successful_queries / total_queries if total_queries > 0 else 0\n",
    "        \n",
    "        if self.successful_results:\n",
    "            avg_response_time = sum(r['processing_time'] for r in self.successful_results) / len(self.successful_results)\n",
    "            avg_confidence = sum(r['confidence_score'] for r in self.successful_results) / len(self.successful_results)\n",
    "            avg_retrieved_chunks = sum(r['retrieved_chunks'] for r in self.successful_results) / len(self.successful_results)\n",
    "        else:\n",
    "            avg_response_time = 0\n",
    "            avg_confidence = 0\n",
    "            avg_retrieved_chunks = 0\n",
    "        \n",
    "        # Model usage distribution\n",
    "        model_usage = {}\n",
    "        for result in self.successful_results:\n",
    "            model = result.get('model_used', 'unknown')\n",
    "            model_usage[model] = model_usage.get(model, 0) + 1\n",
    "        \n",
    "        # Category performance\n",
    "        category_performance = {}\n",
    "        for result in self.test_results:\n",
    "            category = result.get('test_category', 'unknown')\n",
    "            if category not in category_performance:\n",
    "                category_performance[category] = {'total': 0, 'successful': 0}\n",
    "            \n",
    "            category_performance[category]['total'] += 1\n",
    "            if result.get('success', False):\n",
    "                category_performance[category]['successful'] += 1\n",
    "        \n",
    "        # Calculate success rates by category\n",
    "        for category in category_performance:\n",
    "            stats = category_performance[category]\n",
    "            stats['success_rate'] = stats['successful'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'overall': {\n",
    "                'total_queries': total_queries,\n",
    "                'successful_queries': successful_queries,\n",
    "                'success_rate': success_rate,\n",
    "                'avg_response_time': avg_response_time,\n",
    "                'avg_confidence_score': avg_confidence,\n",
    "                'avg_retrieved_chunks': avg_retrieved_chunks\n",
    "            },\n",
    "            'model_usage': model_usage,\n",
    "            'category_performance': category_performance\n",
    "        }\n",
    "    \n",
    "    def generate_detailed_analysis(self) -> Dict:\n",
    "        \"\"\"Generate detailed analysis of system performance\"\"\"\n",
    "        analysis = {\n",
    "            'strengths': [],\n",
    "            'weaknesses': [],\n",
    "            'recommendations': []\n",
    "        }\n",
    "        \n",
    "        metrics = self.calculate_performance_metrics()\n",
    "        \n",
    "        if not metrics:\n",
    "            return analysis\n",
    "        \n",
    "        overall = metrics['overall']\n",
    "        \n",
    "        # Identify strengths\n",
    "        if overall['success_rate'] >= 0.8:\n",
    "            analysis['strengths'].append(f\"High success rate ({overall['success_rate']:.1%})\")\n",
    "        \n",
    "        if overall['avg_response_time'] <= 3.0:\n",
    "            analysis['strengths'].append(f\"Fast response times (avg {overall['avg_response_time']:.1f}s)\")\n",
    "        \n",
    "        if overall['avg_confidence_score'] >= 0.7:\n",
    "            analysis['strengths'].append(f\"High confidence scores (avg {overall['avg_confidence_score']:.2f})\")\n",
    "        \n",
    "        # Identify weaknesses\n",
    "        if overall['success_rate'] < 0.7:\n",
    "            analysis['weaknesses'].append(f\"Low success rate ({overall['success_rate']:.1%})\")\n",
    "        \n",
    "        if overall['avg_response_time'] > 5.0:\n",
    "            analysis['weaknesses'].append(f\"Slow response times (avg {overall['avg_response_time']:.1f}s)\")\n",
    "        \n",
    "        if overall['avg_confidence_score'] < 0.5:\n",
    "            analysis['weaknesses'].append(f\"Low confidence scores (avg {overall['avg_confidence_score']:.2f})\")\n",
    "        \n",
    "        # Category-specific analysis\n",
    "        category_perf = metrics.get('category_performance', {})\n",
    "        for category, stats in category_perf.items():\n",
    "            if stats['success_rate'] < 0.7:\n",
    "                analysis['weaknesses'].append(f\"Poor performance on {category} queries ({stats['success_rate']:.1%} success)\")\n",
    "        \n",
    "        # Generate recommendations\n",
    "        if overall['avg_response_time'] > 3.0:\n",
    "            analysis['recommendations'].append(\"Consider optimizing retrieval pipeline or using faster models for simple queries\")\n",
    "        \n",
    "        if overall['avg_confidence_score'] < 0.6:\n",
    "            analysis['recommendations'].append(\"Improve confidence estimation or add more relevant documents to knowledge base\")\n",
    "        \n",
    "        if overall['success_rate'] < 0.8:\n",
    "            analysis['recommendations'].append(\"Review failed queries and improve error handling or fallback mechanisms\")\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def create_performance_report(self) -> str:\n",
    "        \"\"\"Create a comprehensive performance report\"\"\"\n",
    "        metrics = self.calculate_performance_metrics()\n",
    "        analysis = self.generate_detailed_analysis()\n",
    "        \n",
    "        if not metrics:\n",
    "            return \"No test results available for evaluation.\"\n",
    "        \n",
    "        report = f\"\"\"\n",
    "# RAG System Performance Report\n",
    "\n",
    "## Overall Performance Metrics\n",
    "- **Total Queries**: {metrics['overall']['total_queries']}\n",
    "- **Success Rate**: {metrics['overall']['success_rate']:.1%}\n",
    "- **Average Response Time**: {metrics['overall']['avg_response_time']:.2f} seconds\n",
    "- **Average Confidence Score**: {metrics['overall']['avg_confidence_score']:.2f}\n",
    "- **Average Retrieved Chunks**: {metrics['overall']['avg_retrieved_chunks']:.1f}\n",
    "\n",
    "## Model Usage Distribution\n",
    "\"\"\"\n",
    "        \n",
    "        for model, count in metrics['model_usage'].items():\n",
    "            percentage = (count / metrics['overall']['successful_queries']) * 100\n",
    "            report += f\"- **{model}**: {count} queries ({percentage:.1f}%)\\n\"\n",
    "        \n",
    "        report += \"\\n## Performance by Query Category\\n\"\n",
    "        for category, stats in metrics['category_performance'].items():\n",
    "            report += f\"- **{category}**: {stats['successful']}/{stats['total']} ({stats['success_rate']:.1%})\\n\"\n",
    "        \n",
    "        report += \"\\n## System Analysis\\n\\n### Strengths\\n\"\n",
    "        for strength in analysis['strengths']:\n",
    "            report += f\"- {strength}\\n\"\n",
    "        \n",
    "        if analysis['weaknesses']:\n",
    "            report += \"\\n### Areas for Improvement\\n\"\n",
    "            for weakness in analysis['weaknesses']:\n",
    "                report += f\"- {weakness}\\n\"\n",
    "        \n",
    "        if analysis['recommendations']:\n",
    "            report += \"\\n### Recommendations\\n\"\n",
    "            for recommendation in analysis['recommendations']:\n",
    "                report += f\"- {recommendation}\\n\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Create evaluator and generate report\n",
    "evaluator = RAGEvaluator(test_results)\n",
    "performance_metrics = evaluator.calculate_performance_metrics()\n",
    "performance_report = evaluator.create_performance_report()\n",
    "\n",
    "print(\"📊 Performance Evaluation Complete\")\n",
    "print(performance_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Interactive Visualization Dashboard\n",
    "\n",
    "Let's create comprehensive visualizations of our RAG system performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance visualizations\n",
    "def create_performance_dashboard(test_results: List[Dict], metrics: Dict):\n",
    "    \"\"\"Create interactive performance dashboard\"\"\"\n",
    "    \n",
    "    # Create subplot structure\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Success Rate by Category',\n",
    "            'Response Time Distribution', \n",
    "            'Model Usage Distribution',\n",
    "            'Confidence Score Analysis',\n",
    "            'Retrieved Chunks Analysis',\n",
    "            'Performance Timeline'\n",
    "        ],\n",
    "        specs=[\n",
    "            [{\"type\": \"bar\"}, {\"type\": \"histogram\"}],\n",
    "            [{\"type\": \"pie\"}, {\"type\": \"box\"}],\n",
    "            [{\"type\": \"scatter\"}, {\"type\": \"scatter\"}]\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # 1. Success Rate by Category\n",
    "    categories = list(metrics['category_performance'].keys())\n",
    "    success_rates = [metrics['category_performance'][cat]['success_rate'] * 100 \n",
    "                    for cat in categories]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=categories,\n",
    "            y=success_rates,\n",
    "            name='Success Rate',\n",
    "            marker_color='lightblue'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Response Time Distribution\n",
    "    successful_results = [r for r in test_results if r.get('success', False)]\n",
    "    response_times = [r['processing_time'] for r in successful_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=response_times,\n",
    "            name='Response Time',\n",
    "            marker_color='lightgreen',\n",
    "            nbinsx=10\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # 3. Model Usage Distribution\n",
    "    models = list(metrics['model_usage'].keys())\n",
    "    usage_counts = list(metrics['model_usage'].values())\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Pie(\n",
    "            labels=models,\n",
    "            values=usage_counts,\n",
    "            name='Model Usage'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Confidence Score Analysis\n",
    "    confidence_scores = [r['confidence_score'] for r in successful_results]\n",
    "    categories_for_conf = [r['test_category'] for r in successful_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            y=confidence_scores,\n",
    "            x=categories_for_conf,\n",
    "            name='Confidence Scores',\n",
    "            marker_color='orange'\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # 5. Retrieved Chunks Analysis\n",
    "    retrieved_chunks = [r['retrieved_chunks'] for r in successful_results]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=response_times,\n",
    "            y=retrieved_chunks,\n",
    "            mode='markers',\n",
    "            name='Chunks vs Time',\n",
    "            marker=dict(\n",
    "                size=10,\n",
    "                color=confidence_scores,\n",
    "                colorscale='Viridis',\n",
    "                showscale=True,\n",
    "                colorbar=dict(title=\"Confidence\")\n",
    "            )\n",
    "        ),\n",
    "        row=3, col=1\n",
    "    )\n",
    "    \n",
    "    # 6. Performance Timeline (if timestamps available)\n",
    "    query_indices = list(range(1, len(successful_results) + 1))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=query_indices,\n",
    "            y=response_times,\n",
    "            mode='lines+markers',\n",
    "            name='Response Time Trend',\n",
    "            line=dict(color='red')\n",
    "        ),\n",
    "        row=3, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        title_text=\"Complete RAG System Performance Dashboard\",\n",
    "        title_x=0.5,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Category\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Success Rate (%)\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Response Time (s)\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Category\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Confidence Score\", row=2, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Response Time (s)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Retrieved Chunks\", row=3, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Query Sequence\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Response Time (s)\", row=3, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display dashboard\n",
    "if performance_metrics and test_results:\n",
    "    dashboard = create_performance_dashboard(test_results, performance_metrics)\n",
    "    dashboard.show()\n",
    "    \n",
    "    print(\"📈 Performance dashboard created with:\")\n",
    "    print(\"   - Success rates by query category\")\n",
    "    print(\"   - Response time distributions\")\n",
    "    print(\"   - Model usage patterns\")\n",
    "    print(\"   - Confidence score analysis\")\n",
    "    print(\"   - Retrieval efficiency metrics\")\n",
    "else:\n",
    "    print(\"⚠️ No performance data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional static visualizations using matplotlib/seaborn\n",
    "if performance_metrics and test_results:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('RAG System Performance Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Success Rate by Category\n",
    "    categories = list(performance_metrics['category_performance'].keys())\n",
    "    success_rates = [performance_metrics['category_performance'][cat]['success_rate'] * 100 \n",
    "                    for cat in categories]\n",
    "    \n",
    "    axes[0, 0].bar(categories, success_rates, color='skyblue', alpha=0.8)\n",
    "    axes[0, 0].set_title('Success Rate by Query Category')\n",
    "    axes[0, 0].set_ylabel('Success Rate (%)')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(success_rates):\n",
    "        axes[0, 0].text(i, v + 2, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Model Usage Distribution\n",
    "    models = list(performance_metrics['model_usage'].keys())\n",
    "    usage_counts = list(performance_metrics['model_usage'].values())\n",
    "    \n",
    "    axes[0, 1].pie(usage_counts, labels=models, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Model Usage Distribution')\n",
    "    \n",
    "    # 3. Response Time vs Confidence\n",
    "    successful_results = [r for r in test_results if r.get('success', False)]\n",
    "    response_times = [r['processing_time'] for r in successful_results]\n",
    "    confidence_scores = [r['confidence_score'] for r in successful_results]\n",
    "    categories_for_color = [r['test_category'] for r in successful_results]\n",
    "    \n",
    "    # Create color map for categories\n",
    "    unique_categories = list(set(categories_for_color))\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_categories)))\n",
    "    category_colors = {cat: colors[i] for i, cat in enumerate(unique_categories)}\n",
    "    point_colors = [category_colors[cat] for cat in categories_for_color]\n",
    "    \n",
    "    scatter = axes[1, 0].scatter(response_times, confidence_scores, c=point_colors, alpha=0.7, s=60)\n",
    "    axes[1, 0].set_xlabel('Response Time (seconds)')\n",
    "    axes[1, 0].set_ylabel('Confidence Score')\n",
    "    axes[1, 0].set_title('Response Time vs Confidence Score')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend for categories\n",
    "    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=category_colors[cat], markersize=8, label=cat)\n",
    "                      for cat in unique_categories]\n",
    "    axes[1, 0].legend(handles=legend_elements, loc='best', fontsize='small')\n",
    "    \n",
    "    # 4. System Performance Summary\n",
    "    axes[1, 1].axis('off')  # Turn off axis\n",
    "    \n",
    "    # Create summary text\n",
    "    overall = performance_metrics['overall']\n",
    "    summary_text = f\"\"\"\n",
    "System Performance Summary\n",
    "\n",
    "Total Queries: {overall['total_queries']}\n",
    "Success Rate: {overall['success_rate']:.1%}\n",
    "Avg Response Time: {overall['avg_response_time']:.2f}s\n",
    "Avg Confidence: {overall['avg_confidence_score']:.2f}\n",
    "Avg Retrieved Chunks: {overall['avg_retrieved_chunks']:.1f}\n",
    "\n",
    "Best Performing Category:\n",
    "{max(performance_metrics['category_performance'].items(), \n",
    "     key=lambda x: x[1]['success_rate'])[0]}\n",
    "\n",
    "Most Used Model:\n",
    "{max(performance_metrics['model_usage'].items(), \n",
    "     key=lambda x: x[1])[0]}\n",
    "    \"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.9, summary_text, transform=axes[1, 1].transAxes,\n",
    "                    fontsize=12, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create detailed metrics table\n",
    "    results_df = pd.DataFrame(test_results)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        print(\"\\n📋 Detailed Query Results:\")\n",
    "        display_cols = ['test_category', 'success', 'model_used', 'processing_time', \n",
    "                       'confidence_score', 'retrieved_chunks']\n",
    "        \n",
    "        available_cols = [col for col in display_cols if col in results_df.columns]\n",
    "        display_df = results_df[available_cols].copy()\n",
    "        \n",
    "        if 'processing_time' in display_df.columns:\n",
    "            display_df['processing_time'] = display_df['processing_time'].round(2)\n",
    "        if 'confidence_score' in display_df.columns:\n",
    "            display_df['confidence_score'] = display_df['confidence_score'].round(2)\n",
    "        \n",
    "        print(display_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"⚠️ No performance data available for static visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Web Interface Development\n",
    "\n",
    "Let's create a simple web interface for our RAG system using Gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import Tuple, List\n",
    "\n",
    "class RAGWebInterface:\n",
    "    \"\"\"Web interface for the complete RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self, rag_system: CompleteRAGSystem):\n",
    "        self.rag_system = rag_system\n",
    "        self.query_history = []\n",
    "    \n",
    "    def process_query(self, question: str, expertise_level: str, \n",
    "                     focus_area: str) -> Tuple[str, str, str, str]:\n",
    "        \"\"\"Process query and return formatted results\"\"\"\n",
    "        \n",
    "        if not question.strip():\n",
    "            return \"Please enter a question.\", \"\", \"\", \"\"\n",
    "        \n",
    "        # Create user context\n",
    "        user_context = {\n",
    "            'expertise_level': expertise_level.lower(),\n",
    "            'focus': focus_area.lower() if focus_area != \"General\" else None\n",
    "        }\n",
    "        \n",
    "        # Process query\n",
    "        result = self.rag_system.query(question, user_context)\n",
    "        \n",
    "        # Store in history\n",
    "        self.query_history.append({\n",
    "            'question': question,\n",
    "            'result': result,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        if result['success']:\n",
    "            # Format answer\n",
    "            answer = result['answer']\n",
    "            \n",
    "            # Format metadata\n",
    "            metadata = f\"\"\"\n",
    "**Query ID:** {result['query_id']}\n",
    "**Model Used:** {result['model_used']}\n",
    "**Processing Time:** {result['processing_time']:.2f} seconds\n",
    "**Confidence Score:** {result['confidence_score']:.2f}\n",
    "**Retrieved Chunks:** {result['retrieved_chunks']}\n",
    "\"\"\"\n",
    "            \n",
    "            # Format sources\n",
    "            sources_text = \"### Sources\\n\"\n",
    "            for i, source in enumerate(result['sources'], 1):\n",
    "                source_file = source['source_file'].split('/')[-1]  # Get filename only\n",
    "                sources_text += f\"{i}. **{source_file}** ({source['chunk_id']})\\n\"\n",
    "                sources_text += f\"   *Preview:* {source['content_preview']}\\n\\n\"\n",
    "            \n",
    "            # System stats\n",
    "            stats = self.rag_system.get_system_stats()\n",
    "            stats_text = f\"\"\"\n",
    "### System Status\n",
    "- **Documents Indexed:** {stats['documents_indexed']}\n",
    "- **Available Models:** {len(stats['available_models'])}\n",
    "- **Total Queries:** {stats['total_queries']}\n",
    "- **Average Response Time:** {stats['avg_response_time']:.2f}s\n",
    "- **Success Rate:** {stats['success_rate']:.1%}\n",
    "\"\"\"\n",
    "            \n",
    "            return answer, metadata, sources_text, stats_text\n",
    "            \n",
    "        else:\n",
    "            error_message = f\"❌ Query failed: {result['error']}\"\n",
    "            return error_message, \"\", \"\", \"\"\n",
    "    \n",
    "    def get_query_history(self) -> str:\n",
    "        \"\"\"Get formatted query history\"\"\"\n",
    "        if not self.query_history:\n",
    "            return \"No queries yet.\"\n",
    "        \n",
    "        history_text = \"### Query History\\n\\n\"\n",
    "        \n",
    "        for i, entry in enumerate(self.query_history[-5:], 1):  # Last 5 queries\n",
    "            timestamp = entry['timestamp'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            success = \"✅\" if entry['result']['success'] else \"❌\"\n",
    "            \n",
    "            history_text += f\"**{i}. {timestamp} {success}**\\n\"\n",
    "            history_text += f\"Q: {entry['question'][:100]}...\\n\"\n",
    "            \n",
    "            if entry['result']['success']:\n",
    "                answer_preview = entry['result']['answer'][:150] + \"...\"\n",
    "                history_text += f\"A: {answer_preview}\\n\\n\"\n",
    "            else:\n",
    "                history_text += f\"Error: {entry['result']['error']}\\n\\n\"\n",
    "        \n",
    "        return history_text\n",
    "    \n",
    "    def create_interface(self) -> gr.Blocks:\n",
    "        \"\"\"Create Gradio interface\"\"\"\n",
    "        \n",
    "        with gr.Blocks(\n",
    "            title=\"Complete RAG System\",\n",
    "            theme=gr.themes.Soft(),\n",
    "            css=\"\"\"\n",
    "            .gradio-container {\n",
    "                max-width: 1200px !important;\n",
    "            }\n",
    "            \"\"\"\n",
    "        ) as interface:\n",
    "            \n",
    "            gr.Markdown(\n",
    "                \"\"\"\n",
    "                # 🤖 Complete RAG System Interface\n",
    "                \n",
    "                Ask questions about AI/ML, sustainable energy, or data science best practices.\n",
    "                The system will retrieve relevant information and provide comprehensive answers.\n",
    "                \"\"\"\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=2):\n",
    "                    # Input section\n",
    "                    question_input = gr.Textbox(\n",
    "                        label=\"Your Question\",\n",
    "                        placeholder=\"Enter your question here...\",\n",
    "                        lines=3\n",
    "                    )\n",
    "                    \n",
    "                    with gr.Row():\n",
    "                        expertise_level = gr.Dropdown(\n",
    "                            label=\"Your Expertise Level\",\n",
    "                            choices=[\"Beginner\", \"Intermediate\", \"Advanced\"],\n",
    "                            value=\"Intermediate\"\n",
    "                        )\n",
    "                        \n",
    "                        focus_area = gr.Dropdown(\n",
    "                            label=\"Focus Area\",\n",
    "                            choices=[\"General\", \"AI/ML\", \"Energy\", \"Data Science\"],\n",
    "                            value=\"General\"\n",
    "                        )\n",
    "                    \n",
    "                    submit_btn = gr.Button(\"Ask Question\", variant=\"primary\", size=\"lg\")\n",
    "                    \n",
    "                    # Example questions\n",
    "                    gr.Markdown(\n",
    "                        \"\"\"\n",
    "                        ### 💡 Example Questions:\n",
    "                        - What are the main types of machine learning?\n",
    "                        - How do solar and wind energy compare for large-scale deployment?\n",
    "                        - What are best practices for avoiding overfitting?\n",
    "                        - How can energy storage help with renewable integration?\n",
    "                        - What ethical considerations apply to AI development?\n",
    "                        \"\"\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    # System stats (updated on query)\n",
    "                    stats_display = gr.Markdown(\n",
    "                        value=self._get_initial_stats(),\n",
    "                        label=\"System Status\"\n",
    "                    )\n",
    "            \n",
    "            # Output section\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    answer_output = gr.Markdown(\n",
    "                        label=\"Answer\",\n",
    "                        value=\"Your answer will appear here...\"\n",
    "                    )\n",
    "            \n",
    "            # Additional information tabs\n",
    "            with gr.Tabs():\n",
    "                with gr.TabItem(\"📊 Query Details\"):\n",
    "                    metadata_output = gr.Markdown()\n",
    "                \n",
    "                with gr.TabItem(\"📚 Sources\"):\n",
    "                    sources_output = gr.Markdown()\n",
    "                \n",
    "                with gr.TabItem(\"📈 System Stats\"):\n",
    "                    detailed_stats_output = gr.Markdown()\n",
    "                \n",
    "                with gr.TabItem(\"🕐 History\"):\n",
    "                    history_btn = gr.Button(\"Refresh History\")\n",
    "                    history_output = gr.Markdown()\n",
    "            \n",
    "            # Event handlers\n",
    "            submit_btn.click(\n",
    "                fn=self.process_query,\n",
    "                inputs=[question_input, expertise_level, focus_area],\n",
    "                outputs=[answer_output, metadata_output, sources_output, detailed_stats_output]\n",
    "            )\n",
    "            \n",
    "            history_btn.click(\n",
    "                fn=self.get_query_history,\n",
    "                outputs=history_output\n",
    "            )\n",
    "            \n",
    "            # Auto-submit on Enter\n",
    "            question_input.submit(\n",
    "                fn=self.process_query,\n",
    "                inputs=[question_input, expertise_level, focus_area],\n",
    "                outputs=[answer_output, metadata_output, sources_output, detailed_stats_output]\n",
    "            )\n",
    "        \n",
    "        return interface\n",
    "    \n",
    "    def _get_initial_stats(self) -> str:\n",
    "        \"\"\"Get initial system statistics\"\"\"\n",
    "        stats = self.rag_system.get_system_stats()\n",
    "        return f\"\"\"\n",
    "### 🔧 System Ready\n",
    "- **Documents:** {stats['documents_indexed']}\n",
    "- **Models:** {len(stats['available_models'])}\n",
    "- **Cache:** {stats['cache_size']} entries\n",
    "\n",
    "Ready to answer your questions!\n",
    "\"\"\"\n",
    "\n",
    "# Create web interface\n",
    "web_interface = RAGWebInterface(rag_system)\n",
    "gradio_app = web_interface.create_interface()\n",
    "\n",
    "print(\"🌐 Web interface created successfully!\")\n",
    "print(\"🚀 Launch the interface using: gradio_app.launch()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the web interface\n",
    "# Note: This will create a public URL that you can share\n",
    "# Set share=False to keep it local only\n",
    "\n",
    "try:\n",
    "    print(\"🚀 Launching RAG System Web Interface...\")\n",
    "    print(\"📡 The interface will be available at the provided URL\")\n",
    "    print(\"🔒 Set share=False for local-only access\")\n",
    "    \n",
    "    # Launch with custom configuration\n",
    "    gradio_app.launch(\n",
    "        server_name=\"0.0.0.0\",  # Listen on all interfaces\n",
    "        server_port=7860,       # Default Gradio port\n",
    "        share=True,             # Create public URL (set to False for local only)\n",
    "        debug=True,             # Enable debug mode\n",
    "        show_error=True,        # Show detailed errors\n",
    "        quiet=False             # Show launch messages\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to launch interface: {e}\")\n",
    "    print(\"💡 You can still test the system using the query methods directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final System Summary and Best Practices\n",
    "\n",
    "### 🎯 What We've Built\n",
    "\n",
    "Our complete RAG system includes:\n",
    "\n",
    "1. **Smart Document Processing**\n",
    "   - Multi-format support (PDF, TXT, HTML)\n",
    "   - Adaptive chunking strategies\n",
    "   - Metadata enrichment\n",
    "   - Quality assessment\n",
    "\n",
    "2. **Advanced Retrieval Pipeline**\n",
    "   - Hybrid search (semantic + lexical)\n",
    "   - Multi-stage retrieval with re-ranking\n",
    "   - Context-aware result fusion\n",
    "   - Relevance scoring\n",
    "\n",
    "3. **Intelligent Model Management**\n",
    "   - Multi-model support with automatic routing\n",
    "   - Cost-performance optimization\n",
    "   - Fallback mechanisms\n",
    "   - Model-specific prompt adaptation\n",
    "\n",
    "4. **Production-Ready Features**\n",
    "   - Caching system with TTL\n",
    "   - Comprehensive monitoring\n",
    "   - Error handling and retry logic\n",
    "   - Performance analytics\n",
    "\n",
    "5. **User-Friendly Interface**\n",
    "   - Web-based query interface\n",
    "   - User context adaptation\n",
    "   - Query history tracking\n",
    "   - Real-time system statistics\n",
    "\n",
    "### 🏆 Key Achievements\n",
    "\n",
    "- **Scalability**: Handles multiple document types and large knowledge bases\n",
    "- **Reliability**: Robust error handling and fallback mechanisms\n",
    "- **Performance**: Optimized retrieval and caching strategies\n",
    "- **Maintainability**: Modular architecture with comprehensive monitoring\n",
    "- **Usability**: Intuitive interface with contextual responses\n",
    "\n",
    "### 📈 Performance Insights\n",
    "\n",
    "Based on our testing:\n",
    "- Multi-stage retrieval improves answer relevance\n",
    "- Hybrid search outperforms single-method approaches\n",
    "- Model routing reduces costs while maintaining quality\n",
    "- Caching significantly improves response times\n",
    "- User context adaptation enhances answer appropriateness\n",
    "\n",
    "### 🔮 Future Enhancements\n",
    "\n",
    "1. **Advanced NLP Features**\n",
    "   - Named entity recognition for better metadata\n",
    "   - Sentiment analysis for user queries\n",
    "   - Multi-language support\n",
    "   - Query intent classification\n",
    "\n",
    "2. **Enhanced Retrieval**\n",
    "   - Graph-based knowledge representation\n",
    "   - Temporal reasoning capabilities\n",
    "   - Cross-document relationship modeling\n",
    "   - Personalized retrieval based on user history\n",
    "\n",
    "3. **Production Scaling**\n",
    "   - Distributed vector storage\n",
    "   - Load balancing across multiple models\n",
    "   - Real-time document ingestion\n",
    "   - A/B testing framework\n",
    "\n",
    "4. **Advanced Analytics**\n",
    "   - User behavior analysis\n",
    "   - Knowledge gap identification\n",
    "   - Answer quality prediction\n",
    "   - Automated system optimization\n",
    "\n",
    "### 🛡️ Security and Privacy\n",
    "\n",
    "- **Data Protection**: Implement proper access controls\n",
    "- **Privacy**: Consider data anonymization and user consent\n",
    "- **Security**: Regular security audits and updates\n",
    "- **Compliance**: Ensure regulatory compliance (GDPR, etc.)\n",
    "\n",
    "### 📚 Deployment Checklist\n",
    "\n",
    "Before deploying to production:\n",
    "\n",
    "- [ ] Set up proper API key management\n",
    "- [ ] Configure monitoring and alerting\n",
    "- [ ] Implement proper logging\n",
    "- [ ] Set up backup and recovery procedures\n",
    "- [ ] Performance test with expected load\n",
    "- [ ] Security review and penetration testing\n",
    "- [ ] User acceptance testing\n",
    "- [ ] Documentation and training materials\n",
    "\n",
    "---\n",
    "\n",
    "## Congratulations! 🎉\n",
    "\n",
    "You've successfully built a complete, production-ready RAG system that demonstrates:\n",
    "\n",
    "✅ **Comprehensive Architecture** - All major RAG components integrated\n",
    "✅ **Production Features** - Monitoring, caching, error handling\n",
    "✅ **User Experience** - Web interface with contextual responses\n",
    "✅ **Performance Optimization** - Multi-model routing and hybrid search\n",
    "✅ **Scalability** - Modular design for future enhancements\n",
    "\n",
    "This system serves as a solid foundation for building domain-specific RAG applications. The modular architecture allows you to easily customize and extend functionality based on your specific requirements.\n",
    "\n",
    "**Next Steps**: Deploy this system in your environment, customize it for your specific use case, and continue to iterate based on user feedback and performance metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}