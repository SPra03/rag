{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 10: Prompt Engineering for RAG\n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Design effective prompts that maximize utilization of retrieved context\n",
    "- Implement robust prompt templates for different RAG scenarios\n",
    "- Handle context length limitations and optimization strategies\n",
    "- Create model-specific prompt adaptations for different LLMs\n",
    "- Master advanced techniques like Chain-of-Thought and few-shot prompting in RAG\n",
    "- Build attribution and citation systems for reliable source tracking\n",
    "\n",
    "## ðŸ“š Key Concepts\n",
    "\n",
    "### Why Prompt Engineering is Critical for RAG\n",
    "\n",
    "**The RAG Quality Chain:**\n",
    "```\n",
    "Great Retrieval + Poor Prompt = Mediocre Results\n",
    "Great Retrieval + Great Prompt = Excellent Results\n",
    "```\n",
    "\n",
    "Even perfect retrieval can fail without effective prompts. Research shows that **prompt quality accounts for 30-40% of RAG system performance**.\n",
    "\n",
    "### ðŸ—ï¸ RAG Prompt Architecture (2025)\n",
    "\n",
    "**Modern RAG prompts follow a structured format:**\n",
    "\n",
    "1. **System Context**: Role definition and behavior instructions\n",
    "2. **Retrieved Context**: Relevant documents with metadata\n",
    "3. **Task Instructions**: Specific guidance for using context\n",
    "4. **User Query**: The actual question or request\n",
    "5. **Output Format**: Structure and citation requirements\n",
    "6. **Quality Controls**: Fallback instructions and limitations\n",
    "\n",
    "### ðŸ”„ Context Integration Patterns\n",
    "\n",
    "#### Context-First Pattern (Most Common):\n",
    "```\n",
    "CONTEXT: [Retrieved information]\n",
    "QUESTION: [User query]\n",
    "ANSWER: [LLM response]\n",
    "```\n",
    "\n",
    "#### Question-Context Pattern:\n",
    "```\n",
    "QUESTION: [User query]\n",
    "RELEVANT_CONTEXT: [Retrieved information]\n",
    "ANSWER: [LLM response]\n",
    "```\n",
    "\n",
    "#### Interleaved Pattern (Advanced):\n",
    "```\n",
    "QUESTION: [User query]\n",
    "CONTEXT_1: [Most relevant document]\n",
    "ANALYSIS_1: [How this relates to question]\n",
    "CONTEXT_2: [Secondary document]\n",
    "FINAL_ANSWER: [Synthesized response]\n",
    "```\n",
    "\n",
    "### ðŸ“Š 2025 Prompt Engineering Best Practices\n",
    "\n",
    "| Technique | Impact | Use Case |\n",
    "|-----------|--------|----------|\n",
    "| **Structured Templates** | +25% consistency | Production systems |\n",
    "| **Chain-of-Thought** | +30% reasoning quality | Complex queries |\n",
    "| **Attribution Requirements** | +40% source accuracy | Fact-sensitive domains |\n",
    "| **Few-Shot Examples** | +20% format compliance | Specific output formats |\n",
    "| **Context Summarization** | +15% with long contexts | Large document sets |\n",
    "\n",
    "### âš¡ Context Length Optimization\n",
    "\n",
    "**Context window utilization strategies:**\n",
    "- **GPT-4**: 128K tokens (~200 pages)\n",
    "- **Claude-3**: 200K tokens (~300 pages)  \n",
    "- **Gemini Pro**: 2M tokens (~3000 pages)\n",
    "\n",
    "**Optimization techniques:**\n",
    "1. **Context truncation**: Keep most relevant portions\n",
    "2. **Context summarization**: LLM-powered compression\n",
    "3. **Hierarchical context**: Important info first\n",
    "4. **Dynamic context**: Adjust based on query complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setup\n",
    "Let's install the required packages and set up our prompt engineering lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q langchain langchain-community langchain-openai\n",
    "!pip install -q openai anthropic python-dotenv\n",
    "!pip install -q jinja2 tiktoken\n",
    "!pip install -q numpy pandas matplotlib seaborn plotly\n",
    "!pip install -q textstat nltk\n",
    "# Note: You'll need API keys for OpenAI/Anthropic for full functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Template engine\n",
    "from jinja2 import Template, Environment, BaseLoader\n",
    "import tiktoken\n",
    "\n",
    "# Text analysis\n",
    "import textstat\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# LangChain components\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Initialize tokenizer for token counting\n",
    "try:\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # GPT-4 tokenizer\n",
    "except:\n",
    "    tokenizer = None\n",
    "    print(\"âš ï¸ Tiktoken not available, token counting will be estimated\")\n",
    "\n",
    "print(\"âœ… Setup complete!\")\n",
    "print(f\"ðŸ“… Today's date: {datetime.now().strftime('%Y-%m-%d')}\")\n",
    "print(f\"ðŸ”‘ OpenAI API Key: {'âœ… Available' if os.getenv('OPENAI_API_KEY') else 'âŒ Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 1: RAG Prompt Templates and Patterns\n",
    "\n",
    "Let's build a comprehensive library of RAG prompt templates for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGPromptLibrary:\n",
    "    \"\"\"Comprehensive library of RAG prompt templates and patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.templates = {}\n",
    "        self.template_usage_stats = defaultdict(int)\n",
    "        self.jinja_env = Environment(loader=BaseLoader())\n",
    "        \n",
    "        # Initialize standard templates\n",
    "        self._initialize_standard_templates()\n",
    "    \n",
    "    def _initialize_standard_templates(self):\n",
    "        \"\"\"Initialize standard RAG prompt templates\"\"\"\n",
    "        \n",
    "        # 1. Basic RAG Template\n",
    "        self.templates['basic_rag'] = {\n",
    "            'name': 'Basic RAG',\n",
    "            'description': 'Simple context-question-answer format',\n",
    "            'template': '''You are a helpful assistant that answers questions based on provided context.\n",
    "\n",
    "CONTEXT:\n",
    "{{ context }}\n",
    "\n",
    "QUESTION: {{ question }}\n",
    "\n",
    "Please answer the question based on the provided context. If the context doesn't contain enough information to answer the question, please say so.\n",
    "\n",
    "ANSWER:''',\n",
    "            'variables': ['context', 'question'],\n",
    "            'use_cases': ['General Q&A', 'Simple fact-finding', 'Basic information retrieval']\n",
    "        }\n",
    "        \n",
    "        # 2. Structured RAG with Citations\n",
    "        self.templates['structured_citations'] = {\n",
    "            'name': 'Structured RAG with Citations',\n",
    "            'description': 'Structured format with mandatory source attribution',\n",
    "            'template': '''You are an expert assistant that provides accurate answers with proper citations.\n",
    "\n",
    "CONTEXT SOURCES:\n",
    "{% for doc in context_sources %}\n",
    "[{{ loop.index }}] {{ doc.title or \"Document \" + loop.index|string }}\n",
    "{{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "QUESTION: {{ question }}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Answer the question using ONLY information from the provided sources\n",
    "2. Cite sources using [1], [2], etc. format\n",
    "3. If insufficient information, state what's missing\n",
    "4. Be precise and factual\n",
    "\n",
    "ANSWER:''',\n",
    "            'variables': ['context_sources', 'question'],\n",
    "            'use_cases': ['Research assistance', 'Academic writing', 'Fact-checking', 'Legal/Medical domains']\n",
    "        }\n",
    "        \n",
    "        # 3. Chain-of-Thought RAG\n",
    "        self.templates['chain_of_thought'] = {\n",
    "            'name': 'Chain-of-Thought RAG',\n",
    "            'description': 'Step-by-step reasoning with retrieved context',\n",
    "            'template': '''You are an analytical assistant that thinks step-by-step.\n",
    "\n",
    "CONTEXT:\n",
    "{{ context }}\n",
    "\n",
    "QUESTION: {{ question }}\n",
    "\n",
    "Please think through this step by step:\n",
    "\n",
    "STEP 1 - IDENTIFY RELEVANT INFORMATION:\n",
    "What information from the context is most relevant to the question?\n",
    "\n",
    "STEP 2 - ANALYZE THE INFORMATION:\n",
    "How does this information relate to and help answer the question?\n",
    "\n",
    "STEP 3 - SYNTHESIZE AND CONCLUDE:\n",
    "Based on the analysis, what is the complete answer?\n",
    "\n",
    "Let me work through this:''',\n",
    "            'variables': ['context', 'question'],\n",
    "            'use_cases': ['Complex reasoning', 'Multi-step problems', 'Analysis tasks', 'Educational content']\n",
    "        }\n",
    "        \n",
    "        # 4. Conversational RAG\n",
    "        self.templates['conversational'] = {\n",
    "            'name': 'Conversational RAG',\n",
    "            'description': 'Multi-turn conversation with context and history',\n",
    "            'template': '''You are a helpful assistant in an ongoing conversation.\n",
    "\n",
    "CONTEXT:\n",
    "{{ context }}\n",
    "\n",
    "CONVERSATION HISTORY:\n",
    "{% for turn in conversation_history %}\n",
    "{{ turn.role|upper }}: {{ turn.content }}\n",
    "{% endfor %}\n",
    "\n",
    "CURRENT QUESTION: {{ question }}\n",
    "\n",
    "Please respond naturally, considering both the provided context and the conversation history. Reference previous parts of our conversation when relevant.\n",
    "\n",
    "RESPONSE:''',\n",
    "            'variables': ['context', 'conversation_history', 'question'],\n",
    "            'use_cases': ['Chatbots', 'Customer support', 'Interactive learning', 'Consultation systems']\n",
    "        }\n",
    "        \n",
    "        # 5. Few-Shot RAG\n",
    "        self.templates['few_shot'] = {\n",
    "            'name': 'Few-Shot RAG',\n",
    "            'description': 'Examples-based learning with retrieved context',\n",
    "            'template': '''You are an assistant that learns from examples and applies knowledge to new situations.\n",
    "\n",
    "Here are some examples of how to answer questions using provided context:\n",
    "\n",
    "{% for example in examples %}\n",
    "EXAMPLE {{ loop.index }}:\n",
    "Context: {{ example.context }}\n",
    "Question: {{ example.question }}\n",
    "Answer: {{ example.answer }}\n",
    "\n",
    "{% endfor %}\n",
    "Now answer this new question following the same pattern:\n",
    "\n",
    "CONTEXT:\n",
    "{{ context }}\n",
    "\n",
    "QUESTION: {{ question }}\n",
    "\n",
    "ANSWER:''',\n",
    "            'variables': ['examples', 'context', 'question'],\n",
    "            'use_cases': ['Specific formats', 'Domain adaptation', 'Style consistency', 'Complex outputs']\n",
    "        }\n",
    "        \n",
    "        # 6. Summarization RAG\n",
    "        self.templates['summarization'] = {\n",
    "            'name': 'Summarization RAG',\n",
    "            'description': 'For handling large contexts with summarization',\n",
    "            'template': '''You are an expert at analyzing and summarizing information.\n",
    "\n",
    "LARGE CONTEXT ({{ context_length }} tokens):\n",
    "{{ context }}\n",
    "\n",
    "QUESTION: {{ question }}\n",
    "\n",
    "TASK:\n",
    "1. First, identify the most relevant sections of the context for the question\n",
    "2. Summarize the key information from those sections\n",
    "3. Answer the question based on your summary\n",
    "4. Note if the full context contains additional relevant details\n",
    "\n",
    "RELEVANT SECTIONS SUMMARY:\n",
    "[Summarize key relevant information here]\n",
    "\n",
    "ANSWER BASED ON SUMMARY:\n",
    "[Provide answer here]\n",
    "\n",
    "ADDITIONAL CONTEXT NOTES:\n",
    "[Note any other relevant details from the full context]''',\n",
    "            'variables': ['context', 'context_length', 'question'],\n",
    "            'use_cases': ['Long documents', 'Research papers', 'Legal documents', 'Technical manuals']\n",
    "        }\n",
    "        \n",
    "        # 7. Comparative Analysis RAG\n",
    "        self.templates['comparative'] = {\n",
    "            'name': 'Comparative Analysis RAG',\n",
    "            'description': 'For comparing information across multiple sources',\n",
    "            'template': '''You are an analyst comparing information from multiple sources.\n",
    "\n",
    "{% for source in sources %}\n",
    "SOURCE {{ loop.index }} ({{ source.title or \"Document \" + loop.index|string }}):\n",
    "{{ source.content }}\n",
    "\n",
    "{% endfor %}\n",
    "ANALYSIS QUESTION: {{ question }}\n",
    "\n",
    "Please provide a comparative analysis:\n",
    "\n",
    "SIMILARITIES:\n",
    "- [What do the sources agree on?]\n",
    "\n",
    "DIFFERENCES:\n",
    "- [Where do sources differ or contradict?]\n",
    "\n",
    "KEY INSIGHTS:\n",
    "- [What unique insights emerge from comparing these sources?]\n",
    "\n",
    "CONCLUSION:\n",
    "[Overall assessment based on the comparison]''',\n",
    "            'variables': ['sources', 'question'],\n",
    "            'use_cases': ['Research synthesis', 'Market analysis', 'Literature review', 'Policy comparison']\n",
    "        }\n",
    "    \n",
    "    def get_template(self, template_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get a template by name\"\"\"\n",
    "        if template_name not in self.templates:\n",
    "            raise ValueError(f\"Template '{template_name}' not found. Available: {list(self.templates.keys())}\")\n",
    "        \n",
    "        self.template_usage_stats[template_name] += 1\n",
    "        return self.templates[template_name]\n",
    "    \n",
    "    def render_prompt(self, template_name: str, **kwargs) -> str:\n",
    "        \"\"\"Render a prompt template with provided variables\"\"\"\n",
    "        template_info = self.get_template(template_name)\n",
    "        \n",
    "        # Check required variables\n",
    "        missing_vars = set(template_info['variables']) - set(kwargs.keys())\n",
    "        if missing_vars:\n",
    "            raise ValueError(f\"Missing required variables: {missing_vars}\")\n",
    "        \n",
    "        # Render template\n",
    "        template = self.jinja_env.from_string(template_info['template'])\n",
    "        rendered = template.render(**kwargs)\n",
    "        \n",
    "        return rendered.strip()\n",
    "    \n",
    "    def add_custom_template(self, name: str, template: str, variables: List[str], \n",
    "                          description: str = \"\", use_cases: List[str] = None) -> None:\n",
    "        \"\"\"Add a custom template to the library\"\"\"\n",
    "        self.templates[name] = {\n",
    "            'name': name,\n",
    "            'description': description,\n",
    "            'template': template,\n",
    "            'variables': variables,\n",
    "            'use_cases': use_cases or [],\n",
    "            'custom': True\n",
    "        }\n",
    "        print(f\"âœ… Added custom template: {name}\")\n",
    "    \n",
    "    def list_templates(self) -> pd.DataFrame:\n",
    "        \"\"\"List all available templates\"\"\"\n",
    "        template_data = []\n",
    "        for name, info in self.templates.items():\n",
    "            template_data.append({\n",
    "                'Name': name,\n",
    "                'Description': info['description'],\n",
    "                'Variables': ', '.join(info['variables']),\n",
    "                'Use Cases': '; '.join(info['use_cases']),\n",
    "                'Usage Count': self.template_usage_stats[name],\n",
    "                'Type': 'Custom' if info.get('custom', False) else 'Built-in'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(template_data)\n",
    "    \n",
    "    def count_tokens(self, text: str) -> int:\n",
    "        \"\"\"Count tokens in text (approximate if tiktoken not available)\"\"\"\n",
    "        if tokenizer:\n",
    "            return len(tokenizer.encode(text))\n",
    "        else:\n",
    "            # Rough approximation: 1 token â‰ˆ 4 characters\n",
    "            return len(text) // 4\n",
    "    \n",
    "    def analyze_prompt(self, prompt: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze prompt characteristics\"\"\"\n",
    "        return {\n",
    "            'token_count': self.count_tokens(prompt),\n",
    "            'character_count': len(prompt),\n",
    "            'word_count': len(prompt.split()),\n",
    "            'line_count': len(prompt.split('\\n')),\n",
    "            'readability_score': textstat.flesch_reading_ease(prompt),\n",
    "            'has_instructions': bool(re.search(r'instructions?:|steps?:|task:', prompt.lower())),\n",
    "            'has_examples': 'example' in prompt.lower(),\n",
    "            'has_formatting': bool(re.search(r'answer:|response:|step \\d+', prompt.lower()))\n",
    "        }\n",
    "\n",
    "# Initialize the prompt library\n",
    "prompt_library = RAGPromptLibrary()\n",
    "print(\"ðŸ“š RAG Prompt Library initialized!\")\n",
    "print(f\"   Available templates: {len(prompt_library.templates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore available templates\n",
    "templates_df = prompt_library.list_templates()\n",
    "print(\"ðŸ“‹ AVAILABLE RAG PROMPT TEMPLATES\")\n",
    "print(\"=\" * 60)\n",
    "print(templates_df.to_string(index=False))\n",
    "\n",
    "# Display template examples\n",
    "print(\"\\n\\nðŸ§ª TEMPLATE EXAMPLES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Sample data for demonstrations\n",
    "sample_context = \"\"\"TensorFlow 2.14.0 introduces significant performance improvements for GPU training, \n",
    "with up to 35% faster training times for transformer models. The new version includes optimized \n",
    "kernels and improved memory management. PyTorch 2.1 also offers similar optimizations with \n",
    "torch.compile() providing up to 60% speedup for inference.\"\"\"\n",
    "\n",
    "sample_question = \"What are the performance improvements in the latest deep learning frameworks?\"\n",
    "\n",
    "# Example 1: Basic RAG\n",
    "print(\"\\n1ï¸âƒ£ Basic RAG Template:\")\n",
    "print(\"-\" * 30)\n",
    "basic_prompt = prompt_library.render_prompt(\n",
    "    'basic_rag',\n",
    "    context=sample_context,\n",
    "    question=sample_question\n",
    ")\n",
    "print(basic_prompt[:200] + \"...\")\n",
    "basic_analysis = prompt_library.analyze_prompt(basic_prompt)\n",
    "print(f\"   Tokens: {basic_analysis['token_count']}, Readability: {basic_analysis['readability_score']:.1f}\")\n",
    "\n",
    "# Example 2: Chain-of-Thought RAG\n",
    "print(\"\\n2ï¸âƒ£ Chain-of-Thought RAG Template:\")\n",
    "print(\"-\" * 35)\n",
    "cot_prompt = prompt_library.render_prompt(\n",
    "    'chain_of_thought',\n",
    "    context=sample_context,\n",
    "    question=sample_question\n",
    ")\n",
    "print(cot_prompt[:200] + \"...\")\n",
    "cot_analysis = prompt_library.analyze_prompt(cot_prompt)\n",
    "print(f\"   Tokens: {cot_analysis['token_count']}, Has Structure: {cot_analysis['has_formatting']}\")\n",
    "\n",
    "# Example 3: Few-Shot RAG\n",
    "print(\"\\n3ï¸âƒ£ Few-Shot RAG Template:\")\n",
    "print(\"-\" * 25)\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        'context': 'Python 3.11 is 25% faster than Python 3.10.',\n",
    "        'question': 'How much faster is Python 3.11?',\n",
    "        'answer': 'Python 3.11 is 25% faster than Python 3.10 according to the provided information.'\n",
    "    }\n",
    "]\n",
    "\n",
    "few_shot_prompt = prompt_library.render_prompt(\n",
    "    'few_shot',\n",
    "    examples=few_shot_examples,\n",
    "    context=sample_context,\n",
    "    question=sample_question\n",
    ")\n",
    "print(few_shot_prompt[:200] + \"...\")\n",
    "few_shot_analysis = prompt_library.analyze_prompt(few_shot_prompt)\n",
    "print(f\"   Tokens: {few_shot_analysis['token_count']}, Has Examples: {few_shot_analysis['has_examples']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Exercise 2: Context Management and Optimization\n",
    "\n",
    "Let's implement advanced context management techniques for handling long documents and optimizing context utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextManager:\n",
    "    \"\"\"Advanced context management for RAG systems\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens: int = 4000):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.prompt_library = prompt_library\n",
    "        self.truncation_strategies = {\n",
    "            'head': self._truncate_head,\n",
    "            'tail': self._truncate_tail,\n",
    "            'middle': self._truncate_middle,\n",
    "            'smart': self._truncate_smart,\n",
    "            'summarize': self._truncate_with_summary\n",
    "        }\n",
    "    \n",
    "    def estimate_context_usage(self, template_name: str, context: str, \n",
    "                             question: str, **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Estimate token usage for a complete prompt\"\"\"\n",
    "        \n",
    "        # Render full prompt\n",
    "        full_prompt = self.prompt_library.render_prompt(\n",
    "            template_name,\n",
    "            context=context,\n",
    "            question=question,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # Analyze token distribution\n",
    "        template_info = self.prompt_library.get_template(template_name)\n",
    "        template_without_vars = template_info['template']\n",
    "        \n",
    "        # Remove variable placeholders to estimate fixed template size\n",
    "        for var in template_info['variables']:\n",
    "            template_without_vars = re.sub(rf'\\{{\\{{\\s*{var}.*?\\}}\\}}', '', template_without_vars)\n",
    "        \n",
    "        template_tokens = self.prompt_library.count_tokens(template_without_vars)\n",
    "        context_tokens = self.prompt_library.count_tokens(context)\n",
    "        question_tokens = self.prompt_library.count_tokens(question)\n",
    "        other_tokens = self.prompt_library.count_tokens(full_prompt) - template_tokens - context_tokens - question_tokens\n",
    "        \n",
    "        return {\n",
    "            'total_tokens': self.prompt_library.count_tokens(full_prompt),\n",
    "            'template_tokens': template_tokens,\n",
    "            'context_tokens': context_tokens,\n",
    "            'question_tokens': question_tokens,\n",
    "            'other_tokens': max(0, other_tokens),  # Account for any rendering overhead\n",
    "            'context_percentage': (context_tokens / self.prompt_library.count_tokens(full_prompt)) * 100,\n",
    "            'fits_in_limit': self.prompt_library.count_tokens(full_prompt) <= self.max_tokens,\n",
    "            'tokens_over_limit': max(0, self.prompt_library.count_tokens(full_prompt) - self.max_tokens)\n",
    "        }\n",
    "    \n",
    "    def _truncate_head(self, context: str, target_tokens: int) -> str:\n",
    "        \"\"\"Keep the beginning of context\"\"\"\n",
    "        words = context.split()\n",
    "        # Rough approximation: 1.3 words per token\n",
    "        target_words = int(target_tokens * 1.3)\n",
    "        \n",
    "        if len(words) <= target_words:\n",
    "            return context\n",
    "        \n",
    "        return ' '.join(words[:target_words]) + \"\\n\\n[Context truncated...]\"\n",
    "    \n",
    "    def _truncate_tail(self, context: str, target_tokens: int) -> str:\n",
    "        \"\"\"Keep the end of context\"\"\"\n",
    "        words = context.split()\n",
    "        target_words = int(target_tokens * 1.3)\n",
    "        \n",
    "        if len(words) <= target_words:\n",
    "            return context\n",
    "        \n",
    "        return \"[Context truncated...]\\n\\n\" + ' '.join(words[-target_words:])\n",
    "    \n",
    "    def _truncate_middle(self, context: str, target_tokens: int) -> str:\n",
    "        \"\"\"Keep beginning and end, remove middle\"\"\"\n",
    "        words = context.split()\n",
    "        target_words = int(target_tokens * 1.3)\n",
    "        \n",
    "        if len(words) <= target_words:\n",
    "            return context\n",
    "        \n",
    "        # Keep first and last portions\n",
    "        keep_each_side = target_words // 2\n",
    "        \n",
    "        beginning = ' '.join(words[:keep_each_side])\n",
    "        end = ' '.join(words[-keep_each_side:])\n",
    "        \n",
    "        return f\"{beginning}\\n\\n[Middle section truncated...]\\n\\n{end}\"\n",
    "    \n",
    "    def _truncate_smart(self, context: str, target_tokens: int) -> str:\n",
    "        \"\"\"Smart truncation preserving complete sentences and paragraphs\"\"\"\n",
    "        # Split into sentences\n",
    "        import nltk\n",
    "        sentences = nltk.sent_tokenize(context)\n",
    "        \n",
    "        # Build truncated context sentence by sentence\n",
    "        truncated = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.prompt_library.count_tokens(sentence)\n",
    "            \n",
    "            if current_tokens + sentence_tokens <= target_tokens:\n",
    "                truncated += sentence + \" \"\n",
    "                current_tokens += sentence_tokens\n",
    "            else:\n",
    "                truncated += \"\\n\\n[Remaining context truncated for length...]\"\n",
    "                break\n",
    "        \n",
    "        return truncated.strip()\n",
    "    \n",
    "    def _truncate_with_summary(self, context: str, target_tokens: int) -> str:\n",
    "        \"\"\"Truncate with LLM-generated summary (simulated)\"\"\"\n",
    "        # In production, this would use an LLM to summarize\n",
    "        # For demo, we'll create a simple extractive summary\n",
    "        \n",
    "        sentences = context.split('. ')\n",
    "        if len(sentences) <= 3:\n",
    "            return context\n",
    "        \n",
    "        # Keep first sentence, middle sentence, and last sentence\n",
    "        key_sentences = [\n",
    "            sentences[0],\n",
    "            sentences[len(sentences)//2] if len(sentences) > 2 else \"\",\n",
    "            sentences[-1]\n",
    "        ]\n",
    "        \n",
    "        summary = '. '.join([s for s in key_sentences if s]).strip()\n",
    "        \n",
    "        # Add summary indicator\n",
    "        return f\"[SUMMARY] {summary}\\n\\n[Full context was summarized to fit length constraints]\"\n",
    "    \n",
    "    def optimize_context(self, template_name: str, context: str, question: str,\n",
    "                        strategy: str = 'smart', **kwargs) -> Dict[str, Any]:\n",
    "        \"\"\"Optimize context to fit within token limits\"\"\"\n",
    "        \n",
    "        # First, check if optimization is needed\n",
    "        usage = self.estimate_context_usage(template_name, context, question, **kwargs)\n",
    "        \n",
    "        if usage['fits_in_limit']:\n",
    "            return {\n",
    "                'optimized_context': context,\n",
    "                'optimization_applied': False,\n",
    "                'original_tokens': usage['context_tokens'],\n",
    "                'final_tokens': usage['context_tokens'],\n",
    "                'tokens_saved': 0,\n",
    "                'strategy_used': 'none',\n",
    "                'full_prompt_tokens': usage['total_tokens']\n",
    "            }\n",
    "        \n",
    "        # Calculate how much we need to reduce context\n",
    "        non_context_tokens = usage['total_tokens'] - usage['context_tokens']\n",
    "        target_context_tokens = self.max_tokens - non_context_tokens - 100  # Leave some buffer\n",
    "        \n",
    "        if strategy not in self.truncation_strategies:\n",
    "            strategy = 'smart'\n",
    "        \n",
    "        # Apply truncation strategy\n",
    "        optimized_context = self.truncation_strategies[strategy](context, target_context_tokens)\n",
    "        \n",
    "        # Verify the optimization worked\n",
    "        new_usage = self.estimate_context_usage(template_name, optimized_context, question, **kwargs)\n",
    "        \n",
    "        return {\n",
    "            'optimized_context': optimized_context,\n",
    "            'optimization_applied': True,\n",
    "            'original_tokens': usage['context_tokens'],\n",
    "            'final_tokens': new_usage['context_tokens'],\n",
    "            'tokens_saved': usage['context_tokens'] - new_usage['context_tokens'],\n",
    "            'strategy_used': strategy,\n",
    "            'full_prompt_tokens': new_usage['total_tokens'],\n",
    "            'fits_after_optimization': new_usage['fits_in_limit']\n",
    "        }\n",
    "    \n",
    "    def create_hierarchical_context(self, documents: List[Dict[str, Any]], \n",
    "                                  question: str, max_docs: int = 5) -> str:\n",
    "        \"\"\"Create hierarchical context with most relevant information first\"\"\"\n",
    "        \n",
    "        # Sort documents by relevance (assuming they have relevance scores)\n",
    "        sorted_docs = sorted(documents, key=lambda x: x.get('score', 0), reverse=True)[:max_docs]\n",
    "        \n",
    "        hierarchical_context = \"\"\n",
    "        current_tokens = 0\n",
    "        \n",
    "        for i, doc in enumerate(sorted_docs):\n",
    "            doc_header = f\"\\n--- DOCUMENT {i+1} (Relevance: {doc.get('score', 0):.3f}) ---\\n\"\n",
    "            doc_content = doc.get('content', doc.get('text', ''))\n",
    "            \n",
    "            # Add metadata if available\n",
    "            if 'metadata' in doc:\n",
    "                metadata = doc['metadata']\n",
    "                source_info = f\"Source: {metadata.get('source', 'Unknown')} | \"\n",
    "                source_info += f\"Date: {metadata.get('date', 'Unknown')} | \"\n",
    "                source_info += f\"Category: {metadata.get('category', 'Unknown')}\\n\"\n",
    "                doc_header += source_info\n",
    "            \n",
    "            doc_section = doc_header + doc_content + \"\\n\"\n",
    "            section_tokens = self.prompt_library.count_tokens(doc_section)\n",
    "            \n",
    "            # Check if adding this document would exceed limits\n",
    "            if current_tokens + section_tokens <= (self.max_tokens * 0.7):  # Reserve 30% for template\n",
    "                hierarchical_context += doc_section\n",
    "                current_tokens += section_tokens\n",
    "            else:\n",
    "                hierarchical_context += f\"\\n[{len(sorted_docs) - i} additional documents truncated due to length limits]\\n\"\n",
    "                break\n",
    "        \n",
    "        return hierarchical_context\n",
    "    \n",
    "    def analyze_context_quality(self, context: str, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze context quality and relevance\"\"\"\n",
    "        question_words = set(question.lower().split())\n",
    "        context_words = set(context.lower().split())\n",
    "        \n",
    "        # Calculate word overlap\n",
    "        overlap = len(question_words.intersection(context_words))\n",
    "        overlap_ratio = overlap / len(question_words) if question_words else 0\n",
    "        \n",
    "        # Analyze context structure\n",
    "        sentences = context.count('.') + context.count('!') + context.count('?')\n",
    "        paragraphs = len([p for p in context.split('\\n\\n') if p.strip()])\n",
    "        \n",
    "        # Calculate information density\n",
    "        unique_words = len(set(context.lower().split()))\n",
    "        total_words = len(context.split())\n",
    "        lexical_diversity = unique_words / total_words if total_words > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'word_overlap_count': overlap,\n",
    "            'word_overlap_ratio': overlap_ratio,\n",
    "            'sentence_count': sentences,\n",
    "            'paragraph_count': paragraphs,\n",
    "            'lexical_diversity': lexical_diversity,\n",
    "            'context_length': len(context),\n",
    "            'readability': textstat.flesch_reading_ease(context),\n",
    "            'estimated_tokens': self.prompt_library.count_tokens(context),\n",
    "            'quality_score': (overlap_ratio * 0.4 + lexical_diversity * 0.3 + \n",
    "                             min(1.0, sentences / 10) * 0.3)  # Normalized quality score\n",
    "        }\n",
    "\n",
    "# Initialize context manager\n",
    "context_manager = ContextManager(max_tokens=4000)\n",
    "print(\"ðŸ“ Context Manager initialized!\")\n",
    "print(f\"   Max tokens: {context_manager.max_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test context management with different scenarios\n",
    "\n",
    "# Create long context to test truncation\n",
    "long_context = \"\"\"TensorFlow 2.14.0 represents a major milestone in deep learning frameworks, introducing \n",
    "significant performance optimizations that benefit both training and inference workflows. The new version \n",
    "includes GPU kernel optimizations that can reduce training time by up to 35% for transformer-based models, \n",
    "which have become increasingly important in natural language processing applications.\n",
    "\n",
    "The performance improvements come from several key areas. First, the team has optimized memory management \n",
    "in the CUDA kernels, reducing memory fragmentation and improving cache utilization. This is particularly \n",
    "beneficial for large models that previously struggled with memory constraints. Second, the new version \n",
    "includes improved graph optimization that can eliminate redundant operations and fuse operations together \n",
    "for better hardware utilization.\n",
    "\n",
    "PyTorch 2.1 has also made significant strides in performance optimization. The introduction of torch.compile() \n",
    "represents a fundamental shift in how PyTorch handles model compilation. This new feature can provide up to \n",
    "60% speedup for inference workloads by leveraging aggressive compiler optimizations and hardware-specific \n",
    "code generation.\n",
    "\n",
    "The compile function works by tracing the execution of your model and then optimizing the resulting graph \n",
    "using a variety of techniques including operator fusion, constant folding, and dead code elimination. \n",
    "It also includes support for dynamic shapes, which has been a long-standing challenge in graph-based \n",
    "optimization systems.\n",
    "\n",
    "Both frameworks have also improved their distributed training capabilities. TensorFlow now includes better \n",
    "support for parameter sharding and gradient accumulation, while PyTorch has enhanced its DistributedDataParallel \n",
    "implementation with more efficient communication patterns and better fault tolerance.\n",
    "\n",
    "Looking forward, both frameworks are investing heavily in support for emerging hardware platforms including \n",
    "specialized AI accelerators and edge devices. This includes optimization for quantization, pruning, and \n",
    "other model compression techniques that are essential for deployment in resource-constrained environments.\n",
    "\n",
    "The competition between these frameworks has ultimately benefited the entire machine learning community, \n",
    "driving innovation and performance improvements that make it easier to train and deploy sophisticated \n",
    "models across a wide range of applications and use cases.\"\"\"\n",
    "\n",
    "test_question = \"What are the key performance improvements in recent deep learning frameworks?\"\n",
    "\n",
    "print(\"ðŸ§ª CONTEXT MANAGEMENT TESTING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Analyze original context\n",
    "original_analysis = context_manager.estimate_context_usage('basic_rag', long_context, test_question)\n",
    "print(f\"\\nðŸ“Š Original Context Analysis:\")\n",
    "print(f\"   Total tokens: {original_analysis['total_tokens']}\")\n",
    "print(f\"   Context tokens: {original_analysis['context_tokens']} ({original_analysis['context_percentage']:.1f}%)\")\n",
    "print(f\"   Fits in limit: {original_analysis['fits_in_limit']}\")\n",
    "print(f\"   Tokens over limit: {original_analysis['tokens_over_limit']}\")\n",
    "\n",
    "# 2. Test different truncation strategies\n",
    "truncation_strategies = ['head', 'tail', 'middle', 'smart', 'summarize']\n",
    "\n",
    "print(f\"\\nðŸ”§ Testing Truncation Strategies:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "strategy_results = []\n",
    "\n",
    "for strategy in truncation_strategies:\n",
    "    optimization = context_manager.optimize_context(\n",
    "        'basic_rag', long_context, test_question, strategy=strategy\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{strategy.upper()} Strategy:\")\n",
    "    print(f\"   Original tokens: {optimization['original_tokens']}\")\n",
    "    print(f\"   Final tokens: {optimization['final_tokens']}\")\n",
    "    print(f\"   Tokens saved: {optimization['tokens_saved']}\")\n",
    "    print(f\"   Fits after optimization: {optimization['fits_after_optimization']}\")\n",
    "    print(f\"   Preview: {optimization['optimized_context'][:100]}...\")\n",
    "    \n",
    "    strategy_results.append({\n",
    "        'Strategy': strategy.title(),\n",
    "        'Original Tokens': optimization['original_tokens'],\n",
    "        'Final Tokens': optimization['final_tokens'],\n",
    "        'Tokens Saved': optimization['tokens_saved'],\n",
    "        'Reduction %': (optimization['tokens_saved'] / optimization['original_tokens']) * 100,\n",
    "        'Fits Limit': optimization['fits_after_optimization']\n",
    "    })\n",
    "\n",
    "# 3. Compare strategies\n",
    "strategy_df = pd.DataFrame(strategy_results)\n",
    "print(f\"\\nðŸ“‹ Strategy Comparison:\")\n",
    "print(strategy_df.to_string(index=False, float_format='%.1f'))\n",
    "\n",
    "# 4. Context quality analysis\n",
    "quality_analysis = context_manager.analyze_context_quality(long_context, test_question)\n",
    "print(f\"\\nðŸŽ¯ Context Quality Analysis:\")\n",
    "print(f\"   Word overlap ratio: {quality_analysis['word_overlap_ratio']:.3f}\")\n",
    "print(f\"   Lexical diversity: {quality_analysis['lexical_diversity']:.3f}\")\n",
    "print(f\"   Quality score: {quality_analysis['quality_score']:.3f}\")\n",
    "print(f\"   Readability: {quality_analysis['readability']:.1f}\")\n",
    "print(f\"   Sentence count: {quality_analysis['sentence_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ›ï¸ Exercise 3: Model-Specific Prompt Optimization\n",
    "\n",
    "Let's create model-specific prompt adaptations for different LLM providers and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSpecificOptimizer:\n",
    "    \"\"\"Model-specific prompt optimization for different LLM providers\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model_configurations = {\n",
    "            'gpt-4': {\n",
    "                'provider': 'OpenAI',\n",
    "                'context_window': 128000,\n",
    "                'strengths': ['reasoning', 'code', 'analysis'],\n",
    "                'prompt_style': 'detailed_instructions',\n",
    "                'system_message_support': True,\n",
    "                'preferred_format': 'structured',\n",
    "                'citation_style': 'bracketed_numbers'\n",
    "            },\n",
    "            'gpt-3.5-turbo': {\n",
    "                'provider': 'OpenAI',\n",
    "                'context_window': 16385,\n",
    "                'strengths': ['conversation', 'general_knowledge'],\n",
    "                'prompt_style': 'concise_instructions',\n",
    "                'system_message_support': True,\n",
    "                'preferred_format': 'conversational',\n",
    "                'citation_style': 'inline_references'\n",
    "            },\n",
    "            'claude-3-opus': {\n",
    "                'provider': 'Anthropic',\n",
    "                'context_window': 200000,\n",
    "                'strengths': ['analysis', 'writing', 'safety'],\n",
    "                'prompt_style': 'thinking_aloud',\n",
    "                'system_message_support': True,\n",
    "                'preferred_format': 'structured_thinking',\n",
    "                'citation_style': 'detailed_sources'\n",
    "            },\n",
    "            'claude-3-sonnet': {\n",
    "                'provider': 'Anthropic',\n",
    "                'context_window': 200000,\n",
    "                'strengths': ['balanced', 'efficient'],\n",
    "                'prompt_style': 'efficient_instructions',\n",
    "                'system_message_support': True,\n",
    "                'preferred_format': 'structured',\n",
    "                'citation_style': 'bracketed_numbers'\n",
    "            },\n",
    "            'gemini-pro': {\n",
    "                'provider': 'Google',\n",
    "                'context_window': 2000000,\n",
    "                'strengths': ['large_context', 'multimodal'],\n",
    "                'prompt_style': 'comprehensive_context',\n",
    "                'system_message_support': False,\n",
    "                'preferred_format': 'detailed_analysis',\n",
    "                'citation_style': 'source_blocks'\n",
    "            },\n",
    "            'llama-2-70b': {\n",
    "                'provider': 'Meta',\n",
    "                'context_window': 4096,\n",
    "                'strengths': ['open_source', 'customizable'],\n",
    "                'prompt_style': 'explicit_instructions',\n",
    "                'system_message_support': True,\n",
    "                'preferred_format': 'step_by_step',\n",
    "                'citation_style': 'simple_references'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.model_specific_templates = self._create_model_templates()\n",
    "    \n",
    "    def _create_model_templates(self) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Create model-specific prompt templates\"\"\"\n",
    "        templates = {}\n",
    "        \n",
    "        # GPT-4 template (detailed, structured)\n",
    "        templates['gpt-4'] = {\n",
    "            'system': '''You are an expert research assistant with access to relevant documents. Your task is to provide accurate, well-reasoned answers based on the provided context.\n",
    "\n",
    "Key requirements:\n",
    "1. Base your answer primarily on the provided context\n",
    "2. Use logical reasoning to synthesize information\n",
    "3. Cite sources using [1], [2], etc. format\n",
    "4. Acknowledge limitations when context is insufficient\n",
    "5. Provide detailed, comprehensive responses''',\n",
    "            \n",
    "            'user': '''Context Sources:\n",
    "{% for doc in context_sources %}\n",
    "[{{ loop.index }}] {{ doc.title or \"Document \" + loop.index|string }}\n",
    "{{ doc.content }}\n",
    "\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "\n",
    "Please provide a comprehensive answer based on the context above.'''\n",
    "        }\n",
    "        \n",
    "        # GPT-3.5 template (concise, conversational)\n",
    "        templates['gpt-3.5-turbo'] = {\n",
    "            'system': '''You are a helpful assistant that answers questions using provided context. Be concise but informative.''',\n",
    "            \n",
    "            'user': '''Context:\n",
    "{{ context }}\n",
    "\n",
    "Question: {{ question }}\n",
    "\n",
    "Answer based on the context above, and mention your sources when relevant.'''\n",
    "        }\n",
    "        \n",
    "        # Claude-3-Opus template (thinking aloud)\n",
    "        templates['claude-3-opus'] = {\n",
    "            'system': '''You are Claude, an AI assistant created by Anthropic. You think carefully and provide well-reasoned responses based on given context.''',\n",
    "            \n",
    "            'user': '''I have some relevant documents that may help answer a question. Let me share them first:\n",
    "\n",
    "<documents>\n",
    "{% for doc in context_sources %}\n",
    "<document id=\"{{ loop.index }}\">\n",
    "{{ doc.content }}\n",
    "</document>\n",
    "{% endfor %}\n",
    "</documents>\n",
    "\n",
    "Now, here's my question: {{ question }}\n",
    "\n",
    "Please think through this step by step, referencing the relevant documents as needed.'''\n",
    "        }\n",
    "        \n",
    "        # Gemini Pro template (comprehensive context)\n",
    "        templates['gemini-pro'] = {\n",
    "            'user': '''Given the following comprehensive context, please answer the question with detailed analysis:\n",
    "\n",
    "=== CONTEXT START ===\n",
    "{{ context }}\n",
    "=== CONTEXT END ===\n",
    "\n",
    "QUESTION: {{ question }}\n",
    "\n",
    "ANALYSIS FRAMEWORK:\n",
    "1. Key information from context\n",
    "2. Logical connections and reasoning\n",
    "3. Comprehensive answer with evidence\n",
    "4. Source attribution\n",
    "\n",
    "Please provide your detailed analysis:'''\n",
    "        }\n",
    "        \n",
    "        # Llama-2 template (explicit instructions)\n",
    "        templates['llama-2-70b'] = {\n",
    "            'system': '''<s>[INST] <<SYS>>\n",
    "You are a helpful assistant. Answer questions based on the provided context. Follow these steps:\n",
    "1. Read the context carefully\n",
    "2. Find information relevant to the question\n",
    "3. Provide a clear answer\n",
    "4. Mention your sources\n",
    "<</SYS>>''',\n",
    "            \n",
    "            'user': '''Context:\n",
    "{{ context }}\n",
    "\n",
    "Question: {{ question }}\n",
    "\n",
    "Step-by-step answer: [/INST]'''\n",
    "        }\n",
    "        \n",
    "        return templates\n",
    "    \n",
    "    def get_model_info(self, model_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Get information about a specific model\"\"\"\n",
    "        model_key = self._normalize_model_name(model_name)\n",
    "        return self.model_configurations.get(model_key, {})\n",
    "    \n",
    "    def _normalize_model_name(self, model_name: str) -> str:\n",
    "        \"\"\"Normalize model name to match our configurations\"\"\"\n",
    "        model_name = model_name.lower()\n",
    "        \n",
    "        # Handle common variations\n",
    "        if 'gpt-4' in model_name:\n",
    "            return 'gpt-4'\n",
    "        elif 'gpt-3.5' in model_name:\n",
    "            return 'gpt-3.5-turbo'\n",
    "        elif 'claude-3-opus' in model_name:\n",
    "            return 'claude-3-opus'\n",
    "        elif 'claude-3-sonnet' in model_name:\n",
    "            return 'claude-3-sonnet'\n",
    "        elif 'gemini' in model_name:\n",
    "            return 'gemini-pro'\n",
    "        elif 'llama' in model_name:\n",
    "            return 'llama-2-70b'\n",
    "        \n",
    "        return model_name\n",
    "    \n",
    "    def optimize_for_model(self, model_name: str, context: str, question: str, \n",
    "                          context_sources: List[Dict] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Create model-optimized prompt\"\"\"\n",
    "        model_key = self._normalize_model_name(model_name)\n",
    "        model_config = self.model_configurations.get(model_key, {})\n",
    "        \n",
    "        if not model_config:\n",
    "            return {\n",
    "                'error': f'Model {model_name} not supported',\n",
    "                'supported_models': list(self.model_configurations.keys())\n",
    "            }\n",
    "        \n",
    "        # Get model-specific template\n",
    "        templates = self.model_specific_templates.get(model_key, {})\n",
    "        \n",
    "        # Prepare context based on model preferences\n",
    "        if context_sources:\n",
    "            prepared_context = context_sources\n",
    "        else:\n",
    "            # Convert single context to sources format\n",
    "            prepared_context = [{'content': context, 'title': 'Source Document'}]\n",
    "        \n",
    "        # Check context length against model limits\n",
    "        estimated_tokens = len(context.split()) * 1.3  # Rough estimate\n",
    "        context_limit = model_config.get('context_window', 4096)\n",
    "        \n",
    "        if estimated_tokens > context_limit * 0.8:  # Use 80% of limit\n",
    "            # Context truncation needed\n",
    "            target_tokens = int(context_limit * 0.6)  # Leave room for response\n",
    "            truncated_context = self._truncate_for_model(context, target_tokens, model_config)\n",
    "        else:\n",
    "            truncated_context = context\n",
    "        \n",
    "        # Render template\n",
    "        rendered_prompt = {}\n",
    "        \n",
    "        if 'system' in templates and model_config.get('system_message_support', True):\n",
    "            # Render system message\n",
    "            system_template = Template(templates['system'])\n",
    "            rendered_prompt['system'] = system_template.render(\n",
    "                context=truncated_context,\n",
    "                question=question,\n",
    "                context_sources=prepared_context\n",
    "            )\n",
    "        \n",
    "        if 'user' in templates:\n",
    "            # Render user message\n",
    "            user_template = Template(templates['user'])\n",
    "            rendered_prompt['user'] = user_template.render(\n",
    "                context=truncated_context,\n",
    "                question=question,\n",
    "                context_sources=prepared_context\n",
    "            )\n",
    "        \n",
    "        # Calculate final token usage\n",
    "        total_prompt = '\\n\\n'.join(rendered_prompt.values())\n",
    "        estimated_prompt_tokens = len(total_prompt.split()) * 1.3\n",
    "        \n",
    "        return {\n",
    "            'model': model_name,\n",
    "            'model_config': model_config,\n",
    "            'rendered_prompt': rendered_prompt,\n",
    "            'prompt_optimization': {\n",
    "                'original_context_length': len(context),\n",
    "                'truncated_context_length': len(truncated_context),\n",
    "                'estimated_prompt_tokens': int(estimated_prompt_tokens),\n",
    "                'context_window_utilization': estimated_prompt_tokens / context_limit,\n",
    "                'truncation_applied': len(truncated_context) < len(context)\n",
    "            },\n",
    "            'recommendations': self._get_model_recommendations(model_config)\n",
    "        }\n",
    "    \n",
    "    def _truncate_for_model(self, context: str, target_tokens: int, model_config: Dict) -> str:\n",
    "        \"\"\"Model-specific context truncation\"\"\"\n",
    "        prompt_style = model_config.get('prompt_style', 'detailed_instructions')\n",
    "        \n",
    "        if prompt_style == 'comprehensive_context':\n",
    "            # For models like Gemini with large context windows, preserve more\n",
    "            return context  # Don't truncate for large context models\n",
    "        elif prompt_style == 'thinking_aloud':\n",
    "            # For Claude, preserve complete thoughts\n",
    "            sentences = context.split('. ')\n",
    "            truncated = \"\"\n",
    "            for sentence in sentences:\n",
    "                if len(truncated.split()) * 1.3 < target_tokens:\n",
    "                    truncated += sentence + \". \"\n",
    "                else:\n",
    "                    break\n",
    "            return truncated.strip()\n",
    "        else:\n",
    "            # Standard truncation\n",
    "            words = context.split()\n",
    "            target_words = int(target_tokens / 1.3)\n",
    "            return ' '.join(words[:target_words])\n",
    "    \n",
    "    def _get_model_recommendations(self, model_config: Dict) -> List[str]:\n",
    "        \"\"\"Get model-specific recommendations\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if model_config.get('context_window', 0) < 16000:\n",
    "            recommendations.append(\"Consider context truncation for long documents\")\n",
    "        \n",
    "        if 'reasoning' in model_config.get('strengths', []):\n",
    "            recommendations.append(\"This model excels at complex reasoning tasks\")\n",
    "        \n",
    "        if not model_config.get('system_message_support', True):\n",
    "            recommendations.append(\"Include instructions in the user message (no system message support)\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    def compare_models(self, context: str, question: str) -> pd.DataFrame:\n",
    "        \"\"\"Compare prompt optimization across different models\"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for model_name in self.model_configurations.keys():\n",
    "            optimization = self.optimize_for_model(model_name, context, question)\n",
    "            \n",
    "            if 'error' not in optimization:\n",
    "                prompt_opt = optimization['prompt_optimization']\n",
    "                model_config = optimization['model_config']\n",
    "                \n",
    "                comparison_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Provider': model_config['provider'],\n",
    "                    'Context Window': f\"{model_config['context_window']:,}\",\n",
    "                    'Estimated Tokens': int(prompt_opt['estimated_prompt_tokens']),\n",
    "                    'Window Utilization': f\"{prompt_opt['context_window_utilization']:.1%}\",\n",
    "                    'Truncation Applied': prompt_opt['truncation_applied'],\n",
    "                    'Strengths': ', '.join(model_config.get('strengths', [])),\n",
    "                    'System Message': model_config.get('system_message_support', False)\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(comparison_data)\n",
    "\n",
    "# Initialize model-specific optimizer\n",
    "model_optimizer = ModelSpecificOptimizer()\n",
    "print(\"ðŸŽ›ï¸ Model-Specific Optimizer initialized!\")\n",
    "print(f\"   Supported models: {len(model_optimizer.model_configurations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model-specific optimization\n",
    "test_context = \"\"\"Machine learning model optimization has become increasingly important as models grow larger \n",
    "and more complex. Recent advances in deep learning frameworks have focused on improving training efficiency \n",
    "through better memory management, optimized computation graphs, and hardware-specific optimizations.\n",
    "\n",
    "TensorFlow 2.14.0 introduces GPU kernel optimizations that can reduce training time by up to 35% for \n",
    "transformer models. The improvements come from better memory management and graph optimization techniques.\n",
    "\n",
    "PyTorch 2.1 offers torch.compile() which provides up to 60% speedup for inference through compiler \n",
    "optimizations and hardware-specific code generation. This feature represents a significant advancement \n",
    "in making PyTorch models more efficient in production environments.\"\"\"\n",
    "\n",
    "test_question = \"What are the recent performance improvements in machine learning frameworks?\"\n",
    "\n",
    "print(\"ðŸŽ›ï¸ MODEL-SPECIFIC OPTIMIZATION TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test optimization for different models\n",
    "test_models = ['gpt-4', 'claude-3-opus', 'gpt-3.5-turbo', 'gemini-pro']\n",
    "\n",
    "for model in test_models:\n",
    "    print(f\"\\nðŸ¤– {model.upper()} Optimization:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    optimization = model_optimizer.optimize_for_model(model, test_context, test_question)\n",
    "    \n",
    "    if 'error' in optimization:\n",
    "        print(f\"âŒ Error: {optimization['error']}\")\n",
    "        continue\n",
    "    \n",
    "    model_config = optimization['model_config']\n",
    "    prompt_opt = optimization['prompt_optimization']\n",
    "    rendered = optimization['rendered_prompt']\n",
    "    \n",
    "    print(f\"Provider: {model_config['provider']}\")\n",
    "    print(f\"Context Window: {model_config['context_window']:,} tokens\")\n",
    "    print(f\"Strengths: {', '.join(model_config['strengths'])}\")\n",
    "    print(f\"Estimated Prompt Tokens: {prompt_opt['estimated_prompt_tokens']}\")\n",
    "    print(f\"Window Utilization: {prompt_opt['context_window_utilization']:.1%}\")\n",
    "    \n",
    "    if 'system' in rendered:\n",
    "        print(f\"\\nSystem Message Preview:\")\n",
    "        print(f\"   {rendered['system'][:100]}...\")\n",
    "    \n",
    "    if 'user' in rendered:\n",
    "        print(f\"\\nUser Message Preview:\")\n",
    "        print(f\"   {rendered['user'][:100]}...\")\n",
    "    \n",
    "    if optimization['recommendations']:\n",
    "        print(f\"\\nðŸ’¡ Recommendations:\")\n",
    "        for rec in optimization['recommendations']:\n",
    "            print(f\"   - {rec}\")\n",
    "\n",
    "# Compare all models\n",
    "print(f\"\\n\\nðŸ“Š MODEL COMPARISON TABLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "comparison_df = model_optimizer.compare_models(test_context, test_question)\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize model characteristics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Context window sizes\n",
    "context_windows = [int(w.replace(',', '')) for w in comparison_df['Context Window']]\n",
    "models = comparison_df['Model']\n",
    "\n",
    "bars1 = ax1.barh(range(len(models)), context_windows)\n",
    "ax1.set_title('Context Window Sizes', fontweight='bold', fontsize=14)\n",
    "ax1.set_xlabel('Context Window (tokens)')\n",
    "ax1.set_yticks(range(len(models)))\n",
    "ax1.set_yticklabels(models)\n",
    "ax1.set_xscale('log')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Token utilization\n",
    "utilization = [float(u.rstrip('%')) for u in comparison_df['Window Utilization']]\n",
    "bars2 = ax2.bar(range(len(models)), utilization)\n",
    "ax2.set_title('Context Window Utilization', fontweight='bold', fontsize=14)\n",
    "ax2.set_ylabel('Utilization (%)')\n",
    "ax2.set_xticks(range(len(models)))\n",
    "ax2.set_xticklabels(models, rotation=45, ha='right')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Provider distribution\n",
    "provider_counts = comparison_df['Provider'].value_counts()\n",
    "ax3.pie(provider_counts.values, labels=provider_counts.index, autopct='%1.0f%%')\n",
    "ax3.set_title('Models by Provider', fontweight='bold', fontsize=14)\n",
    "\n",
    "# 4. System message support\n",
    "system_support = comparison_df['System Message'].value_counts()\n",
    "ax4.pie(system_support.values, labels=['Supported', 'Not Supported'], autopct='%1.0f%%')\n",
    "ax4.set_title('System Message Support', fontweight='bold', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Model Selection Guidelines:\")\n",
    "print(\"- GPT-4: Best for complex reasoning and analysis\")\n",
    "print(\"- Claude-3-Opus: Excellent for detailed thinking and safety\")\n",
    "print(\"- GPT-3.5-Turbo: Cost-effective for general conversations\")\n",
    "print(\"- Gemini Pro: Best for very long contexts (2M+ tokens)\")\n",
    "print(\"- Choose based on your specific use case and context length needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Exercise 4: Advanced RAG Prompting Techniques\n",
    "\n",
    "Let's implement advanced prompting techniques including few-shot learning, chain-of-thought, and attribution systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGPrompting:\n",
    "    \"\"\"Advanced RAG prompting techniques and patterns\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompt_library = prompt_library\n",
    "        self.few_shot_examples = self._create_few_shot_examples()\n",
    "        \n",
    "    def _create_few_shot_examples(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Create curated few-shot examples for different domains\"\"\"\n",
    "        return {\n",
    "            'technical_qa': [\n",
    "                {\n",
    "                    'context': 'TensorFlow 2.10 introduced tf.data.Dataset.batch() optimizations that improve throughput by 15% for large datasets.',\n",
    "                    'question': 'How much did TensorFlow 2.10 improve dataset performance?',\n",
    "                    'answer': 'According to the provided context, TensorFlow 2.10 introduced optimizations to tf.data.Dataset.batch() that improved throughput by 15% specifically for large datasets.'\n",
    "                },\n",
    "                {\n",
    "                    'context': 'The new API endpoint /v2/predict requires authentication headers and returns JSON responses with prediction scores and confidence intervals.',\n",
    "                    'question': 'What does the v2 predict API return?',\n",
    "                    'answer': 'The /v2/predict API endpoint returns JSON responses containing prediction scores and confidence intervals. Note that it requires authentication headers for access.'\n",
    "                }\n",
    "            ],\n",
    "            'business_analysis': [\n",
    "                {\n",
    "                    'context': 'Q3 revenue increased 22% year-over-year, driven primarily by enterprise sales which grew 35% while consumer sales remained flat.',\n",
    "                    'question': 'What drove the revenue growth in Q3?',\n",
    "                    'answer': 'Q3 revenue growth of 22% year-over-year was primarily driven by strong enterprise sales, which grew 35%. Consumer sales remained flat during this period, highlighting the enterprise segment as the key growth driver.'\n",
    "                }\n",
    "            ],\n",
    "            'research_synthesis': [\n",
    "                {\n",
    "                    'context': 'Study A found that method X improved accuracy by 12% on dataset Y. Study B reported 8% improvement using the same method on dataset Z.',\n",
    "                    'question': 'How effective is method X across different datasets?',\n",
    "                    'answer': 'Based on the provided studies, method X shows consistent effectiveness across different datasets: Study A reported 12% accuracy improvement on dataset Y, while Study B found 8% improvement on dataset Z. This suggests method X is generally effective, though performance may vary by dataset characteristics.'\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def create_chain_of_thought_prompt(self, context: str, question: str, \n",
    "                                     domain: str = 'general') -> str:\n",
    "        \"\"\"Create chain-of-thought prompt for complex reasoning\"\"\"\n",
    "        \n",
    "        domain_instructions = {\n",
    "            'technical': 'Focus on technical accuracy, implementation details, and potential trade-offs.',\n",
    "            'business': 'Consider business impact, stakeholder perspectives, and strategic implications.',\n",
    "            'research': 'Evaluate evidence quality, methodology, and limitations of conclusions.',\n",
    "            'general': 'Think through the problem systematically and consider multiple perspectives.'\n",
    "        }\n",
    "        \n",
    "        instruction = domain_instructions.get(domain, domain_instructions['general'])\n",
    "        \n",
    "        cot_template = f\"\"\"You are an expert analyst. {instruction}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "Please think through this step-by-step:\n",
    "\n",
    "STEP 1 - INFORMATION EXTRACTION:\n",
    "What are the key facts and data points from the context that relate to this question?\n",
    "\n",
    "STEP 2 - ANALYSIS:\n",
    "How do these facts connect to answer the question? What patterns or relationships do you see?\n",
    "\n",
    "STEP 3 - REASONING:\n",
    "What logical conclusions can you draw? What assumptions are you making?\n",
    "\n",
    "STEP 4 - SYNTHESIS:\n",
    "Based on your analysis, what is the complete answer to the question?\n",
    "\n",
    "STEP 5 - CONFIDENCE AND LIMITATIONS:\n",
    "How confident are you in this answer? What information would strengthen your conclusion?\n",
    "\n",
    "Let me work through this systematically:\"\"\"\n",
    "        \n",
    "        return cot_template\n",
    "    \n",
    "    def create_few_shot_prompt(self, context: str, question: str, \n",
    "                              domain: str = 'technical_qa', num_examples: int = 2) -> str:\n",
    "        \"\"\"Create few-shot prompt with domain-specific examples\"\"\"\n",
    "        \n",
    "        if domain not in self.few_shot_examples:\n",
    "            domain = 'technical_qa'\n",
    "        \n",
    "        examples = self.few_shot_examples[domain][:num_examples]\n",
    "        \n",
    "        few_shot_prompt = \"\"\"You are an expert assistant that provides accurate answers based on context. Here are examples of how to respond:\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add examples\n",
    "        for i, example in enumerate(examples, 1):\n",
    "            few_shot_prompt += f\"\"\"EXAMPLE {i}:\n",
    "Context: {example['context']}\n",
    "Question: {example['question']}\n",
    "Answer: {example['answer']}\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        # Add current query\n",
    "        few_shot_prompt += f\"\"\"Now answer this question following the same pattern:\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        return few_shot_prompt\n",
    "    \n",
    "    def create_attribution_prompt(self, context_sources: List[Dict[str, Any]], \n",
    "                                question: str, citation_style: str = 'academic') -> str:\n",
    "        \"\"\"Create prompt with strong attribution requirements\"\"\"\n",
    "        \n",
    "        citation_styles = {\n",
    "            'academic': 'Use [Source 1], [Source 2] format and include page numbers if available',\n",
    "            'journalistic': 'Use \"according to [Source Name]\" format',\n",
    "            'legal': 'Use precise citations with document names and section numbers',\n",
    "            'medical': 'Include study details and confidence levels with citations'\n",
    "        }\n",
    "        \n",
    "        citation_instruction = citation_styles.get(citation_style, citation_styles['academic'])\n",
    "        \n",
    "        # Build sources section\n",
    "        sources_text = \"\"\n",
    "        for i, source in enumerate(context_sources, 1):\n",
    "            source_header = f\"SOURCE {i}\"\n",
    "            if 'title' in source:\n",
    "                source_header += f\" - {source['title']}\"\n",
    "            if 'date' in source:\n",
    "                source_header += f\" ({source['date']})\"\n",
    "            \n",
    "            sources_text += f\"{source_header}:\\n{source['content']}\\n\\n\"\n",
    "        \n",
    "        attribution_template = f\"\"\"You are a research assistant that provides meticulously cited answers. Accuracy and proper attribution are critical.\n",
    "\n",
    "SOURCES:\n",
    "{sources_text}\n",
    "QUESTION: {question}\n",
    "\n",
    "ATTRIBUTION REQUIREMENTS:\n",
    "1. {citation_instruction}\n",
    "2. Every factual claim must be attributed to a source\n",
    "3. If sources conflict, acknowledge the disagreement\n",
    "4. If information is not in sources, explicitly state this\n",
    "5. Distinguish between direct quotes and paraphrases\n",
    "\n",
    "FORMAT:\n",
    "- Main Answer: [Your response with inline citations]\n",
    "- Source Summary: [Brief note on which sources were most relevant]\n",
    "- Confidence Level: [High/Medium/Low based on source quality and agreement]\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "        \n",
    "        return attribution_template\n",
    "    \n",
    "    def create_conversational_rag_prompt(self, context: str, question: str, \n",
    "                                       conversation_history: List[Dict[str, str]], \n",
    "                                       max_history: int = 5) -> str:\n",
    "        \"\"\"Create conversational RAG prompt with history\"\"\"\n",
    "        \n",
    "        # Limit conversation history\n",
    "        recent_history = conversation_history[-max_history:] if len(conversation_history) > max_history else conversation_history\n",
    "        \n",
    "        # Build conversation history section\n",
    "        history_text = \"\"\n",
    "        for turn in recent_history:\n",
    "            role = turn.get('role', 'user').upper()\n",
    "            content = turn.get('content', '')\n",
    "            history_text += f\"{role}: {content}\\n\"\n",
    "        \n",
    "        conversational_template = f\"\"\"You are a helpful assistant in an ongoing conversation. Use the provided context to answer questions, but also consider the conversation history to maintain continuity and reference previous topics when relevant.\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\n",
    "RECENT CONVERSATION:\n",
    "{history_text}\n",
    "CURRENT QUESTION: {question}\n",
    "\n",
    "CONVERSATION GUIDELINES:\n",
    "1. Reference previous parts of our conversation when relevant\n",
    "2. Build on previously established information\n",
    "3. Maintain a natural conversational flow\n",
    "4. Use the context to provide accurate information\n",
    "5. Ask follow-up questions if clarification would be helpful\n",
    "\n",
    "RESPONSE:\"\"\"\n",
    "        \n",
    "        return conversational_template\n",
    "    \n",
    "    def create_multi_document_synthesis_prompt(self, document_sources: List[Dict[str, Any]], \n",
    "                                             question: str, synthesis_type: str = 'comparative') -> str:\n",
    "        \"\"\"Create prompt for synthesizing information across multiple documents\"\"\"\n",
    "        \n",
    "        synthesis_instructions = {\n",
    "            'comparative': {\n",
    "                'instruction': 'Compare and contrast the information across sources',\n",
    "                'structure': ['SIMILARITIES', 'DIFFERENCES', 'UNIQUE INSIGHTS', 'SYNTHESIS']\n",
    "            },\n",
    "            'chronological': {\n",
    "                'instruction': 'Organize information chronologically to show development over time',\n",
    "                'structure': ['TIMELINE', 'KEY DEVELOPMENTS', 'CURRENT STATE', 'IMPLICATIONS']\n",
    "            },\n",
    "            'thematic': {\n",
    "                'instruction': 'Organize information by themes and topics',\n",
    "                'structure': ['MAIN THEMES', 'SUPPORTING EVIDENCE', 'CONNECTIONS', 'CONCLUSIONS']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        synthesis_config = synthesis_instructions.get(synthesis_type, synthesis_instructions['comparative'])\n",
    "        \n",
    "        # Build document sources section\n",
    "        documents_text = \"\"\n",
    "        for i, doc in enumerate(document_sources, 1):\n",
    "            doc_header = f\"DOCUMENT {i}\"\n",
    "            if 'title' in doc:\n",
    "                doc_header += f\": {doc['title']}\"\n",
    "            if 'metadata' in doc:\n",
    "                metadata = doc['metadata']\n",
    "                if 'date' in metadata:\n",
    "                    doc_header += f\" ({metadata['date']})\"\n",
    "                if 'source' in metadata:\n",
    "                    doc_header += f\" - {metadata['source']}\"\n",
    "            \n",
    "            documents_text += f\"{doc_header}:\\n{doc['content']}\\n\\n\"\n",
    "        \n",
    "        # Build structure section\n",
    "        structure_text = \"\"\n",
    "        for section in synthesis_config['structure']:\n",
    "            structure_text += f\"{section}:\\n[Address this aspect based on the documents]\\n\\n\"\n",
    "        \n",
    "        synthesis_template = f\"\"\"You are an expert analyst specializing in multi-document synthesis. {synthesis_config['instruction']}.\n",
    "\n",
    "DOCUMENTS TO ANALYZE:\n",
    "{documents_text}\n",
    "RESEARCH QUESTION: {question}\n",
    "\n",
    "ANALYSIS FRAMEWORK:\n",
    "{structure_text}\n",
    "SYNTHESIS REQUIREMENTS:\n",
    "1. Draw connections between documents\n",
    "2. Identify patterns and relationships\n",
    "3. Note any conflicts or contradictions\n",
    "4. Provide evidence-based conclusions\n",
    "5. Cite specific documents for each claim\n",
    "\n",
    "COMPREHENSIVE ANALYSIS:\"\"\"\n",
    "        \n",
    "        return synthesis_template\n",
    "    \n",
    "    def evaluate_prompt_effectiveness(self, prompt: str, expected_elements: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate prompt quality and completeness\"\"\"\n",
    "        \n",
    "        prompt_lower = prompt.lower()\n",
    "        \n",
    "        # Check for essential RAG elements\n",
    "        essential_elements = {\n",
    "            'context_section': any(word in prompt_lower for word in ['context', 'sources', 'documents']),\n",
    "            'question_section': 'question' in prompt_lower,\n",
    "            'instructions': any(word in prompt_lower for word in ['instructions', 'requirements', 'guidelines']),\n",
    "            'attribution_guidance': any(word in prompt_lower for word in ['cite', 'source', 'reference', 'attribution']),\n",
    "            'format_specification': any(word in prompt_lower for word in ['format', 'structure', 'answer:']),\n",
    "            'quality_controls': any(word in prompt_lower for word in ['if', 'when', 'ensure', 'avoid'])\n",
    "        }\n",
    "        \n",
    "        # Check for expected elements\n",
    "        expected_found = {element: element.lower() in prompt_lower for element in expected_elements}\n",
    "        \n",
    "        # Calculate scores\n",
    "        essential_score = sum(essential_elements.values()) / len(essential_elements)\n",
    "        expected_score = sum(expected_found.values()) / len(expected_found) if expected_found else 1.0\n",
    "        \n",
    "        # Analyze prompt characteristics\n",
    "        characteristics = {\n",
    "            'length': len(prompt),\n",
    "            'word_count': len(prompt.split()),\n",
    "            'sentence_count': len([s for s in prompt.split('.') if s.strip()]),\n",
    "            'has_examples': 'example' in prompt_lower,\n",
    "            'has_steps': bool(re.search(r'step \\d+|\\d+\\.|first|second|third', prompt_lower)),\n",
    "            'readability': textstat.flesch_reading_ease(prompt)\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'essential_elements': essential_elements,\n",
    "            'essential_score': essential_score,\n",
    "            'expected_elements': expected_found,\n",
    "            'expected_score': expected_score,\n",
    "            'overall_score': (essential_score + expected_score) / 2,\n",
    "            'characteristics': characteristics,\n",
    "            'recommendations': self._get_prompt_recommendations(essential_elements, characteristics)\n",
    "        }\n",
    "    \n",
    "    def _get_prompt_recommendations(self, essential_elements: Dict[str, bool], \n",
    "                                   characteristics: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Generate recommendations for improving prompt quality\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if not essential_elements['attribution_guidance']:\n",
    "            recommendations.append(\"Add citation requirements to ensure proper source attribution\")\n",
    "        \n",
    "        if not essential_elements['quality_controls']:\n",
    "            recommendations.append(\"Include fallback instructions for insufficient context\")\n",
    "        \n",
    "        if characteristics['readability'] < 30:\n",
    "            recommendations.append(\"Simplify language for better readability\")\n",
    "        \n",
    "        if characteristics['word_count'] > 500:\n",
    "            recommendations.append(\"Consider shortening prompt to reduce token usage\")\n",
    "        \n",
    "        if not characteristics['has_steps'] and characteristics['word_count'] > 200:\n",
    "            recommendations.append(\"Consider adding step-by-step structure for complex prompts\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# Initialize advanced RAG prompting\n",
    "advanced_prompting = AdvancedRAGPrompting()\n",
    "print(\"ðŸ§ª Advanced RAG Prompting initialized!\")\n",
    "print(f\"   Few-shot domains: {list(advanced_prompting.few_shot_examples.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced prompting techniques\n",
    "test_context = \"\"\"Recent research has shown significant improvements in large language model performance. \n",
    "Study A (Johnson et al., 2024) found that fine-tuning with human feedback improved accuracy by 23% \n",
    "on reasoning tasks. Study B (Chen et al., 2024) reported that constitutional AI methods reduced \n",
    "harmful outputs by 67% while maintaining helpfulness. Study C (Williams et al., 2024) demonstrated \n",
    "that multi-step reasoning approaches increased problem-solving success rates from 45% to 78%.\"\"\"\n",
    "\n",
    "test_question = \"What recent improvements have been made to large language models?\"\n",
    "\n",
    "print(\"ðŸ§ª ADVANCED RAG PROMPTING TECHNIQUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Chain-of-Thought Prompting\n",
    "print(\"\\n1ï¸âƒ£ Chain-of-Thought RAG:\")\n",
    "print(\"-\" * 30)\n",
    "cot_prompt = advanced_prompting.create_chain_of_thought_prompt(test_context, test_question, 'research')\n",
    "print(cot_prompt[:300] + \"...\")\n",
    "cot_eval = advanced_prompting.evaluate_prompt_effectiveness(cot_prompt, ['step', 'analysis', 'reasoning'])\n",
    "print(f\"Quality Score: {cot_eval['overall_score']:.2f}\")\n",
    "print(f\"Has Steps: {cot_eval['characteristics']['has_steps']}\")\n",
    "\n",
    "# 2. Few-Shot Prompting\n",
    "print(\"\\n2ï¸âƒ£ Few-Shot RAG:\")\n",
    "print(\"-\" * 20)\n",
    "few_shot_prompt = advanced_prompting.create_few_shot_prompt(test_context, test_question, 'research_synthesis')\n",
    "print(few_shot_prompt[:300] + \"...\")\n",
    "fs_eval = advanced_prompting.evaluate_prompt_effectiveness(few_shot_prompt, ['example', 'pattern'])\n",
    "print(f\"Quality Score: {fs_eval['overall_score']:.2f}\")\n",
    "print(f\"Has Examples: {fs_eval['characteristics']['has_examples']}\")\n",
    "\n",
    "# 3. Attribution-Focused Prompting\n",
    "print(\"\\n3ï¸âƒ£ Attribution RAG:\")\n",
    "print(\"-\" * 25)\n",
    "context_sources = [\n",
    "    {'title': 'Johnson et al. (2024)', 'content': 'Study A found that fine-tuning with human feedback improved accuracy by 23% on reasoning tasks.'},\n",
    "    {'title': 'Chen et al. (2024)', 'content': 'Study B reported that constitutional AI methods reduced harmful outputs by 67% while maintaining helpfulness.'},\n",
    "    {'title': 'Williams et al. (2024)', 'content': 'Study C demonstrated that multi-step reasoning approaches increased problem-solving success rates from 45% to 78%.'}\n",
    "]\n",
    "\n",
    "attribution_prompt = advanced_prompting.create_attribution_prompt(context_sources, test_question, 'academic')\n",
    "print(attribution_prompt[:300] + \"...\")\n",
    "attr_eval = advanced_prompting.evaluate_prompt_effectiveness(attribution_prompt, ['citation', 'source', 'attribution'])\n",
    "print(f\"Quality Score: {attr_eval['overall_score']:.2f}\")\n",
    "print(f\"Attribution Guidance: {attr_eval['essential_elements']['attribution_guidance']}\")\n",
    "\n",
    "# 4. Multi-Document Synthesis\n",
    "print(\"\\n4ï¸âƒ£ Multi-Document Synthesis:\")\n",
    "print(\"-\" * 35)\n",
    "document_sources = [\n",
    "    {\n",
    "        'title': 'Study A - Human Feedback',\n",
    "        'content': 'Fine-tuning with human feedback improved accuracy by 23% on reasoning tasks.',\n",
    "        'metadata': {'date': '2024-01', 'source': 'Johnson et al.'}\n",
    "    },\n",
    "    {\n",
    "        'title': 'Study B - Constitutional AI',\n",
    "        'content': 'Constitutional AI methods reduced harmful outputs by 67% while maintaining helpfulness.',\n",
    "        'metadata': {'date': '2024-02', 'source': 'Chen et al.'}\n",
    "    }\n",
    "]\n",
    "\n",
    "synthesis_prompt = advanced_prompting.create_multi_document_synthesis_prompt(\n",
    "    document_sources, test_question, 'comparative'\n",
    ")\n",
    "print(synthesis_prompt[:300] + \"...\")\n",
    "synth_eval = advanced_prompting.evaluate_prompt_effectiveness(synthesis_prompt, ['compare', 'synthesis', 'documents'])\n",
    "print(f\"Quality Score: {synth_eval['overall_score']:.2f}\")\n",
    "\n",
    "# 5. Conversational RAG\n",
    "print(\"\\n5ï¸âƒ£ Conversational RAG:\")\n",
    "print(\"-\" * 25)\n",
    "conversation_history = [\n",
    "    {'role': 'user', 'content': 'What are some recent AI developments?'},\n",
    "    {'role': 'assistant', 'content': 'Recent developments include improvements in reasoning and safety.'},\n",
    "    {'role': 'user', 'content': 'Can you be more specific about the reasoning improvements?'}\n",
    "]\n",
    "\n",
    "conversational_prompt = advanced_prompting.create_conversational_rag_prompt(\n",
    "    test_context, test_question, conversation_history\n",
    ")\n",
    "print(conversational_prompt[:300] + \"...\")\n",
    "conv_eval = advanced_prompting.evaluate_prompt_effectiveness(conversational_prompt, ['conversation', 'history', 'continuity'])\n",
    "print(f\"Quality Score: {conv_eval['overall_score']:.2f}\")\n",
    "\n",
    "# Compare all techniques\n",
    "print(\"\\n\\nðŸ“Š TECHNIQUE COMPARISON\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "techniques_comparison = pd.DataFrame([\n",
    "    {'Technique': 'Chain-of-Thought', 'Quality Score': cot_eval['overall_score'], 'Word Count': cot_eval['characteristics']['word_count'], 'Readability': cot_eval['characteristics']['readability']},\n",
    "    {'Technique': 'Few-Shot', 'Quality Score': fs_eval['overall_score'], 'Word Count': fs_eval['characteristics']['word_count'], 'Readability': fs_eval['characteristics']['readability']},\n",
    "    {'Technique': 'Attribution', 'Quality Score': attr_eval['overall_score'], 'Word Count': attr_eval['characteristics']['word_count'], 'Readability': attr_eval['characteristics']['readability']},\n",
    "    {'Technique': 'Synthesis', 'Quality Score': synth_eval['overall_score'], 'Word Count': synth_eval['characteristics']['word_count'], 'Readability': synth_eval['characteristics']['readability']},\n",
    "    {'Technique': 'Conversational', 'Quality Score': conv_eval['overall_score'], 'Word Count': conv_eval['characteristics']['word_count'], 'Readability': conv_eval['characteristics']['readability']}\n",
    "])\n",
    "\n",
    "print(techniques_comparison.to_string(index=False, float_format='%.2f'))\n",
    "\n",
    "# Show recommendations for improvement\n",
    "print(\"\\nðŸ’¡ IMPROVEMENT RECOMMENDATIONS:\")\n",
    "print(\"-\" * 35)\n",
    "for i, (name, eval_result) in enumerate([\n",
    "    ('Chain-of-Thought', cot_eval),\n",
    "    ('Few-Shot', fs_eval),\n",
    "    ('Attribution', attr_eval)\n",
    "], 1):\n",
    "    if eval_result['recommendations']:\n",
    "        print(f\"\\n{i}. {name}:\")\n",
    "        for rec in eval_result['recommendations']:\n",
    "            print(f\"   - {rec}\")\n",
    "    else:\n",
    "        print(f\"\\n{i}. {name}: No specific recommendations - good quality prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### ðŸ—ï¸ RAG Prompt Architecture:\n",
    "1. **System Context**: Role definition and behavior instructions\n",
    "2. **Retrieved Context**: Relevant documents with proper formatting\n",
    "3. **Task Instructions**: Specific guidance for using context\n",
    "4. **User Query**: The actual question or request\n",
    "5. **Output Format**: Structure and citation requirements\n",
    "6. **Quality Controls**: Fallback instructions and limitations\n",
    "\n",
    "### ðŸ“š Template Library Benefits:\n",
    "- **Consistency**: Standardized prompts across use cases\n",
    "- **Reusability**: Templated approach with variables\n",
    "- **Optimization**: Domain-specific adaptations\n",
    "- **Maintenance**: Centralized prompt management\n",
    "\n",
    "### ðŸ“ Context Management Strategies:\n",
    "\n",
    "#### Truncation Methods:\n",
    "- **Head Truncation**: Keep beginning (good for intros)\n",
    "- **Tail Truncation**: Keep ending (good for conclusions)\n",
    "- **Middle Truncation**: Keep beginning + end\n",
    "- **Smart Truncation**: Preserve complete sentences/paragraphs\n",
    "- **Summarization**: LLM-powered context compression\n",
    "\n",
    "#### Context Optimization:\n",
    "- **Hierarchical**: Most relevant information first\n",
    "- **Quality Analysis**: Measure relevance and diversity\n",
    "- **Token Management**: Stay within model limits\n",
    "- **Metadata Integration**: Rich context with source info\n",
    "\n",
    "### ðŸŽ›ï¸ Model-Specific Adaptations:\n",
    "\n",
    "| Model | Context Window | Best For | Prompt Style |\n",
    "|-------|---------------|----------|-------------|\n",
    "| **GPT-4** | 128K | Complex reasoning | Detailed instructions |\n",
    "| **Claude-3-Opus** | 200K | Analysis & safety | Thinking aloud |\n",
    "| **GPT-3.5-Turbo** | 16K | Conversations | Concise instructions |\n",
    "| **Gemini Pro** | 2M+ | Large contexts | Comprehensive analysis |\n",
    "| **Llama-2** | 4K | Open source | Explicit instructions |\n",
    "\n",
    "### ðŸ§ª Advanced Techniques Impact:\n",
    "\n",
    "#### Chain-of-Thought RAG:\n",
    "- **Impact**: +30% reasoning quality\n",
    "- **Best for**: Complex analysis, multi-step problems\n",
    "- **Structure**: Step-by-step thinking process\n",
    "\n",
    "#### Few-Shot RAG:\n",
    "- **Impact**: +20% format compliance\n",
    "- **Best for**: Specific output formats, style consistency\n",
    "- **Method**: Domain-specific examples\n",
    "\n",
    "#### Attribution Systems:\n",
    "- **Impact**: +40% source accuracy\n",
    "- **Best for**: Fact-sensitive domains, research\n",
    "- **Styles**: Academic, journalistic, legal, medical\n",
    "\n",
    "#### Multi-Document Synthesis:\n",
    "- **Types**: Comparative, chronological, thematic\n",
    "- **Best for**: Research synthesis, analysis across sources\n",
    "- **Structure**: Organized comparison and conclusions\n",
    "\n",
    "### ðŸ“Š Prompt Quality Framework:\n",
    "\n",
    "#### Essential Elements Checklist:\n",
    "- âœ… Context section clearly defined\n",
    "- âœ… Question/task specification\n",
    "- âœ… Clear instructions and requirements\n",
    "- âœ… Attribution guidance\n",
    "- âœ… Format specification\n",
    "- âœ… Quality controls and fallbacks\n",
    "\n",
    "#### Optimization Metrics:\n",
    "- **Token efficiency**: Context utilization vs limits\n",
    "- **Readability**: Flesch reading ease score\n",
    "- **Structure**: Step-by-step organization\n",
    "- **Completeness**: All required elements present\n",
    "\n",
    "### ðŸ”„ Prompt Engineering Workflow:\n",
    "1. **Define Use Case** â†’ 2. **Select Template** â†’ 3. **Adapt for Model** â†’ 4. **Optimize Context** â†’ 5. **Test & Iterate** â†’ 6. **Deploy & Monitor**\n",
    "\n",
    "### ðŸ’¡ Production Guidelines:\n",
    "- **Template Library**: Maintain centralized prompt templates\n",
    "- **Version Control**: Track prompt changes and performance\n",
    "- **A/B Testing**: Compare prompt variations systematically\n",
    "- **Model Adaptation**: Customize prompts for specific LLMs\n",
    "- **Context Management**: Implement smart truncation strategies\n",
    "- **Quality Monitoring**: Track output quality and citation accuracy\n",
    "\n",
    "## ðŸŽ¯ Next Steps\n",
    "\n",
    "In the next modules, we'll explore:\n",
    "- **Module 11**: LLM integration and model selection strategies\n",
    "- **Module 12**: Complete RAG system integration and deployment\n",
    "\n",
    "Mastering prompt engineering is crucial for maximizing the effectiveness of your RAG system and ensuring high-quality, reliable outputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤” Discussion Questions\n",
    "\n",
    "1. How would you adapt prompts for domain-specific applications (legal, medical, technical)?\n",
    "2. What strategies would you use to handle conflicting information in retrieved context?\n",
    "3. How would you balance prompt complexity with token efficiency?\n",
    "4. What role should user feedback play in prompt optimization?\n",
    "5. How would you ensure consistent citation quality across different models?\n",
    "\n",
    "## ðŸ“ Optional Exercises\n",
    "\n",
    "1. **Domain Adaptation**: Create specialized prompt templates for your industry\n",
    "2. **A/B Testing**: Design experiments to compare prompt variations\n",
    "3. **Real LLM Integration**: Connect templates to actual LLM APIs\n",
    "4. **Advanced Attribution**: Implement sophisticated citation systems\n",
    "5. **Conversational Memory**: Build multi-turn RAG conversations with persistent context"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}