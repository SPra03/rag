{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: Understanding Embedding Models\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Connect traditional ML embeddings to modern text embeddings\n",
    "- Understand different embedding model architectures and their trade-offs\n",
    "- Navigate the 2025 MTEB leaderboard to choose appropriate models\n",
    "- Compare embedding models hands-on using real text\n",
    "- Make informed decisions about model selection for production\n",
    "- Understand cost vs performance implications\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### From Traditional to Modern Embeddings üîÑ\n",
    "\n",
    "**If you've worked with ML, you likely know embeddings from:**\n",
    "- **Categorical encoding**: Converting categories to dense vectors\n",
    "- **Word2Vec**: Learning word relationships from context\n",
    "- **Matrix factorization**: Collaborative filtering embeddings\n",
    "\n",
    "**Modern text embeddings are similar but much more powerful:**\n",
    "- **Contextual**: Same word has different embeddings in different contexts\n",
    "- **Semantic**: Capture meaning, not just word co-occurrence\n",
    "- **Task-specific**: Optimized for similarity search, not just word prediction\n",
    "\n",
    "### Embedding Evolution Timeline üìà\n",
    "| Era | Approach | Example | Context |\n",
    "|-----|----------|---------|----------|\n",
    "| 2013 | Static Word Vectors | Word2Vec | One vector per word |\n",
    "| 2018 | Contextualized | BERT | Different vectors per context |\n",
    "| 2019 | Sentence-Level | Sentence-BERT | Optimized for sentences |\n",
    "| 2023 | Large-Scale | OpenAI text-embedding-3 | Billions of parameters |\n",
    "| 2025 | Specialized | NV-Embed-v2, Stella | Domain & task optimized |\n",
    "\n",
    "### 2025 MTEB Leaderboard Leaders üèÜ\n",
    "- **NV-Embed-v2**: 72.31 score (NVIDIA, Mistral-7B based)\n",
    "- **Stella-1.5B**: Best open-source with commercial license\n",
    "- **text-embedding-3-large**: OpenAI's flagship (64.6% MTEB)\n",
    "- **EmbeddingGemma**: Google's best under 500M parameters\n",
    "- **Voyage-3**: Strong commercial performance\n",
    "\n",
    "### Key Selection Criteria üéØ\n",
    "1. **Performance**: MTEB benchmark scores\n",
    "2. **Cost**: API pricing vs local hosting\n",
    "3. **Speed**: Inference latency and throughput\n",
    "4. **Domain fit**: General vs specialized models\n",
    "5. **Licensing**: Commercial use restrictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "Let's install the required packages for embedding experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers openai python-dotenv\n",
    "!pip install -q numpy matplotlib seaborn plotly\n",
    "!pip install -q scikit-learn umap-learn  # For visualization\n",
    "!pip install -q requests  # For API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import umap\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Embedding libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up API keys (optional - some models work without)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(\"üî¢ Ready to explore embedding models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìñ Exercise 1: Traditional vs Modern Embeddings Comparison\n",
    "\n",
    "Let's start by comparing traditional approaches with modern embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentences to demonstrate embedding concepts\n",
    "sample_sentences = [\n",
    "    \"The cat sat on the mat\",\n",
    "    \"A feline rested on the carpet\",  # Similar meaning, different words\n",
    "    \"The dog ran in the park\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"AI encompasses machine learning as one of its components\",  # Similar meaning\n",
    "    \"I love pizza with extra cheese\",\n",
    "    \"The stock market crashed yesterday\",\n",
    "    \"Python is a programming language\",\n",
    "    \"The snake slithered through the grass\",  # Same word \"Python\", different meaning\n",
    "    \"Financial markets experienced significant volatility\"  # Similar to stock market\n",
    "]\n",
    "\n",
    "print(\"üìù Sample sentences for embedding comparison:\")\n",
    "for i, sentence in enumerate(sample_sentences, 1):\n",
    "    print(f\"   {i:2d}. {sentence}\")\n",
    "\n",
    "print(f\"\\nüìä We'll compare how different embedding approaches handle these {len(sample_sentences)} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple bag-of-words approach (traditional)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def traditional_tfidf_embeddings(sentences):\n",
    "    \"\"\"\n",
    "    Create traditional TF-IDF embeddings for comparison\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    return tfidf_matrix.toarray(), vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get traditional embeddings\n",
    "print(\"üîÑ Computing traditional TF-IDF embeddings...\")\n",
    "tfidf_embeddings, feature_names = traditional_tfidf_embeddings(sample_sentences)\n",
    "\n",
    "print(f\"   Embedding dimensions: {tfidf_embeddings.shape[1]}\")\n",
    "print(f\"   Vocabulary size: {len(feature_names)}\")\n",
    "print(f\"   Sample features: {list(feature_names[:10])}\")\n",
    "\n",
    "# Show sparsity\n",
    "sparsity = (tfidf_embeddings == 0).mean()\n",
    "print(f\"   Sparsity: {sparsity:.1%} of values are zero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern sentence embedding model\n",
    "print(\"ü§ñ Loading modern embedding model...\")\n",
    "# Using a lightweight but effective model\n",
    "model_name = 'all-MiniLM-L6-v2'  # Fast, good performance\n",
    "modern_model = SentenceTransformer(model_name)\n",
    "\n",
    "print(f\"   Model: {model_name}\")\n",
    "print(f\"   Embedding dimensions: {modern_model.get_sentence_embedding_dimension()}\")\n",
    "\n",
    "# Get modern embeddings\n",
    "print(\"\\nüîÑ Computing modern sentence embeddings...\")\n",
    "modern_embeddings = modern_model.encode(sample_sentences)\n",
    "\n",
    "print(f\"   Shape: {modern_embeddings.shape}\")\n",
    "print(f\"   Dense: All {modern_embeddings.shape[1]} dimensions have values\")\n",
    "\n",
    "# Compare density\n",
    "print(f\"\\nüìä Embedding comparison:\")\n",
    "print(f\"   TF-IDF: {tfidf_embeddings.shape[1]} dims, {sparsity:.1%} sparse\")\n",
    "print(f\"   Modern: {modern_embeddings.shape[1]} dims, 100% dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare similarity between semantically similar sentences\n",
    "def compare_similarities(embeddings, method_name, sentences):\n",
    "    \"\"\"\n",
    "    Compare similarities between sentences using different embedding methods\n",
    "    \"\"\"\n",
    "    # Calculate similarity matrix\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    print(f\"\\nüîç {method_name} Similarity Analysis:\")\n",
    "    \n",
    "    # Look at specific pairs that should be similar\n",
    "    interesting_pairs = [\n",
    "        (0, 1),  # \"cat sat\" vs \"feline rested\"\n",
    "        (3, 4),  # \"ML is subset of AI\" vs \"AI encompasses ML\"\n",
    "        (6, 9),  # \"stock market crashed\" vs \"financial markets volatility\"\n",
    "        (7, 8),  # \"Python programming\" vs \"snake slithered\" (should be different!)\n",
    "    ]\n",
    "    \n",
    "    for i, j in interesting_pairs:\n",
    "        similarity = similarities[i][j]\n",
    "        print(f\"   '{sentences[i][:30]}...' vs '{sentences[j][:30]}...'\")\n",
    "        print(f\"   Similarity: {similarity:.3f}\")\n",
    "        print()\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "# Compare both methods\n",
    "tfidf_sim = compare_similarities(tfidf_embeddings, \"TF-IDF\", sample_sentences)\n",
    "modern_sim = compare_similarities(modern_embeddings, \"Modern Sentence Transformer\", sample_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# TF-IDF similarity heatmap\n",
    "sns.heatmap(tfidf_sim, annot=True, fmt='.2f', cmap='Blues', \n",
    "            ax=ax1, cbar_kws={'label': 'Cosine Similarity'})\n",
    "ax1.set_title('TF-IDF Similarity Matrix')\n",
    "ax1.set_xlabel('Sentence Index')\n",
    "ax1.set_ylabel('Sentence Index')\n",
    "\n",
    "# Modern embedding similarity heatmap\n",
    "sns.heatmap(modern_sim, annot=True, fmt='.2f', cmap='Reds',\n",
    "            ax=ax2, cbar_kws={'label': 'Cosine Similarity'})\n",
    "ax2.set_title('Modern Embedding Similarity Matrix')\n",
    "ax2.set_xlabel('Sentence Index')\n",
    "ax2.set_ylabel('Sentence Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìà Key Observations:\")\n",
    "print(\"   ‚Ä¢ Modern embeddings better capture semantic similarity\")\n",
    "print(\"   ‚Ä¢ TF-IDF focuses on word overlap, misses meaning\")\n",
    "print(\"   ‚Ä¢ Modern models handle synonyms and paraphrasing better\")\n",
    "print(\"   ‚Ä¢ Context matters: 'Python' programming vs 'snake' properly distinguished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Exercise 2: 2025 Model Comparison\n",
    "\n",
    "Let's compare different state-of-the-art embedding models available in 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to compare (using free/open-source models)\n",
    "models_to_test = {\n",
    "    'all-MiniLM-L6-v2': {\n",
    "        'description': 'Lightweight, fast, good general performance',\n",
    "        'dimensions': 384,\n",
    "        'mteb_score': 'Mid-tier',\n",
    "        'speed': 'Very Fast'\n",
    "    },\n",
    "    'all-mpnet-base-v2': {\n",
    "        'description': 'Higher quality, slightly slower',\n",
    "        'dimensions': 768, \n",
    "        'mteb_score': 'Good',\n",
    "        'speed': 'Fast'\n",
    "    },\n",
    "    'paraphrase-multilingual-MiniLM-L12-v2': {\n",
    "        'description': 'Multilingual support, good for diverse text',\n",
    "        'dimensions': 384,\n",
    "        'mteb_score': 'Good (multilingual)',\n",
    "        'speed': 'Fast'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üèÜ 2025 Embedding Models Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load models and compare performance\n",
    "loaded_models = {}\n",
    "model_embeddings = {}\n",
    "model_times = {}\n",
    "\n",
    "# Test sentences for comparison\n",
    "test_sentences = [\n",
    "    \"Artificial intelligence is transforming healthcare\",\n",
    "    \"AI technology revolutionizes medical treatment\",  # Similar meaning\n",
    "    \"The recipe calls for fresh basil and tomatoes\",    # Different domain\n",
    "    \"Machine learning algorithms require large datasets\",\n",
    "    \"Deep learning models need substantial training data\", # Similar meaning\n",
    "]\n",
    "\n",
    "for model_name, model_info in models_to_test.items():\n",
    "    print(f\"\\nü§ñ Loading {model_name}...\")\n",
    "    try:\n",
    "        # Load model\n",
    "        model = SentenceTransformer(model_name)\n",
    "        loaded_models[model_name] = model\n",
    "        \n",
    "        # Measure encoding time\n",
    "        start_time = time.time()\n",
    "        embeddings = model.encode(test_sentences)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        model_embeddings[model_name] = embeddings\n",
    "        model_times[model_name] = end_time - start_time\n",
    "        \n",
    "        print(f\"   ‚úÖ Loaded successfully\")\n",
    "        print(f\"   üìê Dimensions: {model_info['dimensions']}\")\n",
    "        print(f\"   ‚ö° Encoding time: {model_times[model_name]:.3f}s for {len(test_sentences)} sentences\")\n",
    "        print(f\"   üìä MTEB Score: {model_info['mteb_score']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Failed to load: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully loaded {len(loaded_models)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance on semantic similarity\n",
    "def evaluate_semantic_understanding(model_embeddings, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate how well a model captures semantic similarity\n",
    "    \"\"\"\n",
    "    if model_name not in model_embeddings:\n",
    "        return None\n",
    "        \n",
    "    embeddings = model_embeddings[model_name]\n",
    "    similarities = cosine_similarity(embeddings)\n",
    "    \n",
    "    # Check specific semantic pairs\n",
    "    ai_healthcare_similarity = similarities[0][1]  # AI healthcare vs AI medical\n",
    "    ml_data_similarity = similarities[3][4]        # ML datasets vs DL training data\n",
    "    unrelated_similarity = similarities[0][2]      # AI vs recipe (should be low)\n",
    "    \n",
    "    # Calculate semantic understanding score\n",
    "    semantic_score = (ai_healthcare_similarity + ml_data_similarity - unrelated_similarity) / 2\n",
    "    \n",
    "    return {\n",
    "        'semantic_score': semantic_score,\n",
    "        'ai_similarity': ai_healthcare_similarity,\n",
    "        'ml_similarity': ml_data_similarity,\n",
    "        'unrelated_similarity': unrelated_similarity\n",
    "    }\n",
    "\n",
    "print(\"üìä SEMANTIC UNDERSTANDING EVALUATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "for model_name in loaded_models.keys():\n",
    "    results = evaluate_semantic_understanding(model_embeddings, model_name)\n",
    "    if results:\n",
    "        model_performance[model_name] = results\n",
    "        \n",
    "        print(f\"\\nü§ñ {model_name}:\")\n",
    "        print(f\"   Semantic Score: {results['semantic_score']:.3f}\")\n",
    "        print(f\"   AI-Healthcare similarity: {results['ai_similarity']:.3f}\")\n",
    "        print(f\"   ML-Data similarity: {results['ml_similarity']:.3f}\")\n",
    "        print(f\"   Unrelated similarity: {results['unrelated_similarity']:.3f}\")\n",
    "        print(f\"   Encoding time: {model_times[model_name]:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "if model_performance:\n",
    "    # Prepare data for visualization\n",
    "    model_names = list(model_performance.keys())\n",
    "    semantic_scores = [model_performance[m]['semantic_score'] for m in model_names]\n",
    "    encoding_times = [model_times[m] for m in model_names]\n",
    "    dimensions = [models_to_test[m]['dimensions'] for m in model_names]\n",
    "    \n",
    "    # Create performance vs speed plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Semantic performance comparison\n",
    "    bars1 = ax1.bar(range(len(model_names)), semantic_scores, \n",
    "                    color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "    ax1.set_xlabel('Models')\n",
    "    ax1.set_ylabel('Semantic Understanding Score')\n",
    "    ax1.set_title('Model Semantic Performance')\n",
    "    ax1.set_xticks(range(len(model_names)))\n",
    "    ax1.set_xticklabels([name.split('-')[1] for name in model_names], rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars1, semantic_scores):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Performance vs Speed scatter plot\n",
    "    scatter = ax2.scatter(encoding_times, semantic_scores, \n",
    "                         s=[d/2 for d in dimensions],  # Size by dimensions\n",
    "                         c=range(len(model_names)), cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Add model labels\n",
    "    for i, name in enumerate(model_names):\n",
    "        ax2.annotate(name.split('-')[1], \n",
    "                    (encoding_times[i], semantic_scores[i]),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax2.set_xlabel('Encoding Time (seconds)')\n",
    "    ax2.set_ylabel('Semantic Understanding Score')\n",
    "    ax2.set_title('Performance vs Speed\\n(Bubble size = dimensions)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find best trade-off\n",
    "    # Calculate efficiency score (performance per unit time)\n",
    "    efficiency_scores = [(semantic_scores[i] / encoding_times[i], model_names[i]) \n",
    "                        for i in range(len(model_names))]\n",
    "    best_efficiency = max(efficiency_scores)\n",
    "    \n",
    "    print(f\"\\nüèÜ Best performance/speed trade-off: {best_efficiency[1]}\")\n",
    "    print(f\"   Efficiency score: {best_efficiency[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí∞ Exercise 3: Cost vs Performance Analysis\n",
    "\n",
    "Let's analyze the cost implications of different embedding approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding model cost analysis (2025 pricing)\n",
    "embedding_costs = {\n",
    "    'OpenAI text-embedding-3-large': {\n",
    "        'cost_per_1k_tokens': 0.00013,  # $0.13 per 1K tokens\n",
    "        'dimensions': 3072,\n",
    "        'mteb_score': 64.6,\n",
    "        'context_length': 8192,\n",
    "        'speed': 'Fast (API)',\n",
    "        'type': 'API'\n",
    "    },\n",
    "    'OpenAI text-embedding-3-small': {\n",
    "        'cost_per_1k_tokens': 0.00002,  # $0.02 per 1K tokens  \n",
    "        'dimensions': 1536,\n",
    "        'mteb_score': 62.3,\n",
    "        'context_length': 8192,\n",
    "        'speed': 'Very Fast (API)',\n",
    "        'type': 'API'\n",
    "    },\n",
    "    'all-mpnet-base-v2': {\n",
    "        'cost_per_1k_tokens': 0.0,  # Free (local)\n",
    "        'dimensions': 768,\n",
    "        'mteb_score': 57.8,\n",
    "        'context_length': 512,\n",
    "        'speed': 'Medium (local)',\n",
    "        'type': 'Local'\n",
    "    },\n",
    "    'all-MiniLM-L6-v2': {\n",
    "        'cost_per_1k_tokens': 0.0,  # Free (local)\n",
    "        'dimensions': 384,\n",
    "        'mteb_score': 56.3,\n",
    "        'context_length': 512,\n",
    "        'speed': 'Fast (local)',\n",
    "        'type': 'Local'\n",
    "    },\n",
    "    'Voyage-3-large': {\n",
    "        'cost_per_1k_tokens': 0.00013,  # Similar to OpenAI large\n",
    "        'dimensions': 1024,\n",
    "        'mteb_score': 68.0,  # Estimated\n",
    "        'context_length': 32000,\n",
    "        'speed': 'Fast (API)',\n",
    "        'type': 'API'\n",
    "    },\n",
    "    'Voyage-3-lite': {\n",
    "        'cost_per_1k_tokens': 0.000025,  # 1/5 of OpenAI large\n",
    "        'dimensions': 512,\n",
    "        'mteb_score': 64.0,  # Close to OpenAI large\n",
    "        'context_length': 32000,\n",
    "        'speed': 'Very Fast (API)',\n",
    "        'type': 'API'\n",
    "    }\n",
    "}\n",
    "\n",
    "def calculate_embedding_costs(models, document_count, avg_tokens_per_doc):\n",
    "    \"\"\"\n",
    "    Calculate costs for different embedding models\n",
    "    \"\"\"\n",
    "    total_tokens = document_count * avg_tokens_per_doc\n",
    "    \n",
    "    print(f\"üí∞ COST ANALYSIS FOR {document_count:,} DOCUMENTS\")\n",
    "    print(f\"üìä Average tokens per document: {avg_tokens_per_doc}\")\n",
    "    print(f\"üî¢ Total tokens to embed: {total_tokens:,}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    cost_analysis = {}\n",
    "    \n",
    "    for model_name, info in models.items():\n",
    "        # Calculate API costs\n",
    "        api_cost = (total_tokens / 1000) * info['cost_per_1k_tokens']\n",
    "        \n",
    "        # Estimate local costs (electricity, hardware amortization)\n",
    "        if info['type'] == 'Local':\n",
    "            # Rough estimate: $0.10/hour for local GPU, processing speed estimates\n",
    "            estimated_hours = total_tokens / 50000  # Rough throughput estimate\n",
    "            local_cost = estimated_hours * 0.10\n",
    "            total_cost = local_cost\n",
    "            cost_note = f\"(Local: ~${local_cost:.4f})\"\n",
    "        else:\n",
    "            total_cost = api_cost\n",
    "            cost_note = \"(API)\"\n",
    "        \n",
    "        cost_analysis[model_name] = {\n",
    "            'total_cost': total_cost,\n",
    "            'cost_per_1k_docs': (total_cost / document_count) * 1000,\n",
    "            'mteb_score': info['mteb_score'],\n",
    "            'dimensions': info['dimensions'],\n",
    "            'type': info['type']\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nü§ñ {model_name}:\")\n",
    "        print(f\"   Total cost: ${total_cost:.4f} {cost_note}\")\n",
    "        print(f\"   Cost per 1K docs: ${cost_analysis[model_name]['cost_per_1k_docs']:.4f}\")\n",
    "        print(f\"   MTEB score: {info['mteb_score']}\")\n",
    "        print(f\"   Quality/$ ratio: {info['mteb_score']/max(total_cost, 0.0001):.0f}\")\n",
    "    \n",
    "    return cost_analysis\n",
    "\n",
    "# Example scenarios\n",
    "scenarios = [\n",
    "    (1000, 200),      # Small: 1K docs, 200 tokens each\n",
    "    (50000, 300),     # Medium: 50K docs, 300 tokens each  \n",
    "    (1000000, 400),   # Large: 1M docs, 400 tokens each\n",
    "]\n",
    "\n",
    "for doc_count, avg_tokens in scenarios:\n",
    "    cost_analysis = calculate_embedding_costs(embedding_costs, doc_count, avg_tokens)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost vs performance trade-offs\n",
    "def create_cost_performance_chart(embedding_costs, scenario_docs=50000, scenario_tokens=300):\n",
    "    \"\"\"\n",
    "    Create interactive chart showing cost vs performance\n",
    "    \"\"\"\n",
    "    total_tokens = scenario_docs * scenario_tokens\n",
    "    \n",
    "    model_names = []\n",
    "    costs = []\n",
    "    performances = []\n",
    "    dimensions = []\n",
    "    types = []\n",
    "    \n",
    "    for model_name, info in embedding_costs.items():\n",
    "        # Calculate cost\n",
    "        if info['type'] == 'Local':\n",
    "            estimated_hours = total_tokens / 50000\n",
    "            cost = estimated_hours * 0.10\n",
    "        else:\n",
    "            cost = (total_tokens / 1000) * info['cost_per_1k_tokens']\n",
    "        \n",
    "        model_names.append(model_name)\n",
    "        costs.append(max(cost, 0.001))  # Minimum for log scale\n",
    "        performances.append(info['mteb_score'])\n",
    "        dimensions.append(info['dimensions'])\n",
    "        types.append(info['type'])\n",
    "    \n",
    "    # Create interactive plotly chart\n",
    "    fig = px.scatter(\n",
    "        x=costs,\n",
    "        y=performances,\n",
    "        size=dimensions,\n",
    "        color=types,\n",
    "        hover_name=model_names,\n",
    "        hover_data={\n",
    "            'Dimensions': dimensions,\n",
    "            'Cost ($)': ['${:.4f}'.format(c) for c in costs],\n",
    "            'MTEB Score': performances\n",
    "        },\n",
    "        title=f'Embedding Models: Cost vs Performance<br>Scenario: {scenario_docs:,} docs √ó {scenario_tokens} tokens',\n",
    "        labels={'x': 'Cost ($)', 'y': 'MTEB Performance Score'},\n",
    "        size_max=50\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(type='log', title='Cost ($, log scale)')\n",
    "    fig.update_layout(\n",
    "        width=800,\n",
    "        height=600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Add annotations for key insights\n",
    "    fig.add_annotation(\n",
    "        x=0.02, y=0.98,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        text=\"üéØ Top-right: High performance, high cost<br>üî• Bottom-left: Low performance, low cost<br>‚≠ê Top-left: Best value!\",\n",
    "        showarrow=False,\n",
    "        font=dict(size=10),\n",
    "        bgcolor=\"rgba(255,255,255,0.8)\",\n",
    "        bordercolor=\"black\",\n",
    "        borderwidth=1\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    # Calculate value score (performance per dollar)\n",
    "    value_scores = [(performances[i] / costs[i], model_names[i]) for i in range(len(model_names))]\n",
    "    best_value = max(value_scores)\n",
    "    \n",
    "    print(f\"\\nüíé Best value model: {best_value[1]}\")\n",
    "    print(f\"   Value score: {best_value[0]:.0f} MTEB points per dollar\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create the visualization\n",
    "cost_perf_chart = create_cost_performance_chart(embedding_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exercise 4: Domain-Specific Model Selection\n",
    "\n",
    "Let's explore how to choose models for different domains and use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-specific text samples\n",
    "domain_samples = {\n",
    "    'Technical Documentation': [\n",
    "        \"Configure the API endpoint with proper authentication headers\",\n",
    "        \"Initialize the database connection pool with connection pooling\",\n",
    "        \"Implement error handling for network timeouts and retries\"\n",
    "    ],\n",
    "    'Medical/Scientific': [\n",
    "        \"The patient presents with acute myocardial infarction symptoms\",\n",
    "        \"Administer 300mg aspirin followed by anticoagulation therapy\", \n",
    "        \"Monitor cardiac enzymes and troponin levels every 6 hours\"\n",
    "    ],\n",
    "    'Legal Documents': [\n",
    "        \"The defendant hereby waives all rights to appeal this decision\",\n",
    "        \"In accordance with section 1542 of the civil code provisions\",\n",
    "        \"This contract shall be governed by the laws of California\"\n",
    "    ],\n",
    "    'Business/Financial': [\n",
    "        \"Quarterly revenue increased by 15% year-over-year\",\n",
    "        \"EBITDA margins improved due to operational efficiency gains\",\n",
    "        \"Working capital requirements decreased through inventory optimization\"\n",
    "    ],\n",
    "    'General Knowledge': [\n",
    "        \"The weather forecast predicts rain this weekend\",\n",
    "        \"My favorite restaurant serves excellent Italian cuisine\",\n",
    "        \"The movie received positive reviews from critics\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def test_domain_performance(model, domain_samples):\n",
    "    \"\"\"\n",
    "    Test how well a model handles different domains\n",
    "    \"\"\"\n",
    "    domain_results = {}\n",
    "    \n",
    "    for domain, samples in domain_samples.items():\n",
    "        # Encode samples\n",
    "        embeddings = model.encode(samples)\n",
    "        \n",
    "        # Calculate intra-domain similarity (how similar are samples within domain)\n",
    "        similarities = cosine_similarity(embeddings)\n",
    "        \n",
    "        # Get average similarity (excluding diagonal)\n",
    "        mask = np.ones_like(similarities, dtype=bool)\n",
    "        np.fill_diagonal(mask, False)\n",
    "        avg_similarity = similarities[mask].mean()\n",
    "        \n",
    "        # Calculate embedding variance (how diverse are the embeddings)\n",
    "        embedding_variance = np.var(embeddings.mean(axis=0))\n",
    "        \n",
    "        domain_results[domain] = {\n",
    "            'avg_intra_similarity': avg_similarity,\n",
    "            'embedding_variance': embedding_variance,\n",
    "            'domain_coherence': avg_similarity  # Simple metric\n",
    "        }\n",
    "    \n",
    "    return domain_results\n",
    "\n",
    "print(\"üîç DOMAIN-SPECIFIC PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test available models on different domains\n",
    "for model_name, model in loaded_models.items():\n",
    "    print(f\"\\nü§ñ Testing {model_name}:\")\n",
    "    \n",
    "    domain_results = test_domain_performance(model, domain_samples)\n",
    "    \n",
    "    # Sort domains by coherence score\n",
    "    sorted_domains = sorted(domain_results.items(), \n",
    "                           key=lambda x: x[1]['domain_coherence'], reverse=True)\n",
    "    \n",
    "    print(f\"   Domain performance ranking:\")\n",
    "    for i, (domain, results) in enumerate(sorted_domains, 1):\n",
    "        coherence = results['domain_coherence']\n",
    "        print(f\"   {i}. {domain}: {coherence:.3f} coherence\")\n",
    "    \n",
    "    # Best and worst domains\n",
    "    best_domain = sorted_domains[0][0]\n",
    "    worst_domain = sorted_domains[-1][0]\n",
    "    \n",
    "    print(f\"   ‚úÖ Best: {best_domain}\")\n",
    "    print(f\"   ‚ö†Ô∏è  Challenging: {worst_domain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model selection decision framework\n",
    "def recommend_embedding_model(use_case_params):\n",
    "    \"\"\"\n",
    "    Recommend embedding model based on use case parameters\n",
    "    \"\"\"\n",
    "    budget = use_case_params.get('budget', 'medium')  # low, medium, high\n",
    "    performance_req = use_case_params.get('performance', 'medium')  # low, medium, high\n",
    "    domain = use_case_params.get('domain', 'general')\n",
    "    volume = use_case_params.get('volume', 'medium')  # low, medium, high\n",
    "    latency_req = use_case_params.get('latency', 'medium')  # low, medium, high\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # High performance requirements\n",
    "    if performance_req == 'high':\n",
    "        if budget == 'high':\n",
    "            recommendations.append({\n",
    "                'model': 'OpenAI text-embedding-3-large',\n",
    "                'reason': 'Highest quality, latest technology',\n",
    "                'score': 95\n",
    "            })\n",
    "            recommendations.append({\n",
    "                'model': 'Voyage-3-large', \n",
    "                'reason': 'Excellent performance, large context window',\n",
    "                'score': 90\n",
    "            })\n",
    "        else:\n",
    "            recommendations.append({\n",
    "                'model': 'Voyage-3-lite',\n",
    "                'reason': 'Great performance at 1/5 the cost of OpenAI large',\n",
    "                'score': 85\n",
    "            })\n",
    "    \n",
    "    # Budget constraints\n",
    "    if budget == 'low' or volume == 'high':\n",
    "        recommendations.append({\n",
    "            'model': 'all-mpnet-base-v2',\n",
    "            'reason': 'Free, good performance, local deployment',\n",
    "            'score': 80\n",
    "        })\n",
    "        recommendations.append({\n",
    "            'model': 'all-MiniLM-L6-v2',\n",
    "            'reason': 'Fast, lightweight, free',\n",
    "            'score': 75\n",
    "        })\n",
    "    \n",
    "    # Speed requirements  \n",
    "    if latency_req == 'high':\n",
    "        recommendations.append({\n",
    "            'model': 'OpenAI text-embedding-3-small',\n",
    "            'reason': 'Fast API, good quality, reasonable cost',\n",
    "            'score': 80\n",
    "        })\n",
    "    \n",
    "    # Domain-specific adjustments\n",
    "    if domain in ['medical', 'legal', 'scientific']:\n",
    "        for rec in recommendations:\n",
    "            if 'OpenAI' in rec['model'] or 'Voyage' in rec['model']:\n",
    "                rec['score'] += 5  # Boost API models for specialized domains\n",
    "                rec['reason'] += ' (better for specialized domains)'\n",
    "    \n",
    "    # Sort by score and return top recommendations\n",
    "    recommendations.sort(key=lambda x: x['score'], reverse=True)\n",
    "    return recommendations[:3]\n",
    "\n",
    "# Test different use cases\n",
    "use_cases = [\n",
    "    {\n",
    "        'name': 'Startup MVP',\n",
    "        'params': {'budget': 'low', 'performance': 'medium', 'volume': 'low', 'domain': 'general'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Enterprise Legal Search', \n",
    "        'params': {'budget': 'high', 'performance': 'high', 'volume': 'high', 'domain': 'legal'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Medical Research Platform',\n",
    "        'params': {'budget': 'medium', 'performance': 'high', 'volume': 'medium', 'domain': 'medical'}\n",
    "    },\n",
    "    {\n",
    "        'name': 'Real-time Customer Support',\n",
    "        'params': {'budget': 'medium', 'performance': 'medium', 'volume': 'medium', 'latency': 'high'}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üéØ MODEL SELECTION RECOMMENDATIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "for use_case in use_cases:\n",
    "    print(f\"\\nüìã Use Case: {use_case['name']}\")\n",
    "    print(f\"   Parameters: {use_case['params']}\")\n",
    "    \n",
    "    recommendations = recommend_embedding_model(use_case['params'])\n",
    "    \n",
    "    print(f\"   Recommendations:\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"   {i}. {rec['model']} (Score: {rec['score']})\")\n",
    "        print(f\"      Reason: {rec['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### üîÑ Evolution of Embeddings:\n",
    "1. **Traditional methods** (TF-IDF) focus on word overlap, miss semantic meaning\n",
    "2. **Modern embeddings** capture context and semantic relationships\n",
    "3. **2025 models** are optimized for specific tasks like retrieval and similarity\n",
    "4. **Contextual understanding** allows same words to have different meanings\n",
    "\n",
    "### üèÜ 2025 Model Landscape:\n",
    "- **Performance leaders**: NV-Embed-v2, Voyage-3, OpenAI text-embedding-3\n",
    "- **Best value**: Open-source models like all-mpnet-base-v2 for budget-conscious applications\n",
    "- **Speed champions**: all-MiniLM-L6-v2, OpenAI text-embedding-3-small\n",
    "- **Specialized options**: Domain-specific models emerging\n",
    "\n",
    "### üí∞ Cost Considerations:\n",
    "1. **API models**: Pay per token, scale automatically, latest technology\n",
    "2. **Local models**: Free after setup, require infrastructure, data stays private\n",
    "3. **Volume matters**: High-volume applications favor local deployment\n",
    "4. **Quality vs cost**: 10x cost difference may yield only 10% quality improvement\n",
    "\n",
    "### üéØ Selection Framework:\n",
    "- **High performance + budget**: OpenAI text-embedding-3-large, Voyage-3\n",
    "- **Good performance + budget conscious**: Voyage-3-lite, OpenAI small\n",
    "- **Local deployment**: all-mpnet-base-v2, all-MiniLM-L6-v2\n",
    "- **Specialized domains**: Prefer API models with larger training data\n",
    "\n",
    "### üîç Practical Guidelines:\n",
    "1. **Start simple**: Use all-MiniLM-L6-v2 for prototyping\n",
    "2. **Measure what matters**: Test on your actual data and use cases\n",
    "3. **Consider total cost**: Include infrastructure, maintenance, opportunity cost\n",
    "4. **Plan for scale**: Choose models that can grow with your application\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In **Module 5**, we'll work hands-on with embeddings:\n",
    "- Generate and manipulate embedding vectors\n",
    "- Calculate and interpret similarity metrics\n",
    "- Visualize high-dimensional embedding spaces\n",
    "- Build semantic search functionality\n",
    "- Implement efficient similarity computation\n",
    "\n",
    "Understanding model characteristics helps you make the right choice for your RAG system!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. For your specific use case, what factors would be most important in model selection?\n",
    "2. How would you evaluate whether the cost of a premium model is justified?\n",
    "3. What strategies would you use to transition from a local model to an API model as you scale?\n",
    "4. How might embedding model choice impact downstream RAG performance?\n",
    "\n",
    "## üìù Optional Exercise\n",
    "\n",
    "**Advanced Challenge**: \n",
    "1. Choose a domain relevant to your work\n",
    "2. Create a test set of 20-30 text samples from that domain\n",
    "3. Test 3 different embedding models on your test set\n",
    "4. Evaluate which model best captures the semantic relationships in your domain\n",
    "5. Perform a cost-benefit analysis for your expected usage volume\n",
    "\n",
    "This will give you practical experience choosing models for real-world applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}