{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 9: Retrieval Strategies & Optimization\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this module, you will:\n",
    "- Implement advanced retrieval strategies including multi-stage pipelines\n",
    "- Master re-ranking techniques with cross-encoders and LLM judges\n",
    "- Optimize retrieval parameters for different use cases and domains\n",
    "- Handle edge cases, failures, and implement robust fallback strategies\n",
    "- Measure and evaluate retrieval quality using comprehensive metrics\n",
    "- Build production-ready retrieval pipelines with monitoring and optimization\n",
    "\n",
    "## üìö Key Concepts\n",
    "\n",
    "### Why Retrieval Optimization Matters\n",
    "\n",
    "**Retrieval quality directly determines RAG system performance:**\n",
    "\n",
    "```\n",
    "Poor Retrieval ‚Üí Irrelevant Context ‚Üí Bad LLM Output\n",
    "Great Retrieval ‚Üí Relevant Context ‚Üí Excellent LLM Output\n",
    "```\n",
    "\n",
    "Even the best LLM cannot compensate for poor retrieval. Studies show that **retrieval quality accounts for 70-80% of RAG system performance**.\n",
    "\n",
    "### üèóÔ∏è Modern Retrieval Architecture (2025)\n",
    "\n",
    "**Multi-Stage Retrieval Pipeline:**\n",
    "\n",
    "1. **Stage 1 - Initial Retrieval**: Fast, broad search (top 100-500 candidates)\n",
    "   - Vector similarity search\n",
    "   - BM25 keyword matching\n",
    "   - Hybrid fusion (RRF)\n",
    "\n",
    "2. **Stage 2 - Re-ranking**: Precise, compute-intensive (top 10-50 candidates)\n",
    "   - Cross-encoder models\n",
    "   - LLM-based judges\n",
    "   - Multi-criteria scoring\n",
    "\n",
    "3. **Stage 3 - Post-processing**: Final optimization\n",
    "   - Diversity injection\n",
    "   - Deduplication\n",
    "   - Context window fitting\n",
    "\n",
    "### üîÑ Advanced Retrieval Techniques (2025)\n",
    "\n",
    "#### Multi-Query Retrieval\n",
    "- **Concept**: Generate multiple query variations, retrieve for each, fuse results\n",
    "- **Benefit**: 25-40% improvement in recall\n",
    "- **Cost**: 3-5x more retrieval operations\n",
    "\n",
    "#### Parent-Child Retrieval\n",
    "- **Concept**: Retrieve small chunks for precision, return large chunks for context\n",
    "- **Benefit**: Better balance of specificity and completeness\n",
    "- **Implementation**: Hierarchical document structure\n",
    "\n",
    "#### Contextual Retrieval (Anthropic 2025)\n",
    "- **Concept**: Add contextual information to chunks before embedding\n",
    "- **Benefit**: 49% reduction in failed retrievals\n",
    "- **Method**: LLM generates chunk context within document\n",
    "\n",
    "#### RAPTOR (Recursive Tree Retrieval)\n",
    "- **Concept**: Build hierarchical tree of document summaries\n",
    "- **Benefit**: Better handling of multi-document queries\n",
    "- **Use case**: Large document collections, complex questions\n",
    "\n",
    "### üìä Retrieval Quality Metrics\n",
    "\n",
    "| Metric | Definition | Good Score | Use Case |\n",
    "|--------|------------|------------|---------|\n",
    "| **Recall@K** | Relevant items in top-K results | >0.8 | Coverage measurement |\n",
    "| **Precision@K** | Proportion of relevant results | >0.6 | Relevance measurement |\n",
    "| **MRR** | Mean Reciprocal Rank | >0.7 | Ranking quality |\n",
    "| **NDCG@K** | Normalized Discounted Gain | >0.8 | Ranked relevance |\n",
    "| **Hit Rate** | Queries with ‚â•1 relevant result | >0.9 | System reliability |\n",
    "\n",
    "### ‚ö° Performance vs Quality Trade-offs\n",
    "\n",
    "```python\n",
    "# Speed vs Quality Spectrum\n",
    "retrieval_strategies = {\n",
    "    \"basic_vector\": {\"speed\": \"‚ö°‚ö°‚ö°‚ö°‚ö°\", \"quality\": \"‚≠ê‚≠ê‚≠ê\"},\n",
    "    \"hybrid_search\": {\"speed\": \"‚ö°‚ö°‚ö°‚ö°\", \"quality\": \"‚≠ê‚≠ê‚≠ê‚≠ê\"},\n",
    "    \"cross_encoder\": {\"speed\": \"‚ö°‚ö°\", \"quality\": \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"},\n",
    "    \"llm_rerank\": {\"speed\": \"‚ö°\", \"quality\": \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"},\n",
    "    \"multi_query\": {\"speed\": \"‚ö°‚ö°\", \"quality\": \"‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\"}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup\n",
    "Let's install the required packages and set up our advanced retrieval lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q sentence-transformers transformers torch\n",
    "!pip install -q langchain langchain-community langchain-openai\n",
    "!pip install -q rank-bm25 faiss-cpu numpy pandas matplotlib seaborn\n",
    "!pip install -q scikit-learn nltk textstat python-dotenv\n",
    "!pip install -q plotly wordcloud\n",
    "# Note: For production, add: cross-encoder, openai, anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core ML libraries\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import ndcg_score\n",
    "from rank_bm25 import BM25Okapi\n",
    "import faiss\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import textstat\n",
    "\n",
    "# LangChain components\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Visualization\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Set up visualization\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")\n",
    "print(f\"üìÖ Today's date: {datetime.now().strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Exercise 1: Multi-Stage Retrieval Pipeline\n",
    "\n",
    "Let's build a production-ready multi-stage retrieval pipeline with initial search, re-ranking, and post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStageRetriever:\n",
    "    \"\"\"Advanced multi-stage retrieval pipeline with re-ranking and optimization\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        # Initialize models\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        # Note: In production, use a more powerful cross-encoder like 'ms-marco-MiniLM-L-12-v2'\n",
    "        self.cross_encoder = None  # Will simulate cross-encoder for demo\n",
    "        \n",
    "        # Initialize components\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        self.bm25 = None\n",
    "        self.tokenized_docs = []\n",
    "        self.document_metadata = []\n",
    "        \n",
    "        # Preprocessing tools\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.retrieval_stats = defaultdict(list)\n",
    "    \n",
    "    def create_comprehensive_dataset(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Create a comprehensive dataset with metadata for advanced retrieval testing\"\"\"\n",
    "        \n",
    "        dataset = [\n",
    "            # Technical Documentation\n",
    "            {\n",
    "                \"text\": \"TensorFlow 2.14.0 introduces optimized GPU kernels for transformer models, reducing training time by up to 35%. The new tf.keras.layers.MultiHeadAttention layer includes improved memory management and support for mixed precision training.\",\n",
    "                \"category\": \"technical\",\n",
    "                \"domain\": \"machine_learning\",\n",
    "                \"difficulty\": \"advanced\",\n",
    "                \"date\": \"2024-01-15\",\n",
    "                \"source\": \"tensorflow_docs\",\n",
    "                \"relevance_keywords\": [\"tensorflow\", \"gpu\", \"optimization\", \"training\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"PyTorch 2.1 brings native support for distributed training with automatic mixed precision. The torch.compile() function can optimize models for inference with up to 60% speedup on modern GPUs.\",\n",
    "                \"category\": \"technical\",\n",
    "                \"domain\": \"machine_learning\",\n",
    "                \"difficulty\": \"advanced\",\n",
    "                \"date\": \"2024-02-20\",\n",
    "                \"source\": \"pytorch_docs\",\n",
    "                \"relevance_keywords\": [\"pytorch\", \"distributed\", \"inference\", \"speedup\"]\n",
    "            },\n",
    "            # Business Analytics\n",
    "            {\n",
    "                \"text\": \"Q4 2024 revenue reached $125M, representing a 22% year-over-year increase. The AI products division contributed $45M, driven by strong enterprise adoption of our machine learning platform.\",\n",
    "                \"category\": \"business\",\n",
    "                \"domain\": \"finance\",\n",
    "                \"difficulty\": \"beginner\",\n",
    "                \"date\": \"2024-12-31\",\n",
    "                \"source\": \"earnings_report\",\n",
    "                \"relevance_keywords\": [\"revenue\", \"growth\", \"ai products\", \"enterprise\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Customer acquisition costs decreased by 18% in 2024 due to improved targeting algorithms. The machine learning-powered recommendation system increased customer lifetime value by 31%.\",\n",
    "                \"category\": \"business\",\n",
    "                \"domain\": \"marketing\",\n",
    "                \"difficulty\": \"intermediate\",\n",
    "                \"date\": \"2024-11-15\",\n",
    "                \"source\": \"marketing_analysis\",\n",
    "                \"relevance_keywords\": [\"customer acquisition\", \"targeting\", \"recommendations\", \"lifetime value\"]\n",
    "            },\n",
    "            # Research Papers\n",
    "            {\n",
    "                \"text\": \"Attention mechanisms in transformer architectures enable models to focus on relevant parts of the input sequence. Multi-head attention allows the model to jointly attend to information from different representation subspaces.\",\n",
    "                \"category\": \"research\",\n",
    "                \"domain\": \"nlp\",\n",
    "                \"difficulty\": \"advanced\",\n",
    "                \"date\": \"2024-03-10\",\n",
    "                \"source\": \"research_paper\",\n",
    "                \"relevance_keywords\": [\"attention\", \"transformer\", \"multi-head\", \"sequence\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Large language models demonstrate emergent capabilities at scale, including few-shot learning, chain-of-thought reasoning, and cross-lingual transfer. Performance scaling follows predictable power laws with model size and compute.\",\n",
    "                \"category\": \"research\",\n",
    "                \"domain\": \"nlp\",\n",
    "                \"difficulty\": \"advanced\",\n",
    "                \"date\": \"2024-04-22\",\n",
    "                \"source\": \"research_paper\",\n",
    "                \"relevance_keywords\": [\"large language models\", \"emergent\", \"few-shot\", \"scaling\"]\n",
    "            },\n",
    "            # API Documentation\n",
    "            {\n",
    "                \"text\": \"The /api/v1/predict endpoint accepts POST requests with JSON payload containing feature vectors. Returns prediction scores and confidence intervals. Rate limit: 1000 requests per hour.\",\n",
    "                \"category\": \"documentation\",\n",
    "                \"domain\": \"api\",\n",
    "                \"difficulty\": \"intermediate\",\n",
    "                \"date\": \"2024-05-01\",\n",
    "                \"source\": \"api_docs\",\n",
    "                \"relevance_keywords\": [\"api\", \"predict\", \"endpoint\", \"rate limit\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Authentication uses Bearer tokens with JWT format. Include Authorization header in all requests. Tokens expire after 24 hours and must be refreshed using the /auth/refresh endpoint.\",\n",
    "                \"category\": \"documentation\",\n",
    "                \"domain\": \"api\",\n",
    "                \"difficulty\": \"intermediate\",\n",
    "                \"date\": \"2024-05-01\",\n",
    "                \"source\": \"api_docs\",\n",
    "                \"relevance_keywords\": [\"authentication\", \"bearer token\", \"jwt\", \"authorization\"]\n",
    "            },\n",
    "            # Tutorials and Guides\n",
    "            {\n",
    "                \"text\": \"To train a neural network for image classification, start by preprocessing your images to a consistent size and format. Use data augmentation techniques like rotation and scaling to improve model generalization.\",\n",
    "                \"category\": \"tutorial\",\n",
    "                \"domain\": \"computer_vision\",\n",
    "                \"difficulty\": \"beginner\",\n",
    "                \"date\": \"2024-06-15\",\n",
    "                \"source\": \"tutorial\",\n",
    "                \"relevance_keywords\": [\"neural network\", \"image classification\", \"preprocessing\", \"augmentation\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Natural language processing pipelines typically include tokenization, part-of-speech tagging, named entity recognition, and sentiment analysis. Modern approaches use transformer-based models for end-to-end processing.\",\n",
    "                \"category\": \"tutorial\",\n",
    "                \"domain\": \"nlp\",\n",
    "                \"difficulty\": \"intermediate\",\n",
    "                \"date\": \"2024-07-20\",\n",
    "                \"source\": \"tutorial\",\n",
    "                \"relevance_keywords\": [\"nlp\", \"tokenization\", \"named entity\", \"sentiment analysis\"]\n",
    "            },\n",
    "            # News and Updates\n",
    "            {\n",
    "                \"text\": \"OpenAI releases GPT-4 Turbo with improved performance and reduced costs. The new model supports 128K context windows and demonstrates better reasoning capabilities across multiple domains.\",\n",
    "                \"category\": \"news\",\n",
    "                \"domain\": \"ai_updates\",\n",
    "                \"difficulty\": \"beginner\",\n",
    "                \"date\": \"2024-08-01\",\n",
    "                \"source\": \"tech_news\",\n",
    "                \"relevance_keywords\": [\"openai\", \"gpt-4 turbo\", \"context window\", \"reasoning\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Google announces Gemini Ultra, achieving state-of-the-art performance on 30 of 32 academic benchmarks. The model excels at multimodal reasoning, combining text, images, and code understanding.\",\n",
    "                \"category\": \"news\",\n",
    "                \"domain\": \"ai_updates\",\n",
    "                \"difficulty\": \"beginner\",\n",
    "                \"date\": \"2024-08-15\",\n",
    "                \"source\": \"tech_news\",\n",
    "                \"relevance_keywords\": [\"google\", \"gemini ultra\", \"benchmarks\", \"multimodal\"]\n",
    "            },\n",
    "            # Code Examples\n",
    "            {\n",
    "                \"text\": \"def train_model(X_train, y_train, epochs=100, batch_size=32): model = Sequential([Dense(128, activation='relu'), Dense(64, activation='relu'), Dense(1, activation='sigmoid')]) model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) return model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\",\n",
    "                \"category\": \"code\",\n",
    "                \"domain\": \"machine_learning\",\n",
    "                \"difficulty\": \"intermediate\",\n",
    "                \"date\": \"2024-09-01\",\n",
    "                \"source\": \"code_example\",\n",
    "                \"relevance_keywords\": [\"train_model\", \"sequential\", \"dense\", \"binary_crossentropy\"]\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"class DataProcessor: def __init__(self, config): self.config = config def preprocess(self, raw_data): normalized = self.normalize(raw_data) cleaned = self.remove_outliers(normalized) return self.feature_engineering(cleaned)\",\n",
    "                \"category\": \"code\",\n",
    "                \"domain\": \"data_science\",\n",
    "                \"difficulty\": \"intermediate\",\n",
    "                \"date\": \"2024-09-05\",\n",
    "                \"source\": \"code_example\",\n",
    "                \"relevance_keywords\": [\"DataProcessor\", \"preprocess\", \"normalize\", \"feature_engineering\"]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return dataset\n",
    "    \n",
    "    def build_indexes(self, dataset: List[Dict[str, Any]]) -> None:\n",
    "        \"\"\"Build all necessary indexes for multi-stage retrieval\"\"\"\n",
    "        print(\"üèóÔ∏è Building multi-stage retrieval indexes...\")\n",
    "        \n",
    "        # Extract documents and metadata\n",
    "        self.documents = [item[\"text\"] for item in dataset]\n",
    "        self.document_metadata = [{k: v for k, v in item.items() if k != \"text\"} for item in dataset]\n",
    "        \n",
    "        # Build semantic index (embeddings)\n",
    "        print(\"   Building semantic embeddings...\")\n",
    "        self.embeddings = self.embedding_model.encode(self.documents, show_progress_bar=True)\n",
    "        self.embeddings = self.embeddings / np.linalg.norm(self.embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Build BM25 index\n",
    "        print(\"   Building BM25 index...\")\n",
    "        self.tokenized_docs = [self._preprocess_text(doc) for doc in self.documents]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        print(f\"‚úÖ Indexes built for {len(self.documents)} documents\")\n",
    "    \n",
    "    def _preprocess_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Preprocess text for BM25\"\"\"\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    def stage1_initial_retrieval(self, query: str, top_k: int = 100, alpha: float = 0.7) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Stage 1: Fast initial retrieval with hybrid search\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Semantic search\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        semantic_similarities = np.dot(self.embeddings, query_embedding.T).flatten()\n",
    "        \n",
    "        # BM25 search\n",
    "        query_tokens = self._preprocess_text(query)\n",
    "        bm25_scores = self.bm25.get_scores(query_tokens)\n",
    "        \n",
    "        # Reciprocal Rank Fusion (RRF)\n",
    "        semantic_ranks = np.argsort(semantic_similarities)[::-1]\n",
    "        bm25_ranks = np.argsort(bm25_scores)[::-1]\n",
    "        \n",
    "        # Calculate RRF scores\n",
    "        rrf_scores = {}\n",
    "        k = 60  # RRF parameter\n",
    "        \n",
    "        for rank, doc_id in enumerate(semantic_ranks):\n",
    "            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + alpha / (k + rank + 1)\n",
    "        \n",
    "        for rank, doc_id in enumerate(bm25_ranks):\n",
    "            rrf_scores[doc_id] = rrf_scores.get(doc_id, 0) + (1 - alpha) / (k + rank + 1)\n",
    "        \n",
    "        # Sort by RRF score\n",
    "        sorted_results = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for doc_id, score in sorted_results:\n",
    "            results.append((doc_id, score, self.documents[doc_id]))\n",
    "        \n",
    "        stage1_time = time.time() - start_time\n",
    "        self.retrieval_stats[\"stage1_time\"].append(stage1_time)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def stage2_reranking(self, query: str, initial_results: List[Tuple[int, float, str]], \n",
    "                        top_k: int = 20) -> List[Tuple[int, float, str]]:\n",
    "        \"\"\"Stage 2: Re-ranking with simulated cross-encoder\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Simulate cross-encoder scoring\n",
    "        # In production, use: self.cross_encoder.predict([(query, doc) for _, _, doc in initial_results])\n",
    "        reranked_results = []\n",
    "        \n",
    "        for doc_id, initial_score, doc_text in initial_results:\n",
    "            # Simulate cross-encoder with sophisticated heuristics\n",
    "            cross_encoder_score = self._simulate_cross_encoder_score(query, doc_text, doc_id)\n",
    "            reranked_results.append((doc_id, cross_encoder_score, doc_text))\n",
    "        \n",
    "        # Sort by cross-encoder score\n",
    "        reranked_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        stage2_time = time.time() - start_time\n",
    "        self.retrieval_stats[\"stage2_time\"].append(stage2_time)\n",
    "        \n",
    "        return reranked_results[:top_k]\n",
    "    \n",
    "    def _simulate_cross_encoder_score(self, query: str, document: str, doc_id: int) -> float:\n",
    "        \"\"\"Simulate cross-encoder scoring with advanced heuristics\"\"\"\n",
    "        base_score = 0.5\n",
    "        \n",
    "        # Query-document similarity (semantic)\n",
    "        query_emb = self.embedding_model.encode([query])\n",
    "        doc_emb = self.embeddings[doc_id:doc_id+1]\n",
    "        semantic_sim = np.dot(query_emb, doc_emb.T)[0, 0]\n",
    "        base_score += semantic_sim * 0.3\n",
    "        \n",
    "        # Keyword overlap bonus\n",
    "        query_words = set(query.lower().split())\n",
    "        doc_words = set(document.lower().split())\n",
    "        overlap = len(query_words.intersection(doc_words)) / len(query_words.union(doc_words))\n",
    "        base_score += overlap * 0.2\n",
    "        \n",
    "        # Metadata relevance\n",
    "        metadata = self.document_metadata[doc_id]\n",
    "        \n",
    "        # Boost for relevant keywords in metadata\n",
    "        if 'relevance_keywords' in metadata:\n",
    "            keyword_matches = sum(1 for keyword in metadata['relevance_keywords'] \n",
    "                                if keyword.lower() in query.lower())\n",
    "            base_score += keyword_matches * 0.1\n",
    "        \n",
    "        # Boost for recent documents\n",
    "        if 'date' in metadata:\n",
    "            doc_date = datetime.strptime(metadata['date'], '%Y-%m-%d')\n",
    "            days_old = (datetime.now() - doc_date).days\n",
    "            recency_score = max(0, 1 - days_old / 365)  # Decay over 1 year\n",
    "            base_score += recency_score * 0.1\n",
    "        \n",
    "        # Category-specific adjustments\n",
    "        if 'category' in metadata:\n",
    "            if metadata['category'] == 'research' and any(word in query.lower() \n",
    "                                                        for word in ['study', 'research', 'paper', 'analysis']):\n",
    "                base_score += 0.1\n",
    "            elif metadata['category'] == 'tutorial' and any(word in query.lower() \n",
    "                                                          for word in ['how', 'tutorial', 'guide', 'learn']):\n",
    "                base_score += 0.1\n",
    "        \n",
    "        # Add some randomness to simulate real cross-encoder variance\n",
    "        base_score += np.random.normal(0, 0.05)\n",
    "        \n",
    "        return max(0.0, min(1.0, base_score))\n",
    "    \n",
    "    def stage3_postprocessing(self, query: str, reranked_results: List[Tuple[int, float, str]], \n",
    "                            final_k: int = 10, diversity_threshold: float = 0.8) -> List[Tuple[int, float, str, Dict]]:\n",
    "        \"\"\"Stage 3: Post-processing with diversity and deduplication\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        final_results = []\n",
    "        selected_embeddings = []\n",
    "        \n",
    "        for doc_id, score, doc_text in reranked_results:\n",
    "            if len(final_results) >= final_k:\n",
    "                break\n",
    "            \n",
    "            doc_embedding = self.embeddings[doc_id]\n",
    "            \n",
    "            # Check diversity (avoid too similar documents)\n",
    "            is_diverse = True\n",
    "            for selected_emb in selected_embeddings:\n",
    "                similarity = np.dot(doc_embedding, selected_emb)\n",
    "                if similarity > diversity_threshold:\n",
    "                    is_diverse = False\n",
    "                    break\n",
    "            \n",
    "            if is_diverse:\n",
    "                # Add metadata for context\n",
    "                metadata = self.document_metadata[doc_id].copy()\n",
    "                metadata['retrieval_score'] = score\n",
    "                metadata['rank'] = len(final_results) + 1\n",
    "                \n",
    "                final_results.append((doc_id, score, doc_text, metadata))\n",
    "                selected_embeddings.append(doc_embedding)\n",
    "        \n",
    "        stage3_time = time.time() - start_time\n",
    "        self.retrieval_stats[\"stage3_time\"].append(stage3_time)\n",
    "        \n",
    "        return final_results\n",
    "    \n",
    "    def retrieve(self, query: str, stage1_k: int = 100, stage2_k: int = 20, \n",
    "                final_k: int = 10, alpha: float = 0.7) -> Dict[str, Any]:\n",
    "        \"\"\"Complete multi-stage retrieval pipeline\"\"\"\n",
    "        total_start = time.time()\n",
    "        \n",
    "        # Stage 1: Initial retrieval\n",
    "        stage1_results = self.stage1_initial_retrieval(query, top_k=stage1_k, alpha=alpha)\n",
    "        \n",
    "        # Stage 2: Re-ranking\n",
    "        stage2_results = self.stage2_reranking(query, stage1_results, top_k=stage2_k)\n",
    "        \n",
    "        # Stage 3: Post-processing\n",
    "        final_results = self.stage3_postprocessing(query, stage2_results, final_k=final_k)\n",
    "        \n",
    "        total_time = time.time() - total_start\n",
    "        self.retrieval_stats[\"total_time\"].append(total_time)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'results': final_results,\n",
    "            'stage1_candidates': len(stage1_results),\n",
    "            'stage2_candidates': len(stage2_results),\n",
    "            'final_results': len(final_results),\n",
    "            'total_time': total_time,\n",
    "            'stage_times': {\n",
    "                'stage1': self.retrieval_stats[\"stage1_time\"][-1],\n",
    "                'stage2': self.retrieval_stats[\"stage2_time\"][-1],\n",
    "                'stage3': self.retrieval_stats[\"stage3_time\"][-1]\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Initialize the multi-stage retriever\n",
    "retriever = MultiStageRetriever()\n",
    "print(\"üîç Multi-Stage Retriever initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and build indexes\n",
    "dataset = retriever.create_comprehensive_dataset()\n",
    "retriever.build_indexes(dataset)\n",
    "\n",
    "print(f\"üìä Dataset Overview:\")\n",
    "print(f\"   Total documents: {len(dataset)}\")\n",
    "\n",
    "# Analyze dataset composition\n",
    "categories = Counter(item['category'] for item in dataset)\n",
    "domains = Counter(item['domain'] for item in dataset)\n",
    "difficulties = Counter(item['difficulty'] for item in dataset)\n",
    "\n",
    "print(f\"   Categories: {dict(categories)}\")\n",
    "print(f\"   Domains: {dict(domains)}\")\n",
    "print(f\"   Difficulty levels: {dict(difficulties)}\")\n",
    "\n",
    "# Display sample documents\n",
    "print(\"\\nüìÑ Sample Documents:\")\n",
    "for i, item in enumerate(dataset[:3]):\n",
    "    print(f\"   {i+1}. [{item['category']}] {item['text'][:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-stage retrieval with different query types\n",
    "test_queries = [\n",
    "    \"How to optimize TensorFlow training performance?\",  # Technical query\n",
    "    \"What are the latest developments in large language models?\",  # Research query\n",
    "    \"API authentication methods and security\",  # Documentation query\n",
    "    \"Business revenue growth from AI products\",  # Business query\n",
    "    \"Machine learning model training best practices\"  # Tutorial query\n",
    "]\n",
    "\n",
    "print(\"üß™ MULTI-STAGE RETRIEVAL TESTING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "all_retrieval_results = []\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\nüîç Query {i+1}: '{query}'\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Perform multi-stage retrieval\n",
    "    result = retriever.retrieve(query, stage1_k=50, stage2_k=10, final_k=5)\n",
    "    all_retrieval_results.append(result)\n",
    "    \n",
    "    # Display timing information\n",
    "    print(f\"‚è±Ô∏è Performance:\")\n",
    "    print(f\"   Total time: {result['total_time']:.3f}s\")\n",
    "    print(f\"   Stage 1 (Initial): {result['stage_times']['stage1']:.3f}s\")\n",
    "    print(f\"   Stage 2 (Re-rank): {result['stage_times']['stage2']:.3f}s\")\n",
    "    print(f\"   Stage 3 (Post-proc): {result['stage_times']['stage3']:.3f}s\")\n",
    "    \n",
    "    # Display pipeline funnel\n",
    "    print(f\"\\nüîÑ Retrieval Funnel:\")\n",
    "    print(f\"   Stage 1 ‚Üí {result['stage1_candidates']} candidates\")\n",
    "    print(f\"   Stage 2 ‚Üí {result['stage2_candidates']} re-ranked\")\n",
    "    print(f\"   Stage 3 ‚Üí {result['final_results']} final results\")\n",
    "    \n",
    "    # Display top results with metadata\n",
    "    print(f\"\\nüìã Top Results:\")\n",
    "    for j, (doc_id, score, doc_text, metadata) in enumerate(result['results']):\n",
    "        print(f\"   {j+1}. [{score:.4f}] [{metadata['category']}] {doc_text[:60]}...\")\n",
    "        print(f\"       Domain: {metadata['domain']}, Difficulty: {metadata['difficulty']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Exercise 2: Advanced Retrieval Techniques\n",
    "\n",
    "Let's implement cutting-edge retrieval techniques including multi-query, parent-child, and contextual retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRetrievalTechniques:\n",
    "    \"\"\"Implementation of advanced retrieval techniques from 2025 research\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever: MultiStageRetriever):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.embedding_model = base_retriever.embedding_model\n",
    "        \n",
    "        # For parent-child retrieval\n",
    "        self.parent_documents = []\n",
    "        self.child_chunks = []\n",
    "        self.parent_child_mapping = {}\n",
    "        \n",
    "        # Query expansion templates\n",
    "        self.query_expansion_templates = [\n",
    "            \"What is {query}?\",\n",
    "            \"How to {query}?\",\n",
    "            \"Explain {query}\",\n",
    "            \"{query} examples\",\n",
    "            \"{query} best practices\",\n",
    "            \"Latest developments in {query}\"\n",
    "        ]\n",
    "    \n",
    "    def multi_query_retrieval(self, query: str, num_variations: int = 3, \n",
    "                            final_k: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Multi-query retrieval with query expansion and result fusion\"\"\"\n",
    "        print(f\"üîç Multi-Query Retrieval for: '{query}'\")\n",
    "        \n",
    "        # Generate query variations\n",
    "        query_variations = self._generate_query_variations(query, num_variations)\n",
    "        print(f\"   Generated {len(query_variations)} query variations\")\n",
    "        \n",
    "        all_results = []\n",
    "        query_results = {}\n",
    "        \n",
    "        # Retrieve for each query variation\n",
    "        for i, var_query in enumerate(query_variations):\n",
    "            print(f\"   Querying: '{var_query}'\", end=\" \")\n",
    "            result = self.base_retriever.retrieve(var_query, final_k=20)\n",
    "            query_results[var_query] = result\n",
    "            all_results.extend(result['results'])\n",
    "            print(f\"({len(result['results'])} results)\")\n",
    "        \n",
    "        # Fuse results using Reciprocal Rank Fusion\n",
    "        fused_results = self._fuse_multi_query_results(all_results, final_k)\n",
    "        \n",
    "        return {\n",
    "            'original_query': query,\n",
    "            'query_variations': query_variations,\n",
    "            'individual_results': query_results,\n",
    "            'fused_results': fused_results,\n",
    "            'total_candidates': len(all_results),\n",
    "            'final_count': len(fused_results)\n",
    "        }\n",
    "    \n",
    "    def _generate_query_variations(self, query: str, num_variations: int) -> List[str]:\n",
    "        \"\"\"Generate query variations using templates and paraphrasing\"\"\"\n",
    "        variations = [query]  # Include original\n",
    "        \n",
    "        # Template-based variations\n",
    "        for template in self.query_expansion_templates[:num_variations]:\n",
    "            if len(variations) >= num_variations + 1:\n",
    "                break\n",
    "            \n",
    "            # Simple template filling\n",
    "            if '{query}' in template:\n",
    "                variation = template.format(query=query)\n",
    "            else:\n",
    "                variation = f\"{template} {query}\"\n",
    "            \n",
    "            if variation not in variations:\n",
    "                variations.append(variation)\n",
    "        \n",
    "        # Synonym-based variations (simplified)\n",
    "        synonyms = {\n",
    "            'machine learning': ['ML', 'artificial intelligence', 'AI algorithms'],\n",
    "            'optimization': ['improvement', 'enhancement', 'tuning'],\n",
    "            'training': ['learning', 'education', 'instruction'],\n",
    "            'performance': ['efficiency', 'speed', 'effectiveness'],\n",
    "            'api': ['application programming interface', 'service interface', 'endpoint'],\n",
    "            'authentication': ['auth', 'login', 'access control']\n",
    "        }\n",
    "        \n",
    "        query_lower = query.lower()\n",
    "        for term, syns in synonyms.items():\n",
    "            if term in query_lower and len(variations) < num_variations + 1:\n",
    "                for syn in syns:\n",
    "                    if len(variations) >= num_variations + 1:\n",
    "                        break\n",
    "                    syn_query = query_lower.replace(term, syn)\n",
    "                    if syn_query not in variations:\n",
    "                        variations.append(syn_query)\n",
    "        \n",
    "        return variations[:num_variations + 1]\n",
    "    \n",
    "    def _fuse_multi_query_results(self, all_results: List[Tuple], final_k: int) -> List[Tuple]:\n",
    "        \"\"\"Fuse results from multiple queries using RRF\"\"\"\n",
    "        doc_scores = defaultdict(list)\n",
    "        doc_info = {}\n",
    "        \n",
    "        # Collect all scores for each document\n",
    "        for doc_id, score, doc_text, metadata in all_results:\n",
    "            doc_scores[doc_id].append(score)\n",
    "            doc_info[doc_id] = (doc_text, metadata)\n",
    "        \n",
    "        # Calculate fused score (max score aggregation)\n",
    "        fused_scores = []\n",
    "        for doc_id, scores in doc_scores.items():\n",
    "            # Use maximum score as fusion method\n",
    "            max_score = max(scores)\n",
    "            # Also consider frequency (how many queries returned this doc)\n",
    "            frequency_bonus = len(scores) * 0.1\n",
    "            final_score = max_score + frequency_bonus\n",
    "            \n",
    "            doc_text, metadata = doc_info[doc_id]\n",
    "            metadata['fusion_score'] = final_score\n",
    "            metadata['appearance_count'] = len(scores)\n",
    "            \n",
    "            fused_scores.append((doc_id, final_score, doc_text, metadata))\n",
    "        \n",
    "        # Sort by fused score\n",
    "        fused_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return fused_scores[:final_k]\n",
    "    \n",
    "    def create_parent_child_structure(self, documents: List[str], \n",
    "                                     chunk_size: int = 200, overlap: int = 50) -> None:\n",
    "        \"\"\"Create parent-child document structure\"\"\"\n",
    "        print(\"üèóÔ∏è Building parent-child document structure...\")\n",
    "        \n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=overlap,\n",
    "            length_function=len\n",
    "        )\n",
    "        \n",
    "        self.parent_documents = documents\n",
    "        self.child_chunks = []\n",
    "        self.parent_child_mapping = {}\n",
    "        \n",
    "        for parent_id, parent_doc in enumerate(documents):\n",
    "            # Split parent document into child chunks\n",
    "            chunks = text_splitter.split_text(parent_doc)\n",
    "            \n",
    "            for chunk in chunks:\n",
    "                child_id = len(self.child_chunks)\n",
    "                self.child_chunks.append(chunk)\n",
    "                self.parent_child_mapping[child_id] = parent_id\n",
    "        \n",
    "        print(f\"   Created {len(self.child_chunks)} child chunks from {len(self.parent_documents)} parent documents\")\n",
    "    \n",
    "    def parent_child_retrieval(self, query: str, top_k: int = 10, \n",
    "                              child_retrieval_k: int = 20) -> Dict[str, Any]:\n",
    "        \"\"\"Parent-child retrieval: search children, return parents\"\"\"\n",
    "        print(f\"üë®‚Äçüëß‚Äçüë¶ Parent-Child Retrieval for: '{query}'\")\n",
    "        \n",
    "        if not self.child_chunks:\n",
    "            self.create_parent_child_structure(self.base_retriever.documents)\n",
    "        \n",
    "        # Search in child chunks for precision\n",
    "        child_embeddings = self.embedding_model.encode(self.child_chunks)\n",
    "        child_embeddings = child_embeddings / np.linalg.norm(child_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        # Find most relevant child chunks\n",
    "        similarities = np.dot(child_embeddings, query_embedding.T).flatten()\n",
    "        top_child_indices = np.argsort(similarities)[::-1][:child_retrieval_k]\n",
    "        \n",
    "        # Map to parent documents and aggregate scores\n",
    "        parent_scores = defaultdict(list)\n",
    "        child_results = []\n",
    "        \n",
    "        for child_id in top_child_indices:\n",
    "            parent_id = self.parent_child_mapping[child_id]\n",
    "            score = similarities[child_id]\n",
    "            \n",
    "            parent_scores[parent_id].append(score)\n",
    "            child_results.append({\n",
    "                'child_id': child_id,\n",
    "                'parent_id': parent_id,\n",
    "                'child_text': self.child_chunks[child_id],\n",
    "                'score': score\n",
    "            })\n",
    "        \n",
    "        # Rank parents by best child score\n",
    "        parent_results = []\n",
    "        for parent_id, scores in parent_scores.items():\n",
    "            best_score = max(scores)\n",
    "            avg_score = np.mean(scores)\n",
    "            \n",
    "            # If we have original metadata, include it\n",
    "            if parent_id < len(self.base_retriever.document_metadata):\n",
    "                metadata = self.base_retriever.document_metadata[parent_id].copy()\n",
    "            else:\n",
    "                metadata = {}\n",
    "            \n",
    "            metadata.update({\n",
    "                'best_child_score': best_score,\n",
    "                'avg_child_score': avg_score,\n",
    "                'matching_children': len(scores)\n",
    "            })\n",
    "            \n",
    "            parent_results.append((\n",
    "                parent_id, \n",
    "                best_score, \n",
    "                self.parent_documents[parent_id], \n",
    "                metadata\n",
    "            ))\n",
    "        \n",
    "        # Sort by best child score\n",
    "        parent_results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'child_results': child_results,\n",
    "            'parent_results': parent_results[:top_k],\n",
    "            'total_children_searched': len(self.child_chunks),\n",
    "            'relevant_children': len(child_results),\n",
    "            'unique_parents': len(parent_scores)\n",
    "        }\n",
    "    \n",
    "    def contextual_retrieval(self, query: str, top_k: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"Contextual retrieval with chunk augmentation (simplified version)\"\"\"\n",
    "        print(f\"üåç Contextual Retrieval for: '{query}'\")\n",
    "        \n",
    "        # Create contextual versions of documents\n",
    "        contextual_documents = []\n",
    "        \n",
    "        for i, doc in enumerate(self.base_retriever.documents):\n",
    "            metadata = self.base_retriever.document_metadata[i]\n",
    "            \n",
    "            # Generate context based on metadata\n",
    "            context_parts = []\n",
    "            \n",
    "            if 'category' in metadata:\n",
    "                context_parts.append(f\"This is a {metadata['category']} document\")\n",
    "            \n",
    "            if 'domain' in metadata:\n",
    "                context_parts.append(f\"in the domain of {metadata['domain']}\")\n",
    "            \n",
    "            if 'source' in metadata:\n",
    "                context_parts.append(f\"from {metadata['source']}\")\n",
    "            \n",
    "            if 'date' in metadata:\n",
    "                context_parts.append(f\"published on {metadata['date']}\")\n",
    "            \n",
    "            # Add context to document\n",
    "            context_prefix = \". \".join(context_parts) + \". Content: \"\n",
    "            contextual_doc = context_prefix + doc\n",
    "            contextual_documents.append(contextual_doc)\n",
    "        \n",
    "        # Generate embeddings for contextual documents\n",
    "        contextual_embeddings = self.embedding_model.encode(contextual_documents)\n",
    "        contextual_embeddings = contextual_embeddings / np.linalg.norm(contextual_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Perform semantic search with contextual embeddings\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "        \n",
    "        similarities = np.dot(contextual_embeddings, query_embedding.T).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            metadata = self.base_retriever.document_metadata[idx].copy()\n",
    "            metadata['contextual_score'] = similarities[idx]\n",
    "            \n",
    "            results.append((\n",
    "                idx,\n",
    "                similarities[idx],\n",
    "                self.base_retriever.documents[idx],  # Return original document\n",
    "                metadata\n",
    "            ))\n",
    "        \n",
    "        return {\n",
    "            'query': query,\n",
    "            'results': results,\n",
    "            'contextual_documents_sample': contextual_documents[:2]  # Show examples\n",
    "        }\n",
    "\n",
    "# Initialize advanced retrieval techniques\n",
    "advanced_retrieval = AdvancedRetrievalTechniques(retriever)\n",
    "print(\"üöÄ Advanced Retrieval Techniques initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test advanced retrieval techniques\n",
    "test_query = \"machine learning model optimization techniques\"\n",
    "\n",
    "print(\"üß™ ADVANCED RETRIEVAL TECHNIQUES COMPARISON\")\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Multi-Query Retrieval\n",
    "print(\"\\n1Ô∏è‚É£ Multi-Query Retrieval:\")\n",
    "multi_query_result = advanced_retrieval.multi_query_retrieval(test_query, num_variations=3, final_k=5)\n",
    "\n",
    "print(f\"   Query variations: {multi_query_result['query_variations']}\")\n",
    "print(f\"   Total candidates: {multi_query_result['total_candidates']}\")\n",
    "print(f\"   Final results: {multi_query_result['final_count']}\")\n",
    "print(\"   Top results:\")\n",
    "for i, (doc_id, score, doc_text, metadata) in enumerate(multi_query_result['fused_results'][:3]):\n",
    "    print(f\"      {i+1}. [{score:.4f}] (appeared {metadata['appearance_count']}x) {doc_text[:60]}...\")\n",
    "\n",
    "# 2. Parent-Child Retrieval\n",
    "print(\"\\n2Ô∏è‚É£ Parent-Child Retrieval:\")\n",
    "parent_child_result = advanced_retrieval.parent_child_retrieval(test_query, top_k=5, child_retrieval_k=15)\n",
    "\n",
    "print(f\"   Children searched: {parent_child_result['total_children_searched']}\")\n",
    "print(f\"   Relevant children: {parent_child_result['relevant_children']}\")\n",
    "print(f\"   Unique parents: {parent_child_result['unique_parents']}\")\n",
    "print(\"   Top parent results:\")\n",
    "for i, (parent_id, score, doc_text, metadata) in enumerate(parent_child_result['parent_results'][:3]):\n",
    "    print(f\"      {i+1}. [{score:.4f}] ({metadata['matching_children']} children) {doc_text[:60]}...\")\n",
    "\n",
    "# 3. Contextual Retrieval\n",
    "print(\"\\n3Ô∏è‚É£ Contextual Retrieval:\")\n",
    "contextual_result = advanced_retrieval.contextual_retrieval(test_query, top_k=5)\n",
    "\n",
    "print(\"   Example contextual document:\")\n",
    "print(f\"      {contextual_result['contextual_documents_sample'][0][:100]}...\")\n",
    "print(\"   Top results:\")\n",
    "for i, (doc_id, score, doc_text, metadata) in enumerate(contextual_result['results'][:3]):\n",
    "    print(f\"      {i+1}. [{score:.4f}] [{metadata['category']}] {doc_text[:60]}...\")\n",
    "\n",
    "# 4. Standard Multi-Stage (for comparison)\n",
    "print(\"\\n4Ô∏è‚É£ Standard Multi-Stage (Baseline):\")\n",
    "baseline_result = retriever.retrieve(test_query, final_k=5)\n",
    "print(\"   Top results:\")\n",
    "for i, (doc_id, score, doc_text, metadata) in enumerate(baseline_result['results'][:3]):\n",
    "    print(f\"      {i+1}. [{score:.4f}] [{metadata['category']}] {doc_text[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exercise 3: Retrieval Quality Evaluation\n",
    "\n",
    "Let's implement comprehensive evaluation metrics for retrieval quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrievalEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for retrieval systems\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_results = []\n",
    "        self.ground_truth = {}\n",
    "    \n",
    "    def create_evaluation_dataset(self) -> Dict[str, List[int]]:\n",
    "        \"\"\"Create ground truth dataset for evaluation\"\"\"\n",
    "        # Simplified ground truth based on document categories and keywords\n",
    "        ground_truth = {\n",
    "            \"TensorFlow optimization and performance\": [0, 1],  # TensorFlow and PyTorch docs\n",
    "            \"API authentication and security methods\": [6, 7],  # API docs\n",
    "            \"business revenue and AI products growth\": [2, 3],  # Business docs\n",
    "            \"transformer attention mechanisms research\": [4, 5],  # Research papers\n",
    "            \"machine learning model training tutorial\": [8, 9, 10, 11],  # Tutorial and code\n",
    "            \"latest developments in large language models\": [5, 10],  # Research and news\n",
    "            \"neural network image classification guide\": [8, 9],  # Tutorial docs\n",
    "            \"natural language processing pipeline steps\": [9, 4],  # Tutorial and research\n",
    "        }\n",
    "        \n",
    "        self.ground_truth = ground_truth\n",
    "        return ground_truth\n",
    "    \n",
    "    def calculate_precision_recall(self, retrieved_docs: List[int], \n",
    "                                  relevant_docs: List[int], k: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Calculate precision and recall at k\"\"\"\n",
    "        retrieved_set = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "        \n",
    "        true_positives = len(retrieved_set.intersection(relevant_set))\n",
    "        \n",
    "        precision_at_k = true_positives / len(retrieved_set) if retrieved_set else 0.0\n",
    "        recall_at_k = true_positives / len(relevant_set) if relevant_set else 0.0\n",
    "        \n",
    "        return {\n",
    "            'precision_at_k': precision_at_k,\n",
    "            'recall_at_k': recall_at_k,\n",
    "            'true_positives': true_positives,\n",
    "            'retrieved_count': len(retrieved_set),\n",
    "            'relevant_count': len(relevant_set)\n",
    "        }\n",
    "    \n",
    "    def calculate_mrr(self, retrieved_docs: List[int], relevant_docs: List[int]) -> float:\n",
    "        \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "        relevant_set = set(relevant_docs)\n",
    "        \n",
    "        for rank, doc_id in enumerate(retrieved_docs, 1):\n",
    "            if doc_id in relevant_set:\n",
    "                return 1.0 / rank\n",
    "        \n",
    "        return 0.0\n",
    "    \n",
    "    def calculate_ndcg(self, retrieved_docs: List[int], relevant_docs: List[int], k: int = 10) -> float:\n",
    "        \"\"\"Calculate Normalized Discounted Cumulative Gain\"\"\"\n",
    "        relevant_set = set(relevant_docs)\n",
    "        \n",
    "        # Create relevance scores (1 for relevant, 0 for not relevant)\n",
    "        relevance_scores = [1 if doc_id in relevant_set else 0 for doc_id in retrieved_docs[:k]]\n",
    "        \n",
    "        # Ideal relevance scores (all relevant docs first)\n",
    "        ideal_scores = [1] * min(len(relevant_docs), k) + [0] * max(0, k - len(relevant_docs))\n",
    "        \n",
    "        if not any(relevance_scores):\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Use sklearn's ndcg_score\n",
    "            return ndcg_score([ideal_scores], [relevance_scores], k=k)\n",
    "        except:\n",
    "            # Manual calculation if sklearn fails\n",
    "            dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance_scores) if rel > 0)\n",
    "            idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal_scores) if rel > 0)\n",
    "            return dcg / idcg if idcg > 0 else 0.0\n",
    "    \n",
    "    def calculate_hit_rate(self, retrieved_docs: List[int], relevant_docs: List[int], k: int = 10) -> float:\n",
    "        \"\"\"Calculate hit rate (whether any relevant doc is in top-k)\"\"\"\n",
    "        retrieved_set = set(retrieved_docs[:k])\n",
    "        relevant_set = set(relevant_docs)\n",
    "        \n",
    "        return 1.0 if retrieved_set.intersection(relevant_set) else 0.0\n",
    "    \n",
    "    def evaluate_retrieval_method(self, retrieval_function, method_name: str, \n",
    "                                queries: List[str], k_values: List[int] = [5, 10]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate a retrieval method across multiple queries and k values\"\"\"\n",
    "        print(f\"üìä Evaluating {method_name}...\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for query in queries:\n",
    "            if query not in self.ground_truth:\n",
    "                continue\n",
    "                \n",
    "            relevant_docs = self.ground_truth[query]\n",
    "            \n",
    "            # Get retrieval results\n",
    "            retrieval_result = retrieval_function(query)\n",
    "            \n",
    "            # Extract document IDs (handle different result formats)\n",
    "            if isinstance(retrieval_result, dict):\n",
    "                if 'results' in retrieval_result:\n",
    "                    retrieved_docs = [item[0] for item in retrieval_result['results']]\n",
    "                elif 'fused_results' in retrieval_result:\n",
    "                    retrieved_docs = [item[0] for item in retrieval_result['fused_results']]\n",
    "                elif 'parent_results' in retrieval_result:\n",
    "                    retrieved_docs = [item[0] for item in retrieval_result['parent_results']]\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                retrieved_docs = [item[0] for item in retrieval_result]\n",
    "            \n",
    "            # Calculate metrics for different k values\n",
    "            for k in k_values:\n",
    "                precision_recall = self.calculate_precision_recall(retrieved_docs, relevant_docs, k)\n",
    "                mrr = self.calculate_mrr(retrieved_docs, relevant_docs)\n",
    "                ndcg = self.calculate_ndcg(retrieved_docs, relevant_docs, k)\n",
    "                hit_rate = self.calculate_hit_rate(retrieved_docs, relevant_docs, k)\n",
    "                \n",
    "                results.append({\n",
    "                    'method': method_name,\n",
    "                    'query': query[:40] + '...' if len(query) > 40 else query,\n",
    "                    'k': k,\n",
    "                    'precision_at_k': precision_recall['precision_at_k'],\n",
    "                    'recall_at_k': precision_recall['recall_at_k'],\n",
    "                    'mrr': mrr,\n",
    "                    'ndcg_at_k': ndcg,\n",
    "                    'hit_rate_at_k': hit_rate,\n",
    "                    'true_positives': precision_recall['true_positives'],\n",
    "                    'relevant_count': precision_recall['relevant_count']\n",
    "                })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def compare_retrieval_methods(self, methods: Dict[str, callable], \n",
    "                                queries: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Compare multiple retrieval methods\"\"\"\n",
    "        print(\"üîç COMPREHENSIVE RETRIEVAL EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for method_name, method_func in methods.items():\n",
    "            method_results = self.evaluate_retrieval_method(method_func, method_name, queries)\n",
    "            all_results.append(method_results)\n",
    "        \n",
    "        # Combine all results\n",
    "        combined_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        return combined_results\n",
    "    \n",
    "    def generate_evaluation_report(self, results_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        print(\"\\nüìã EVALUATION REPORT\")\n",
    "        print(\"=\" * 40)\n",
    "        \n",
    "        # Overall performance by method\n",
    "        method_performance = results_df.groupby(['method', 'k']).agg({\n",
    "            'precision_at_k': 'mean',\n",
    "            'recall_at_k': 'mean',\n",
    "            'mrr': 'mean',\n",
    "            'ndcg_at_k': 'mean',\n",
    "            'hit_rate_at_k': 'mean'\n",
    "        }).round(4)\n",
    "        \n",
    "        print(\"\\nüìä Average Performance by Method:\")\n",
    "        print(method_performance.to_string())\n",
    "        \n",
    "        # Best performing method for each metric\n",
    "        best_methods = {}\n",
    "        for k in [5, 10]:\n",
    "            k_results = results_df[results_df['k'] == k]\n",
    "            method_avg = k_results.groupby('method').mean()\n",
    "            \n",
    "            best_methods[f'k={k}'] = {\n",
    "                'precision': method_avg['precision_at_k'].idxmax(),\n",
    "                'recall': method_avg['recall_at_k'].idxmax(),\n",
    "                'mrr': method_avg['mrr'].idxmax(),\n",
    "                'ndcg': method_avg['ndcg_at_k'].idxmax(),\n",
    "                'hit_rate': method_avg['hit_rate_at_k'].idxmax()\n",
    "            }\n",
    "        \n",
    "        print(\"\\nüèÜ Best Methods by Metric:\")\n",
    "        for k, metrics in best_methods.items():\n",
    "            print(f\"   {k}:\")\n",
    "            for metric, method in metrics.items():\n",
    "                score = results_df[(results_df['method'] == method) & \n",
    "                                 (results_df['k'] == int(k.split('=')[1]))][f'{metric}_at_k' if 'rate' not in metric and metric != 'mrr' else metric].mean()\n",
    "                print(f\"      {metric.capitalize()}: {method} ({score:.3f})\")\n",
    "        \n",
    "        return {\n",
    "            'method_performance': method_performance,\n",
    "            'best_methods': best_methods,\n",
    "            'raw_results': results_df\n",
    "        }\n",
    "\n",
    "# Initialize evaluator\n",
    "evaluator = RetrievalEvaluator()\n",
    "ground_truth = evaluator.create_evaluation_dataset()\n",
    "\n",
    "print(\"üìä Retrieval Evaluator initialized!\")\n",
    "print(f\"   Created ground truth for {len(ground_truth)} queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comprehensive evaluation\n",
    "evaluation_queries = list(ground_truth.keys())\n",
    "\n",
    "# Define retrieval methods to compare\n",
    "retrieval_methods = {\n",
    "    'Multi-Stage': lambda q: retriever.retrieve(q, final_k=10),\n",
    "    'Multi-Query': lambda q: advanced_retrieval.multi_query_retrieval(q, final_k=10),\n",
    "    'Parent-Child': lambda q: advanced_retrieval.parent_child_retrieval(q, top_k=10),\n",
    "    'Contextual': lambda q: advanced_retrieval.contextual_retrieval(q, top_k=10)\n",
    "}\n",
    "\n",
    "# Run evaluation\n",
    "evaluation_results = evaluator.compare_retrieval_methods(retrieval_methods, evaluation_queries)\n",
    "\n",
    "# Generate report\n",
    "report = evaluator.generate_evaluation_report(evaluation_results)\n",
    "\n",
    "# Visualize results\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Precision@K comparison\n",
    "precision_data = evaluation_results.groupby(['method', 'k'])['precision_at_k'].mean().unstack()\n",
    "precision_data.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_title('Precision@K by Method', fontweight='bold', fontsize=14)\n",
    "ax1.set_ylabel('Precision@K')\n",
    "ax1.set_xlabel('Retrieval Method')\n",
    "ax1.legend(title='K Value')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Recall@K comparison\n",
    "recall_data = evaluation_results.groupby(['method', 'k'])['recall_at_k'].mean().unstack()\n",
    "recall_data.plot(kind='bar', ax=ax2, width=0.8)\n",
    "ax2.set_title('Recall@K by Method', fontweight='bold', fontsize=14)\n",
    "ax2.set_ylabel('Recall@K')\n",
    "ax2.set_xlabel('Retrieval Method')\n",
    "ax2.legend(title='K Value')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. NDCG@K comparison\n",
    "ndcg_data = evaluation_results.groupby(['method', 'k'])['ndcg_at_k'].mean().unstack()\n",
    "ndcg_data.plot(kind='bar', ax=ax3, width=0.8)\n",
    "ax3.set_title('NDCG@K by Method', fontweight='bold', fontsize=14)\n",
    "ax3.set_ylabel('NDCG@K')\n",
    "ax3.set_xlabel('Retrieval Method')\n",
    "ax3.legend(title='K Value')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Hit Rate@K comparison\n",
    "hit_rate_data = evaluation_results.groupby(['method', 'k'])['hit_rate_at_k'].mean().unstack()\n",
    "hit_rate_data.plot(kind='bar', ax=ax4, width=0.8)\n",
    "ax4.set_title('Hit Rate@K by Method', fontweight='bold', fontsize=14)\n",
    "ax4.set_ylabel('Hit Rate@K')\n",
    "ax4.set_xlabel('Retrieval Method')\n",
    "ax4.legend(title='K Value')\n",
    "ax4.tick_params(axis='x', rotation=45)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Evaluation Insights:\")\n",
    "print(\"- Multi-Query retrieval often improves recall by finding diverse relevant documents\")\n",
    "print(\"- Parent-Child retrieval balances precision and completeness\")\n",
    "print(\"- Contextual retrieval helps with ambiguous queries\")\n",
    "print(\"- Multi-Stage baseline provides good overall performance\")\n",
    "print(\"- Choice of method should depend on specific use case requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Exercise 4: Failure Handling and Robust Retrieval\n",
    "\n",
    "Let's implement robust failure handling, edge case management, and confidence scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustRetrievalSystem:\n",
    "    \"\"\"Production-ready retrieval system with failure handling and monitoring\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever: MultiStageRetriever):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.embedding_model = base_retriever.embedding_model\n",
    "        \n",
    "        # Failure tracking\n",
    "        self.failure_stats = defaultdict(int)\n",
    "        self.query_history = []\n",
    "        \n",
    "        # Confidence thresholds\n",
    "        self.min_confidence_threshold = 0.3\n",
    "        self.high_confidence_threshold = 0.7\n",
    "        \n",
    "        # Query preprocessing patterns\n",
    "        self.query_cleaning_patterns = [\n",
    "            (r'[^\\w\\s]', ' '),  # Remove special characters\n",
    "            (r'\\s+', ' '),      # Normalize whitespace\n",
    "            (r'^\\s+|\\s+$', ''), # Strip leading/trailing whitespace\n",
    "        ]\n",
    "        \n",
    "        # Fallback queries\n",
    "        self.fallback_strategies = [\n",
    "            'expand_query',\n",
    "            'simplify_query', \n",
    "            'broaden_search',\n",
    "            'use_keywords_only'\n",
    "        ]\n",
    "    \n",
    "    def preprocess_query(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Clean and analyze query before retrieval\"\"\"\n",
    "        original_query = query\n",
    "        \n",
    "        # Clean query\n",
    "        cleaned_query = query.lower().strip()\n",
    "        \n",
    "        for pattern, replacement in self.query_cleaning_patterns:\n",
    "            cleaned_query = re.sub(pattern, replacement, cleaned_query)\n",
    "        \n",
    "        # Analyze query characteristics\n",
    "        analysis = {\n",
    "            'original': original_query,\n",
    "            'cleaned': cleaned_query,\n",
    "            'length': len(cleaned_query.split()),\n",
    "            'is_empty': len(cleaned_query.strip()) == 0,\n",
    "            'is_too_short': len(cleaned_query.split()) < 2,\n",
    "            'is_too_long': len(cleaned_query.split()) > 50,\n",
    "            'has_question_words': any(word in cleaned_query.lower() \n",
    "                                    for word in ['what', 'how', 'why', 'when', 'where', 'who']),\n",
    "            'complexity_score': textstat.flesch_reading_ease(cleaned_query) if cleaned_query else 0\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def calculate_confidence_score(self, results: List[Tuple], query_analysis: Dict) -> float:\n",
    "        \"\"\"Calculate confidence score for retrieval results\"\"\"\n",
    "        if not results:\n",
    "            return 0.0\n",
    "        \n",
    "        confidence_factors = []\n",
    "        \n",
    "        # Factor 1: Top result score\n",
    "        top_score = results[0][1] if results else 0\n",
    "        confidence_factors.append(min(1.0, top_score * 2))  # Scale up score\n",
    "        \n",
    "        # Factor 2: Score distribution (less variation = higher confidence)\n",
    "        scores = [r[1] for r in results]\n",
    "        if len(scores) > 1:\n",
    "            score_std = np.std(scores)\n",
    "            score_uniformity = 1.0 - min(1.0, score_std * 2)\n",
    "            confidence_factors.append(score_uniformity)\n",
    "        \n",
    "        # Factor 3: Number of results found\n",
    "        result_count_factor = min(1.0, len(results) / 5.0)  # Normalize to 5 results\n",
    "        confidence_factors.append(result_count_factor)\n",
    "        \n",
    "        # Factor 4: Query complexity\n",
    "        if not query_analysis['is_too_short'] and not query_analysis['is_too_long']:\n",
    "            confidence_factors.append(0.8)\n",
    "        else:\n",
    "            confidence_factors.append(0.4)\n",
    "        \n",
    "        # Factor 5: Keyword matching in top results\n",
    "        query_words = set(query_analysis['cleaned'].split())\n",
    "        if results and query_words:\n",
    "            top_doc_words = set(results[0][2].lower().split())\n",
    "            word_overlap = len(query_words.intersection(top_doc_words)) / len(query_words)\n",
    "            confidence_factors.append(word_overlap)\n",
    "        \n",
    "        # Calculate weighted average\n",
    "        weights = [0.3, 0.2, 0.2, 0.15, 0.15]  # Prioritize top score and distribution\n",
    "        confidence = sum(f * w for f, w in zip(confidence_factors, weights[:len(confidence_factors)]))\n",
    "        \n",
    "        return min(1.0, max(0.0, confidence))\n",
    "    \n",
    "    def expand_query(self, query: str) -> str:\n",
    "        \"\"\"Expand query with related terms\"\"\"\n",
    "        expansion_terms = {\n",
    "            'ml': 'machine learning',\n",
    "            'ai': 'artificial intelligence',\n",
    "            'llm': 'large language model',\n",
    "            'api': 'application programming interface',\n",
    "            'gpu': 'graphics processing unit',\n",
    "            'nlp': 'natural language processing'\n",
    "        }\n",
    "        \n",
    "        expanded = query\n",
    "        for abbrev, full_term in expansion_terms.items():\n",
    "            if abbrev in query.lower():\n",
    "                expanded = expanded.replace(abbrev, f\"{abbrev} {full_term}\")\n",
    "        \n",
    "        # Add context words\n",
    "        if 'optimization' in query.lower():\n",
    "            expanded += \" performance improvement efficiency\"\n",
    "        elif 'training' in query.lower():\n",
    "            expanded += \" learning education tutorial\"\n",
    "        elif 'authentication' in query.lower():\n",
    "            expanded += \" security login access control\"\n",
    "        \n",
    "        return expanded\n",
    "    \n",
    "    def simplify_query(self, query: str) -> str:\n",
    "        \"\"\"Simplify complex query to key terms\"\"\"\n",
    "        # Remove question words and common words\n",
    "        stop_words = {'what', 'how', 'why', 'when', 'where', 'who', 'is', 'are', 'the', 'a', 'an'}\n",
    "        words = query.lower().split()\n",
    "        key_words = [w for w in words if w not in stop_words and len(w) > 2]\n",
    "        return ' '.join(key_words[:5])  # Take top 5 key words\n",
    "    \n",
    "    def extract_keywords(self, query: str) -> str:\n",
    "        \"\"\"Extract only the most important keywords\"\"\"\n",
    "        # Simple keyword extraction (in production, use more sophisticated NLP)\n",
    "        important_terms = []\n",
    "        words = query.lower().split()\n",
    "        \n",
    "        # Priority terms\n",
    "        priority_terms = ['tensorflow', 'pytorch', 'api', 'authentication', 'training', \n",
    "                         'optimization', 'machine learning', 'neural network']\n",
    "        \n",
    "        for term in priority_terms:\n",
    "            if term in query.lower():\n",
    "                important_terms.append(term)\n",
    "        \n",
    "        # Add remaining significant words\n",
    "        for word in words:\n",
    "            if len(word) > 4 and word not in important_terms:\n",
    "                important_terms.append(word)\n",
    "        \n",
    "        return ' '.join(important_terms[:3])\n",
    "    \n",
    "    def robust_retrieve(self, query: str, max_attempts: int = 3, \n",
    "                       min_results: int = 1) -> Dict[str, Any]:\n",
    "        \"\"\"Robust retrieval with fallback strategies\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Preprocess query\n",
    "        query_analysis = self.preprocess_query(query)\n",
    "        \n",
    "        # Handle obvious failures early\n",
    "        if query_analysis['is_empty']:\n",
    "            self.failure_stats['empty_query'] += 1\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': 'Empty query provided',\n",
    "                'results': [],\n",
    "                'confidence': 0.0,\n",
    "                'attempts': 0,\n",
    "                'total_time': time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # Try different strategies\n",
    "        attempts = []\n",
    "        current_query = query_analysis['cleaned']\n",
    "        \n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                print(f\"   Attempt {attempt + 1}: '{current_query[:50]}{'...' if len(current_query) > 50 else ''}'\")\n",
    "                \n",
    "                # Perform retrieval\n",
    "                result = self.base_retriever.retrieve(current_query, final_k=10)\n",
    "                \n",
    "                # Calculate confidence\n",
    "                confidence = self.calculate_confidence_score(result['results'], query_analysis)\n",
    "                \n",
    "                attempts.append({\n",
    "                    'attempt': attempt + 1,\n",
    "                    'query': current_query,\n",
    "                    'results': result['results'],\n",
    "                    'confidence': confidence,\n",
    "                    'result_count': len(result['results'])\n",
    "                })\n",
    "                \n",
    "                # Check if results are satisfactory\n",
    "                if (len(result['results']) >= min_results and \n",
    "                    confidence >= self.min_confidence_threshold):\n",
    "                    \n",
    "                    # Success!\n",
    "                    self.query_history.append({\n",
    "                        'query': query,\n",
    "                        'success': True,\n",
    "                        'attempts': attempt + 1,\n",
    "                        'confidence': confidence,\n",
    "                        'timestamp': datetime.now()\n",
    "                    })\n",
    "                    \n",
    "                    return {\n",
    "                        'success': True,\n",
    "                        'results': result['results'],\n",
    "                        'confidence': confidence,\n",
    "                        'attempts': attempt + 1,\n",
    "                        'final_query': current_query,\n",
    "                        'all_attempts': attempts,\n",
    "                        'total_time': time.time() - start_time\n",
    "                    }\n",
    "                \n",
    "                # Try different strategy for next attempt\n",
    "                if attempt < max_attempts - 1:\n",
    "                    strategy = self.fallback_strategies[attempt % len(self.fallback_strategies)]\n",
    "                    \n",
    "                    if strategy == 'expand_query':\n",
    "                        current_query = self.expand_query(current_query)\n",
    "                    elif strategy == 'simplify_query':\n",
    "                        current_query = self.simplify_query(current_query)\n",
    "                    elif strategy == 'use_keywords_only':\n",
    "                        current_query = self.extract_keywords(current_query)\n",
    "                    elif strategy == 'broaden_search':\n",
    "                        # Use broader terms\n",
    "                        current_query = current_query + \" guide tutorial introduction\"\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.failure_stats['retrieval_error'] += 1\n",
    "                attempts.append({\n",
    "                    'attempt': attempt + 1,\n",
    "                    'query': current_query,\n",
    "                    'error': str(e),\n",
    "                    'results': [],\n",
    "                    'confidence': 0.0\n",
    "                })\n",
    "        \n",
    "        # All attempts failed\n",
    "        self.failure_stats['max_attempts_reached'] += 1\n",
    "        self.query_history.append({\n",
    "            'query': query,\n",
    "            'success': False,\n",
    "            'attempts': max_attempts,\n",
    "            'timestamp': datetime.now()\n",
    "        })\n",
    "        \n",
    "        # Return best attempt (highest confidence)\n",
    "        best_attempt = max(attempts, key=lambda x: x.get('confidence', 0))\n",
    "        \n",
    "        return {\n",
    "            'success': False,\n",
    "            'results': best_attempt.get('results', []),\n",
    "            'confidence': best_attempt.get('confidence', 0.0),\n",
    "            'attempts': max_attempts,\n",
    "            'final_query': best_attempt.get('query', current_query),\n",
    "            'all_attempts': attempts,\n",
    "            'warning': 'Low confidence results - manual review recommended',\n",
    "            'total_time': time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    def get_system_health(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system health and performance statistics\"\"\"\n",
    "        total_queries = len(self.query_history)\n",
    "        successful_queries = sum(1 for q in self.query_history if q['success'])\n",
    "        \n",
    "        if total_queries > 0:\n",
    "            success_rate = successful_queries / total_queries\n",
    "            avg_attempts = np.mean([q['attempts'] for q in self.query_history])\n",
    "            avg_confidence = np.mean([q.get('confidence', 0) for q in self.query_history if q['success']])\n",
    "        else:\n",
    "            success_rate = avg_attempts = avg_confidence = 0\n",
    "        \n",
    "        return {\n",
    "            'total_queries': total_queries,\n",
    "            'successful_queries': successful_queries,\n",
    "            'success_rate': success_rate,\n",
    "            'average_attempts': avg_attempts,\n",
    "            'average_confidence': avg_confidence,\n",
    "            'failure_breakdown': dict(self.failure_stats),\n",
    "            'health_status': 'Healthy' if success_rate > 0.8 else 'Warning' if success_rate > 0.6 else 'Critical'\n",
    "        }\n",
    "\n",
    "# Initialize robust retrieval system\n",
    "robust_system = RobustRetrievalSystem(retriever)\n",
    "print(\"üõ°Ô∏è Robust Retrieval System initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test robust retrieval with various edge cases\n",
    "test_cases = [\n",
    "    \"tensorflow optimization performance\",  # Normal query\n",
    "    \"\",  # Empty query\n",
    "    \"   \",  # Whitespace only\n",
    "    \"how\",  # Too short\n",
    "    \"xyz123 nonexistent made-up-term\",  # No matches expected\n",
    "    \"What are the best practices for optimizing machine learning model training performance with GPU acceleration?\",  # Long complex query\n",
    "    \"api auth\",  # Abbreviations\n",
    "    \"ML performance tuning\",  # Mixed abbreviations and terms\n",
    "]\n",
    "\n",
    "print(\"üß™ ROBUST RETRIEVAL TESTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, test_query in enumerate(test_cases):\n",
    "    print(f\"\\nüîç Test Case {i+1}: '{test_query}'\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = robust_system.robust_retrieve(test_query, max_attempts=3)\n",
    "    \n",
    "    print(f\"‚úÖ Success: {result['success']}\")\n",
    "    print(f\"üéØ Confidence: {result['confidence']:.3f}\")\n",
    "    print(f\"üîÑ Attempts: {result['attempts']}\")\n",
    "    print(f\"‚è±Ô∏è Time: {result['total_time']:.3f}s\")\n",
    "    \n",
    "    if 'final_query' in result:\n",
    "        print(f\"üîß Final Query: '{result['final_query']}'\")\n",
    "    \n",
    "    if result['results']:\n",
    "        print(f\"üìÑ Top Result: {result['results'][0][2][:60]}...\")\n",
    "    \n",
    "    if 'warning' in result:\n",
    "        print(f\"‚ö†Ô∏è Warning: {result['warning']}\")\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "\n",
    "# Display system health\n",
    "health = robust_system.get_system_health()\n",
    "print(f\"\\nüè• SYSTEM HEALTH REPORT\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"Status: {health['health_status']}\")\n",
    "print(f\"Success Rate: {health['success_rate']:.1%}\")\n",
    "print(f\"Average Attempts: {health['average_attempts']:.1f}\")\n",
    "print(f\"Average Confidence: {health['average_confidence']:.3f}\")\n",
    "print(f\"Total Queries: {health['total_queries']}\")\n",
    "print(f\"Failures: {health['failure_breakdown']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "From this module, you should now understand:\n",
    "\n",
    "### üèóÔ∏è Multi-Stage Retrieval Architecture:\n",
    "1. **Stage 1 - Initial Retrieval**: Fast, broad search (vector + BM25 hybrid)\n",
    "2. **Stage 2 - Re-ranking**: Precise scoring with cross-encoders or LLM judges\n",
    "3. **Stage 3 - Post-processing**: Diversity, deduplication, context optimization\n",
    "4. **Benefits**: Balances speed and quality, handles large-scale retrieval efficiently\n",
    "\n",
    "### üöÄ Advanced Retrieval Techniques (2025):\n",
    "\n",
    "#### Multi-Query Retrieval:\n",
    "- **Improvement**: 25-40% better recall through query diversification\n",
    "- **Cost**: 3-5x more retrieval operations\n",
    "- **Best for**: High recall requirements, diverse terminology domains\n",
    "\n",
    "#### Parent-Child Retrieval:\n",
    "- **Strategy**: Search small chunks (precision), return large chunks (context)\n",
    "- **Benefit**: Optimal balance of specificity and completeness\n",
    "- **Implementation**: Hierarchical document chunking with mapping\n",
    "\n",
    "#### Contextual Retrieval (Anthropic 2025):\n",
    "- **Innovation**: Add context to chunks before embedding\n",
    "- **Improvement**: 49% reduction in failed retrievals\n",
    "- **Method**: LLM generates contextual information for each chunk\n",
    "\n",
    "### üìä Evaluation Metrics Hierarchy:\n",
    "1. **Precision@K**: Relevance of retrieved results\n",
    "2. **Recall@K**: Coverage of relevant documents\n",
    "3. **NDCG@K**: Ranked relevance quality\n",
    "4. **MRR**: Mean Reciprocal Rank for ranking quality\n",
    "5. **Hit Rate**: System reliability (any relevant result found)\n",
    "\n",
    "### üõ°Ô∏è Production Robustness:\n",
    "\n",
    "#### Failure Handling:\n",
    "- **Query preprocessing**: Clean, validate, and analyze queries\n",
    "- **Fallback strategies**: Expand, simplify, broaden, extract keywords\n",
    "- **Confidence scoring**: Multi-factor confidence assessment\n",
    "- **Graceful degradation**: Return best effort results with warnings\n",
    "\n",
    "#### Monitoring & Health:\n",
    "- **Success rate tracking**: Monitor query success/failure rates\n",
    "- **Confidence distribution**: Track confidence score patterns\n",
    "- **Performance metrics**: Query latency and throughput monitoring\n",
    "- **Error categorization**: Systematic failure analysis\n",
    "\n",
    "### üéØ Performance Optimization Guidelines:\n",
    "\n",
    "| Stage | Optimization Focus | Typical Latency |\n",
    "|-------|-------------------|----------------|\n",
    "| **Stage 1** | Vector index efficiency, hybrid fusion | 50-200ms |\n",
    "| **Stage 2** | Cross-encoder batching, model size | 100-500ms |\n",
    "| **Stage 3** | Diversity algorithms, metadata processing | 10-50ms |\n",
    "| **Total** | End-to-end pipeline optimization | 200-800ms |\n",
    "\n",
    "### üîÑ Retrieval Strategy Selection:\n",
    "\n",
    "1. **High Precision Needs**: Multi-stage with cross-encoder re-ranking\n",
    "2. **High Recall Needs**: Multi-query retrieval with fusion\n",
    "3. **Context Rich Results**: Parent-child retrieval\n",
    "4. **Ambiguous Queries**: Contextual retrieval with metadata\n",
    "5. **Production Deployment**: Robust system with fallbacks\n",
    "\n",
    "### üìà Implementation Roadmap:\n",
    "1. **Start Simple** ‚Üí 2. **Add Multi-Stage** ‚Üí 3. **Implement Advanced Techniques** ‚Üí 4. **Add Robustness** ‚Üí 5. **Monitor & Optimize**\n",
    "\n",
    "## üéØ Next Steps\n",
    "\n",
    "In the next modules, we'll explore:\n",
    "- **Module 10**: Prompt engineering for optimal RAG performance\n",
    "- **Module 11**: LLM integration and model selection strategies  \n",
    "- **Module 12**: Complete RAG system integration and deployment\n",
    "\n",
    "Mastering retrieval strategies is crucial for building high-quality RAG systems that can handle diverse queries and scale to production workloads!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§î Discussion Questions\n",
    "\n",
    "1. When would you choose multi-query retrieval over parent-child retrieval?\n",
    "2. How would you balance retrieval latency vs quality in a real-time system?\n",
    "3. What additional confidence factors would you implement for your domain?\n",
    "4. How would you handle retrieval for multi-lingual documents?\n",
    "5. What metrics would you use to trigger re-indexing in production?\n",
    "\n",
    "## üìù Optional Exercises\n",
    "\n",
    "1. **Real Cross-Encoder**: Implement actual cross-encoder re-ranking with Hugging Face models\n",
    "2. **LLM Judge**: Use GPT-4 or Claude for LLM-based re-ranking\n",
    "3. **Custom Confidence**: Design domain-specific confidence scoring\n",
    "4. **A/B Testing**: Compare retrieval strategies with statistical significance\n",
    "5. **Production Monitoring**: Build dashboards for retrieval system health"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}